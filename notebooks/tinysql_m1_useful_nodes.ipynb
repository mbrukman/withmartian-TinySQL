{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQAsYjSQVXHC"
      },
      "source": [
        "# M1 useful nodes\n",
        "This notebook identifies M1 nodes (attention heads and MLPs) that, when ablated, cause a decrease in model prediction accuracy. These nodes are needed (aka useful) for accurate predictions.\n",
        "\n",
        "\n",
        "This notebook was:\n",
        "- Developed on Google Colab using an **T4**\n",
        "- Runs with M1 (TinyStories) with base/CS1/CS2/CS3 models.\n",
        "- Requires a GITHUB_TOKEN secret to access Martian TinySQL code repository.\n",
        "- Requires a HF_TOKEN secret to access Martian HuggingFace repository.\n",
        "\n",
        "This notebook relies on the nnsight library. Useful background:\n",
        "- https://nnsight.net/notebooks/tutorials/walkthrough/#Batching\n",
        "- https://nnsight.net/notebooks/tutorials/walkthrough/#Looping\n",
        "\n",
        "This notebook relies on the https://github.com/PhilipQuirke/quanta_mech_interp library for graphing useful nodes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1tQZgLUnojc"
      },
      "source": [
        "# Import libraries\n",
        "Imports standard libraries. Do not read."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qt2jt2bHcY2v",
        "outputId": "a4023824-37dd-42ea-9577-e8eb626ec430"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nnsight in /usr/local/lib/python3.10/dist-packages (0.3.7)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from nnsight) (4.47.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from nnsight) (4.25.5)\n",
            "Requirement already satisfied: python-socketio[client] in /usr/local/lib/python3.10/dist-packages (from nnsight) (5.12.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from nnsight) (0.21.0)\n",
            "Requirement already satisfied: pydantic>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from nnsight) (2.10.3)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from nnsight) (2.5.1+cu121)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from nnsight) (0.2.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from nnsight) (0.20.1+cu121)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from nnsight) (1.2.1)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.10/dist-packages (from nnsight) (0.31.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from nnsight) (0.8.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.0->nnsight) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.0->nnsight) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.0->nnsight) (4.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.0->nnsight) (0.27.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->nnsight) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->nnsight) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->nnsight) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->nnsight) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->nnsight) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.4.0->nnsight) (1.3.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate->nnsight) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->nnsight) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->nnsight) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate->nnsight) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate->nnsight) (0.4.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers->nnsight) (8.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers->nnsight) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers->nnsight) (2.32.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers->nnsight) (11.0.0)\n",
            "Requirement already satisfied: bidict>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from python-socketio[client]->nnsight) (0.23.1)\n",
            "Requirement already satisfied: python-engineio>=4.11.0 in /usr/local/lib/python3.10/dist-packages (from python-socketio[client]->nnsight) (4.11.2)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from python-socketio[client]->nnsight) (1.8.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->nnsight) (4.67.1)\n",
            "Requirement already satisfied: simple-websocket>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from python-engineio>=4.11.0->python-socketio[client]->nnsight) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers->nnsight) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers->nnsight) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers->nnsight) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers->nnsight) (2024.12.14)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers->nnsight) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.4.0->nnsight) (3.0.2)\n",
            "Requirement already satisfied: wsproto in /usr/local/lib/python3.10/dist-packages (from simple-websocket>=0.10.0->python-engineio>=4.11.0->python-socketio[client]->nnsight) (1.2.0)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto->simple-websocket>=0.10.0->python-engineio>=4.11.0->python-socketio[client]->nnsight) (0.14.0)\n"
          ]
        }
      ],
      "source": [
        "# https://nnsight.net/\n",
        "# Access 0.4 prerelease version (as at Dec 2024)\n",
        "#!pip install nnsight==0.4.0.dev0\n",
        "!pip install -U nnsight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Vif7qLNrlC0P"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import einops\n",
        "import torch\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = \"colab\"\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm.auto as tqdm\n",
        "\n",
        "import nnsight\n",
        "from nnsight import LanguageModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "orVn0wTnosHO"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "from google.colab import userdata\n",
        "import gc\n",
        "import weakref"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1zClTbuyFUG",
        "outputId": "8f77b71a-4220-415d-8b94-cf40d78544f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/PhilipQuirke/quanta_mech_interp.git\n",
            "  Cloning https://github.com/PhilipQuirke/quanta_mech_interp.git to /tmp/pip-req-build-yaa0wjir\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/PhilipQuirke/quanta_mech_interp.git /tmp/pip-req-build-yaa0wjir\n",
            "  Resolved https://github.com/PhilipQuirke/quanta_mech_interp.git to commit 7a53d2b4cf91a41532b2f8437fe43e58febca204\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from QuantaMechInterp==1.0) (1.26.4)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from QuantaMechInterp==1.0) (5.24.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from QuantaMechInterp==1.0) (3.8.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from QuantaMechInterp==1.0) (1.6.0)\n",
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.10/dist-packages (from QuantaMechInterp==1.0) (0.10.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from QuantaMechInterp==1.0) (4.47.1)\n",
            "Requirement already satisfied: typeguard>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from QuantaMechInterp==1.0) (4.4.1)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from QuantaMechInterp==1.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchtyping>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from QuantaMechInterp==1.0) (0.1.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->QuantaMechInterp==1.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->QuantaMechInterp==1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->QuantaMechInterp==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->QuantaMechInterp==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->QuantaMechInterp==1.0) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->QuantaMechInterp==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10->QuantaMechInterp==1.0) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->QuantaMechInterp==1.0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->QuantaMechInterp==1.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->QuantaMechInterp==1.0) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->QuantaMechInterp==1.0) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->QuantaMechInterp==1.0) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->QuantaMechInterp==1.0) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->QuantaMechInterp==1.0) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->QuantaMechInterp==1.0) (2.8.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->QuantaMechInterp==1.0) (9.0.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->QuantaMechInterp==1.0) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->QuantaMechInterp==1.0) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->QuantaMechInterp==1.0) (3.5.0)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize->QuantaMechInterp==1.0) (24.12.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers->QuantaMechInterp==1.0) (0.27.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->QuantaMechInterp==1.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->QuantaMechInterp==1.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->QuantaMechInterp==1.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->QuantaMechInterp==1.0) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->QuantaMechInterp==1.0) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->QuantaMechInterp==1.0) (4.67.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->QuantaMechInterp==1.0) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->QuantaMechInterp==1.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->QuantaMechInterp==1.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->QuantaMechInterp==1.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->QuantaMechInterp==1.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->QuantaMechInterp==1.0) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade git+https://github.com/PhilipQuirke/quanta_mech_interp.git\n",
        "import QuantaMechInterp as qmi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMWEb8TJoske",
        "outputId": "7fe4f345-dd29-47bf-e498-236d9932bae8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://****@github.com/withmartian/TinySQL.git\n",
            "  Cloning https://****@github.com/withmartian/TinySQL.git to /tmp/pip-req-build-9wgo544c\n",
            "  Running command git clone --filter=blob:none --quiet 'https://****@github.com/withmartian/TinySQL.git' /tmp/pip-req-build-9wgo544c\n",
            "  Resolved https://****@github.com/withmartian/TinySQL.git to commit 8c4ec093c1f14305ee081a3f1b54b7cf44c7cda2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from TinySQL==1.0) (1.26.4)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from TinySQL==1.0) (0.45.1)\n"
          ]
        }
      ],
      "source": [
        "github_token = userdata.get(\"GITHUB_TOKEN\")\n",
        "\n",
        "# Install the private repository using the token\n",
        "!pip install --upgrade git+https://{github_token}@github.com/withmartian/TinySQL.git\n",
        "\n",
        "import TinySQL as qts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6k59y0OZhp7"
      },
      "source": [
        "# Select model and command set to investigate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "rl1akjeAUfoV"
      },
      "outputs": [],
      "source": [
        "model_num = 1                 # 1=TinyStories, 2=Qwen, 3=Llama, 4=Granite, 5=SmolLM\n",
        "cs_num = 1                    # 0=BaseModel, 1=CS1, 2=CS2 or 3=CS3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y72URtyynvnv"
      },
      "source": [
        "# Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFaFTy6LnbxI"
      },
      "outputs": [],
      "source": [
        "model = qts.load_tinysql_model(model_num, cs_num, auth_token=userdata.get(\"HF_TOKEN\"))\n",
        "clear_output()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRL4ckXZqElr"
      },
      "outputs": [],
      "source": [
        "N_LAYERS, N_HEADS, D_MODEL, D_HEAD = qts.get_model_sizes(model_num, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTJ2LHzo5flR"
      },
      "outputs": [],
      "source": [
        "model_hf_name = qts.sql_interp_model_location(model_num, cs_num)\n",
        "\n",
        "# Singleton QuantaTool \"main\" configuration class. qmi.AlgoConfig is derived from the chain qmi.UsefulConfig > qmi.ModelConfig\n",
        "cfg = qmi.AlgoConfig()\n",
        "cfg.repo_name, cfg.model_name = model_hf_name.rsplit(\"/\", 1)\n",
        "cfg.main_model = model\n",
        "cfg.n_layers = N_LAYERS\n",
        "cfg.n_heads = N_HEADS\n",
        "cfg.d_model = D_MODEL\n",
        "cfg.d_head = D_HEAD\n",
        "cfg.set_seed(673023)\n",
        "\n",
        "print(\"cfg.repo_name=\"+cfg.repo_name, \"cfg.model_name=\"+cfg.model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjjpDQ2_cS2-"
      },
      "source": [
        "# Generate test data and experiment runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76YZ8VErsHjP"
      },
      "outputs": [],
      "source": [
        "# Generate a batch of prompts with 3 field names\n",
        "def generate_batch(batch_size):\n",
        "    cfg.batch_size = batch_size\n",
        "\n",
        "    return qts.generate_csn(batch_size=batch_size, csn=max(1,cs_num), min_cols=3, max_cols=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBS0v8Gir-d4"
      },
      "outputs": [],
      "source": [
        "# Check if the model can correctly predict the question's answer\n",
        "def run_attention_experiment(run_layer_idx, run_head_idx, run_token_idx, all_tokens, run_prompt_tokens, run_generate, make_changes = True, extra_tokens=0):\n",
        "\n",
        "    assert len(all_tokens) == run_prompt_tokens + run_generate\n",
        "\n",
        "    if run_generate == 0:\n",
        "        return True, \"\", \"\"\n",
        "\n",
        "    start = run_head_idx * D_HEAD\n",
        "    end = (run_head_idx + 1) * D_HEAD\n",
        "\n",
        "    with model.generate(all_tokens[:run_prompt_tokens], max_new_tokens=run_generate + extra_tokens, pad_token_id=model.tokenizer.eos_token_id):\n",
        "\n",
        "        if make_changes:\n",
        "            model.transformer.h[run_layer_idx].output[0][:, run_token_idx, start:end] = 0\n",
        "\n",
        "        final_output = model.generator.output.save()\n",
        "\n",
        "    # Did the output change?\n",
        "    decoded_input = model.tokenizer.decode(all_tokens, skip_special_tokens=True)\n",
        "    decoded_output = model.tokenizer.decode(final_output[0], skip_special_tokens=True)\n",
        "    same = decoded_input==decoded_output\n",
        "\n",
        "    return same, decoded_input, decoded_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the model can correctly predict the question's answer and print result\n",
        "def check_experiment(prompt, answer, all_tokens, run_prompt_tokens, run_generate, make_changes = True, show_same = True):\n",
        "\n",
        "    same, decoded_input, decoded_output = run_attention_experiment(0, 0, 0, all_tokens, run_prompt_tokens, run_generate, make_changes, extra_tokens=100)\n",
        "\n",
        "    if not same:\n",
        "        print( \"Ignoring example that model doesn't predict correctly:\")\n",
        "\n",
        "    if (not same) or show_same:\n",
        "        print(\"Prompt (\"+str(run_prompt_tokens)+\") :\", prompt.replace('\\n', '\\\\n'))\n",
        "        print(\"Answer (\"+str(run_generate)+\") :\", answer.replace('\\n', '\\\\n'))\n",
        "        print(\"Output:\", decoded_output.replace('\\n', '\\\\n'))\n",
        "        print()\n",
        "\n",
        "    return same, decoded_input, decoded_output"
      ],
      "metadata": {
        "id": "e1Ze2PY7Nber"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check that generators work\n",
        "def check_generator_on_clean_input(examples):\n",
        "    num_good = 0\n",
        "    for example in examples:\n",
        "        prompt = example.get_alpaca_prompt()\n",
        "        answer = example.sql_statement\n",
        "\n",
        "        prompt_tokens = model.tokenizer(prompt)[\"input_ids\"]\n",
        "        answer_tokens = model.tokenizer(answer)[\"input_ids\"]\n",
        "        all_tokens = prompt_tokens + answer_tokens\n",
        "\n",
        "        num_answer_tokens = len(answer_tokens)\n",
        "        num_prompt_tokens = len(prompt_tokens)\n",
        "\n",
        "        same, _, decoded_output = check_experiment(prompt, answer, all_tokens, num_prompt_tokens, num_answer_tokens, make_changes = False, show_same = False)\n",
        "        if same:\n",
        "            num_good += 1\n",
        "\n",
        "    # Almost all examples should succeed\n",
        "    assert num_good >= len(examples) - 1\n",
        "\n",
        "\n",
        "generator = qts.CorruptFeatureTestGenerator(model_num=model_num, cs_num=cs_num, tokenizer=model.tokenizer, use_novel_names=True)\n",
        "check_generator_on_clean_input( generator.generate_feature_examples(qts.ENGFIELDNAME, 6) )\n",
        "\n",
        "check_generator_on_clean_input( qts.generate_csn(batch_size=6, csn=max(1,cs_num), min_cols=3, max_cols=3) )"
      ],
      "metadata": {
        "id": "XuCHN1hWfeBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ju8wtYel8xRm"
      },
      "outputs": [],
      "source": [
        "# Return a list of experiments to run\n",
        "def get_experiment_list(examples, by_attention_head):\n",
        "    run_list = []\n",
        "    max_prompt_tokens = 0\n",
        "    max_answer_tokens = 0\n",
        "    max_tokens = 0\n",
        "    show_examples = 3\n",
        "\n",
        "    for example in examples:\n",
        "        prompt = example.get_alpaca_prompt()\n",
        "        answer = example.sql_statement\n",
        "\n",
        "        prompt_tokens = model.tokenizer(prompt)[\"input_ids\"]\n",
        "        answer_tokens = model.tokenizer(answer)[\"input_ids\"]\n",
        "        all_tokens = prompt_tokens + answer_tokens\n",
        "\n",
        "        num_answer_tokens = len(answer_tokens)\n",
        "        num_prompt_tokens = len(prompt_tokens)\n",
        "        num_tokens = len(all_tokens)\n",
        "        assert num_tokens == num_prompt_tokens + num_answer_tokens\n",
        "\n",
        "        max_prompt_tokens = max(max_prompt_tokens, num_prompt_tokens)\n",
        "        max_answer_tokens = max(max_answer_tokens, num_answer_tokens)\n",
        "        max_tokens = max(max_tokens, num_tokens)\n",
        "\n",
        "        # Check that the model can correctly predict the question's answer\n",
        "        same, _, decoded_output = check_experiment(prompt, answer, all_tokens, num_prompt_tokens, num_answer_tokens, make_changes = False, show_same = show_examples > 0)\n",
        "\n",
        "        if not same:\n",
        "            continue\n",
        "        show_examples -= 1\n",
        "\n",
        "        if by_attention_head:\n",
        "            for layer_idx in range(N_LAYERS):\n",
        "                for head_idx in range(N_HEADS):\n",
        "                    for token_idx in range(num_tokens):\n",
        "                        # Important logic:\n",
        "                        # num_prompt_tokens and num_answer_tokens are the normal interpretation of the prompt/answer sizes.\n",
        "                        # If ablating a token in the MIDDLE of the PROMPT, we provide ALL the prompt tokens, and generate num_answer_tokens.\n",
        "                        # If ablating a token in the MIDDLE of the ANSWER, we increase the size of the \"prompt\" and decrease the generated tokens.\n",
        "                        exp_prompt_tokens = max(num_prompt_tokens, token_idx+1)\n",
        "                        exp_num_generate = num_tokens - exp_prompt_tokens\n",
        "\n",
        "                        assert num_tokens == exp_prompt_tokens + exp_num_generate\n",
        "\n",
        "                        run_list.append([layer_idx, head_idx, token_idx, all_tokens, exp_prompt_tokens, exp_num_generate])\n",
        "        else:\n",
        "            for token_idx in range(num_prompt_tokens):\n",
        "                run_list.append([prompt, answer, token_idx, prompt_tokens])\n",
        "\n",
        "    return run_list, max_prompt_tokens, max_answer_tokens, max_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wdUVEfAcXJa"
      },
      "source": [
        "# Which token positions are useful?\n",
        "This information is used to shrink the size of search spaces in following sections. For the SQL model all token positions are useful"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETOrSFQqVSG6"
      },
      "outputs": [],
      "source": [
        "N_BATCH = 50\n",
        "\n",
        "def run_token_experiments( ):\n",
        "    examples = generate_batch(N_BATCH)\n",
        "    run_list, max_prompt_tokens, max_good_answer_tokens, max_tokens = get_experiment_list(examples, False)\n",
        "    cfg.initialize_token_positions( max_prompt_tokens, max_good_answer_tokens, True )\n",
        "    num_exps = len(run_list)\n",
        "\n",
        "    print(\"N_BATCH=\"+str(N_BATCH), \"max_prompt_tokens=\"+str(max_prompt_tokens), \"max_good_answer_tokens=\"+str(max_good_answer_tokens), \"max_tokens=\"+str(max_tokens), \"num_exps=\"+str(num_exps))\n",
        "\n",
        "    try_results = np.zeros(max_tokens, dtype=int)\n",
        "    fail_results = np.zeros(max_tokens, dtype=int)\n",
        "\n",
        "    for item_num in tqdm.tqdm(range(num_exps)):\n",
        "\n",
        "        run_item = run_list[item_num]\n",
        "        run_prompt, run_answer, run_token_idx, prompt_tokens = run_item\n",
        "\n",
        "        with model.generate(prompt_tokens, max_new_tokens=max_good_answer_tokens, pad_token_id=model.tokenizer.eos_token_id) :\n",
        "\n",
        "            # Zero out just the portion of the output corresponding to this token position\n",
        "            for run_layer_idx in range(N_LAYERS):\n",
        "                model.transformer.h[run_layer_idx].output[0][:, run_token_idx, :] = 0\n",
        "\n",
        "            final_output = model.generator.output.save()\n",
        "\n",
        "        decoded_output = model.tokenizer.decode(final_output[0], skip_special_tokens=True)\n",
        "\n",
        "        # Did the output change?\n",
        "        if run_prompt + run_answer != decoded_output:\n",
        "            fail_results[run_token_idx] += 1\n",
        "        try_results[run_token_idx] += 1\n",
        "\n",
        "    # Compute the failure rate as percentage\n",
        "    failure_rate = (1.0 * fail_results / (try_results + 1e-10)) * 100\n",
        "    failure_rate = np.round(failure_rate, 2)\n",
        "\n",
        "    return max_tokens, fail_results, failure_rate\n",
        "\n",
        "g_max_tokens, g_token_fail_results, g_token_failure_rate = run_token_experiments()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RT7zG1EDdA5C"
      },
      "outputs": [],
      "source": [
        "print( \"Useful token positions:\" )\n",
        "for token_idx in range(len(g_token_failure_rate)):\n",
        "    if g_token_failure_rate[token_idx] > 0 :\n",
        "        cfg.add_useful_position(token_idx)\n",
        "        print( \"Position:\", token_idx, \"    % Fails:\", g_token_failure_rate[token_idx], \"    # Fails:\", g_token_fail_results[token_idx] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZ0aAic8jqiV"
      },
      "outputs": [],
      "source": [
        "#cfg.calc_position_failures_map(g_token_fail_results.tolist())\n",
        "#qmi.save_plt_to_file(cfg=cfg, full_title=\"Failures When Position Ablated\")\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMs7MpPocb6_"
      },
      "source": [
        "# Which token+layer+attention head nodes are useful?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bINHUagozRoo"
      },
      "outputs": [],
      "source": [
        "N_BATCH = 10\n",
        "\n",
        "def run_attention_experiments():\n",
        "    show_diff = True\n",
        "    examples = generate_batch(N_BATCH)\n",
        "    run_list, max_prompt_tokens, max_good_answer_tokens, max_tokens = get_experiment_list(examples, True)\n",
        "    num_exps = len(run_list)\n",
        "\n",
        "    print(\"N_BATCH=\"+str(N_BATCH), \"max_prompt_tokens=\"+str(max_prompt_tokens), \"max_answer_tokens=\"+str(max_good_answer_tokens), \"max_tokens=\"+str(max_tokens), \"num_exps=\"+str(num_exps))\n",
        "\n",
        "    try_results = np.zeros((N_LAYERS, N_HEADS, max_tokens), dtype=int)\n",
        "    fail_results = np.zeros((N_LAYERS, N_HEADS, max_tokens), dtype=int)\n",
        "\n",
        "    for item_num in tqdm.tqdm(range(num_exps)):\n",
        "\n",
        "        run_item = run_list[item_num]\n",
        "        run_layer_idx, run_head_idx, run_token_idx, all_tokens, num_prompt_tokens, num_generate = run_item\n",
        "\n",
        "        same, decoded_input, decoded_output = run_attention_experiment(run_layer_idx, run_head_idx, run_token_idx, all_tokens, num_prompt_tokens, num_generate)\n",
        "\n",
        "        if not same:\n",
        "            fail_results[run_layer_idx, run_head_idx, run_token_idx] += 1\n",
        "            if show_diff:\n",
        "                print(\"Failure when intervening:\", \"Layer=\"+str(run_layer_idx), \"Head=\"+str(run_head_idx), \"Pos=\"+str(run_token_idx), \"NumPrompts=\"+str(num_prompt_tokens), \"NumGenerate=\"+str(num_generate))\n",
        "                print(\"Input :\", decoded_input.replace('\\n', '\\\\n'))\n",
        "                print(\"Output:\", decoded_output.replace('\\n', '\\\\n'))\n",
        "                show_diff = False\n",
        "        try_results[run_layer_idx, run_head_idx, run_token_idx] += 1\n",
        "\n",
        "\n",
        "    # Compute the failure rate as percentage\n",
        "    failure_rate = (1.0 * fail_results / (try_results + 1e-10)) * 100\n",
        "    failure_rate = np.round(failure_rate, 2)\n",
        "\n",
        "    return max_tokens, fail_results, failure_rate, try_results\n",
        "\n",
        "g_max_tokens, g_attn_failure_results, g_attn_failure_rate, g_attn_try_results = run_attention_experiments()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nViXbBVDxUt"
      },
      "outputs": [],
      "source": [
        "cfg.useful_nodes = qmi.UsefulNodeList()\n",
        "for layer_idx in range(N_LAYERS):\n",
        "    for head_idx in range(N_HEADS):\n",
        "        for token_idx in range(g_max_tokens):\n",
        "            fail_perc = int(g_attn_failure_rate[layer_idx, head_idx, token_idx])\n",
        "            if fail_perc > 0 :\n",
        "                # Add percentage failure quanta\n",
        "                node_location = qmi.NodeLocation(token_idx, layer_idx, True, head_idx)\n",
        "                cfg.add_useful_node_tag( node_location, qmi.QType.FAIL.value, str(fail_perc) )\n",
        "\n",
        "cfg.useful_nodes.sort_nodes()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2lbWJrUwY7p"
      },
      "outputs": [],
      "source": [
        "for layer_idx in range(N_LAYERS):\n",
        "    title = \"TinySQL Percentage of Output Changes by Zeroing Activations in Layer \" + str(layer_idx)\n",
        "    plt.imshow(g_attn_failure_rate[layer_idx], cmap=\"viridis\", aspect=\"auto\")\n",
        "    plt.colorbar(label=\"Percentage Change\")\n",
        "    plt.xlabel(\"Token Position\")\n",
        "    plt.ylabel(\"Attention Head\")\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "    qmi.save_plt_to_file(cfg=cfg, full_title=title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAHRK6TOmMac"
      },
      "outputs": [],
      "source": [
        "# cfg.useful_nodes.print_node_tags()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCAu8QO1mNB0"
      },
      "outputs": [],
      "source": [
        "title = \"TinySQL useful attention heads\"\n",
        "ax1, quanta_results, num_results = qmi.calc_quanta_map(\n",
        "    cfg, True, 6,\n",
        "    cfg.useful_nodes, qmi.QType.FAIL.value, \"\", qmi.get_quanta_fail_perc,\n",
        "    combine_identical_cells=False)\n",
        "\n",
        "if num_results > 0:\n",
        "    if cfg.graph_file_suffix > \"\":\n",
        "        print(\"Saving quanta map:\", title)\n",
        "        qmi.save_plt_to_file(cfg=cfg, full_title=title)\n",
        "    else:\n",
        "        ax1.set_title(title + ' ({} nodes)'.format(len(quanta_results)))\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_szu6Trokt2"
      },
      "outputs": [],
      "source": [
        "# Serialize and save the useful nodes list to a temporary CoLab file in JSON format. Manually download.\n",
        "useful_node_json_filename = 'tinysql_bm' + str(model_num) + \"_cs\" + str(cs_num) + '_useful_nodes.json'\n",
        "print( \"Saving useful node list with behavior tags:\", useful_node_json_filename)\n",
        "cfg.useful_nodes.save_nodes(useful_node_json_filename)\n",
        "\n",
        "#TODO: Auto save to Martian wandb"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "z1tQZgLUnojc"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}