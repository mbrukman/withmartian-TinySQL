{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQAsYjSQVXHC"
      },
      "source": [
        "# TinySQL : M1 useful nodes\n",
        "\n",
        "**Background:** A \"TinySQL\" model takes as input 1) An Instruction, which is an english data request sentence and 2) A Context, which is a SQL table create statement. The model outputs a Response, which is a SQL select statement.  \n",
        "\n",
        "**Notebook purpose:** Identifies nodes (attention heads and MLPs) in the M1 model that, when ablated, cause a decrease in model prediction accuracy. These nodes are needed (aka useful) for accurate predictions.\n",
        "\n",
        "**Notebook details:** This notebook:\n",
        "- Was developed on Google Colab using an **T4**\n",
        "- Runs with M1 (TinyStories) with base/CS1/CS2/CS3 models.\n",
        "- Requires a GITHUB_TOKEN secret to access Martian TinySQL code repository.\n",
        "- Requires a HF_TOKEN secret to access Martian HuggingFace repository.\n",
        "- Was developed under a grant provided by withmartian.com ( https://withmartian.com )\n",
        "- Relies on the nnsight library. Refer also https://nnsight.net/notebooks/tutorials/walkthrough/#Batching and https://nnsight.net/notebooks/tutorials/walkthrough/#Looping\n",
        "- Relies on the https://github.com/PhilipQuirke/quanta_mech_interp library for graphing useful nodes.\n",
        "Is based on the https://nnsight.net/notebooks/tutorials/activation_patching tutorial\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1tQZgLUnojc"
      },
      "source": [
        "# Import libraries\n",
        "Imports standard libraries. Do not read."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qt2jt2bHcY2v"
      },
      "outputs": [],
      "source": [
        "# https://nnsight.net/\n",
        "# !pip install -U nnsight\n",
        "!pip install nnsight==0.3.7 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vif7qLNrlC0P"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import einops\n",
        "import torch\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = \"colab\"\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm.auto as tqdm\n",
        "\n",
        "import nnsight\n",
        "from nnsight import LanguageModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orVn0wTnosHO"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "from google.colab import userdata\n",
        "import gc\n",
        "import weakref"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1zClTbuyFUG"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade git+https://github.com/PhilipQuirke/quanta_mech_interp.git -q\n",
        "import QuantaMechInterp as qmi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcZUcaYkS28X"
      },
      "outputs": [],
      "source": [
        "!pip install datasets -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMWEb8TJoske"
      },
      "outputs": [],
      "source": [
        "github_token = userdata.get(\"GITHUB_TOKEN\")\n",
        "\n",
        "# Install the private repository using the token\n",
        "!pip install --upgrade git+https://{github_token}@github.com/withmartian/TinySQL.git\n",
        "\n",
        "import TinySQL as qts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6k59y0OZhp7"
      },
      "source": [
        "# Investigation control parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rl1akjeAUfoV"
      },
      "outputs": [],
      "source": [
        "model_num = 1                 # Model to load: 1=TinyStories, 2=Qwen, 3=Llama\n",
        "model_cs_num = 3              # Model variant to load: 0=BaseModel, 1=CS1, 2=CS2 or 3=CS3\n",
        "prompt_cs_num = 2             # Style of prompt data generated that is used in testing. Often set equal to model_cs_num\n",
        "use_synonyms = True          # Use synonyms for prompt english Instruction table and field names?\n",
        "constant_num_tokens = True    # If true, all prompts have the same number of tokens. Less variability but aids comparisons between param variations.\n",
        "batch_size = 30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y72URtyynvnv"
      },
      "source": [
        "# Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFaFTy6LnbxI"
      },
      "outputs": [],
      "source": [
        "hf_token = auth_token=userdata.get(\"HF_TOKEN\")\n",
        "model = qts.load_tinysql_model(model_num, model_cs_num, synonym=use_synonyms, auth_token=hf_token)\n",
        "clear_output()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRL4ckXZqElr"
      },
      "outputs": [],
      "source": [
        "N_LAYERS, N_HEADS, D_MODEL, D_HEAD = qts.get_model_sizes(model_num, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTJ2LHzo5flR"
      },
      "outputs": [],
      "source": [
        "model_hf_name = qts.sql_interp_model_location(model_num, model_cs_num)\n",
        "\n",
        "# Singleton QuantaTool \"main\" configuration class. qmi.AlgoConfig is derived from the chain qmi.UsefulConfig > qmi.ModelConfig\n",
        "cfg = qmi.AlgoConfig()\n",
        "cfg.repo_name, cfg.model_name = model_hf_name.rsplit(\"/\", 1)\n",
        "cfg.main_model = model\n",
        "cfg.n_layers = N_LAYERS\n",
        "cfg.n_heads = N_HEADS\n",
        "cfg.d_model = D_MODEL\n",
        "cfg.d_head = D_HEAD\n",
        "cfg.set_seed(673023)\n",
        "cfg.batch_size = batch_size\n",
        "\n",
        "print(\"cfg.repo_name=\"+cfg.repo_name, \"cfg.model_name=\"+cfg.model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfjPZrQmjaHz"
      },
      "outputs": [],
      "source": [
        "model_title = \"BM\" + str(model_num) + \".CS\" + str(model_cs_num)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjjpDQ2_cS2-"
      },
      "source": [
        "# Generate test data and experiment runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76YZ8VErsHjP"
      },
      "outputs": [],
      "source": [
        "# Generate a batch of prompts\n",
        "def generate_batch():\n",
        "    if constant_num_tokens:\n",
        "        # Less variability. Each example has same number of tokens and\n",
        "        # instructions \"show me the {eng_fields} from the {table_name}\"\n",
        "        generator = qts.CorruptFeatureTestGenerator(\n",
        "            model_num=model_num,\n",
        "            cs_num=prompt_cs_num,\n",
        "            tokenizer=model.tokenizer,\n",
        "            use_novel_names=False,\n",
        "            use_synonyms_field=False,\n",
        "            use_synonyms_table=use_synonyms)\n",
        "\n",
        "        # Generates a batch of examples with the same number of tokens\n",
        "        return generator.generate_feature_examples(\n",
        "            qts.ENGTABLENAME,   # Not important as we will not be using the \"Corrupt\"\n",
        "            batch_size)\n",
        "    else:\n",
        "        # Generates a batch of examples with different number of tokens per example, and 3 field names\n",
        "        return qts.generate_csn(batch_size=batch_size, csn=prompt_cs_num, min_cols=3, max_cols=3, use_synonyms=use_synonyms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBS0v8Gir-d4"
      },
      "outputs": [],
      "source": [
        "# Check if the model can correctly predict the question's answer\n",
        "def run_attention_experiment(run_layer_idx, run_head_idx, run_token_idx, all_tokens, run_prompt_tokens, run_generate, make_changes = True, extra_tokens=0):\n",
        "\n",
        "    if run_generate == 0:\n",
        "        return True, \"\", \"\"\n",
        "\n",
        "\n",
        "    # Check head indices against model architecture\n",
        "    assert run_layer_idx < N_LAYERS, f\"Layer index {run_layer_idx} exceeds N_LAYERS {N_LAYERS}\"\n",
        "    assert run_head_idx < N_HEADS, f\"Head index {run_head_idx} exceeds N_HEADS {N_HEADS}\"\n",
        "\n",
        "    # Check input validity\n",
        "    assert isinstance(all_tokens, (list, torch.Tensor)), \"all_tokens must be a list or tensor\"\n",
        "    assert run_layer_idx >= 0, \"Layer index must be non-negative\"\n",
        "    assert run_head_idx >= 0, \"Head index must be non-negative\"\n",
        "    assert run_token_idx >= 0, \"Token index must be non-negative\"\n",
        "\n",
        "    # Check token indices are valid\n",
        "    assert run_token_idx < len(all_tokens), f\"Token index {run_token_idx} exceeds sequence length {len(all_tokens)}\"\n",
        "    assert run_prompt_tokens <= len(all_tokens), \"Prompt tokens cannot exceed total tokens\"\n",
        "\n",
        "    assert len(all_tokens) == run_prompt_tokens + run_generate\n",
        "\n",
        "\n",
        "    start = run_head_idx * D_HEAD\n",
        "    end = start + D_HEAD\n",
        "\n",
        "\n",
        "    sample_head_output = None\n",
        "    sample_head_slice = None\n",
        "\n",
        "    with model.generate(all_tokens[:run_prompt_tokens], max_new_tokens=run_generate + extra_tokens, pad_token_id=model.tokenizer.eos_token_id):\n",
        "\n",
        "        head_output = model.transformer.h[run_layer_idx].output[0]\n",
        "\n",
        "        if sample_head_output is None:\n",
        "            sample_head_output = head_output.save()\n",
        "\n",
        "        head_slice = head_output[:, run_token_idx, start:end]\n",
        "\n",
        "        if sample_head_slice is None:\n",
        "            sample_head_slice = head_slice.save()\n",
        "\n",
        "        if make_changes:\n",
        "            mean_value = head_slice.mean()\n",
        "            model.transformer.h[run_layer_idx].output[0][:, run_token_idx, start:end] = mean_value\n",
        "\n",
        "        final_output = model.generator.output.save()\n",
        "\n",
        "\n",
        "    if sample_head_output is not None:\n",
        "        # Verify head_output dimensions match model architecture\n",
        "        size = sample_head_output.shape[-1]\n",
        "        assert size == D_MODEL, f\"Hidden dimension {size} doesn't match D_MODEL {D_MODEL}\"\n",
        "\n",
        "        # Verify the slice we're taking has the expected size\n",
        "        size = sample_head_slice.shape[-1]\n",
        "        assert size == D_HEAD, f\"Head slice dimension {size} doesn't match D_HEAD {D_HEAD}\"\n",
        "\n",
        "    # Verify output dimensions\n",
        "    assert len(final_output) > 0, \"Empty model output\"\n",
        "    assert isinstance(final_output[0], (list, torch.Tensor)), \"Output must be a list or tensor\"\n",
        "\n",
        "\n",
        "    # Did the output change?\n",
        "    decoded_input = model.tokenizer.decode(all_tokens, skip_special_tokens=True)\n",
        "    decoded_output = model.tokenizer.decode(final_output[0], skip_special_tokens=True)\n",
        "    same = decoded_input==decoded_output\n",
        "\n",
        "    return same, decoded_input, decoded_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1Ze2PY7Nber"
      },
      "outputs": [],
      "source": [
        "# Check if the model can correctly predict the question's answer and print result\n",
        "def check_experiment(prompt, answer, all_tokens, run_prompt_tokens, run_generate, make_changes = True, show_same = True):\n",
        "\n",
        "    same, decoded_input, decoded_output = run_attention_experiment(0, 0, 0, all_tokens, run_prompt_tokens, run_generate, make_changes, extra_tokens=0)\n",
        "\n",
        "    if not same:\n",
        "        print( \"Ignoring example that model doesn't predict correctly:\")\n",
        "\n",
        "    if (not same) or show_same:\n",
        "        if show_same:\n",
        "            print(\"Example:\")\n",
        "        print(\"Prompt (\"+str(run_prompt_tokens)+\"):\", prompt)\n",
        "        print(\"Answer (\"+str(run_generate)+\") :\", answer)\n",
        "        print(\"Output     :\", decoded_output)\n",
        "        #print(len(decoded_input), len(decoded_output))\n",
        "        print()\n",
        "\n",
        "    return same, decoded_input, decoded_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuCHN1hWfeBk"
      },
      "outputs": [],
      "source": [
        "# Check that generators work and models are accurate\n",
        "def check_generator_on_clean_input(title, examples):\n",
        "    num_good = 0\n",
        "    for example in examples:\n",
        "        prompt = example.get_alpaca_prompt()\n",
        "        answer = example.sql_statement\n",
        "\n",
        "        prompt_tokens = model.tokenizer(prompt)[\"input_ids\"]\n",
        "        answer_tokens = model.tokenizer(answer)[\"input_ids\"]\n",
        "        all_tokens = prompt_tokens + answer_tokens\n",
        "\n",
        "        num_answer_tokens = len(answer_tokens)\n",
        "        num_prompt_tokens = len(prompt_tokens)\n",
        "\n",
        "        same, _, decoded_output = check_experiment(prompt, answer, all_tokens, num_prompt_tokens, num_answer_tokens, make_changes = False, show_same = False)\n",
        "        if same:\n",
        "            num_good += 1\n",
        "\n",
        "    print(\"Use synonyms:\", use_synonyms, \"Constant num tokens:\", constant_num_tokens, \"#examples:\", len(examples), \"num_good:\", num_good )\n",
        "\n",
        "\n",
        "check_generator_on_clean_input( \"generate_csn\", generate_batch() )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ju8wtYel8xRm"
      },
      "outputs": [],
      "source": [
        "# Return a list of experiments to run\n",
        "def get_experiment_list(examples, by_attention_head):\n",
        "    run_list = []\n",
        "    max_prompt_tokens = 0\n",
        "    max_answer_tokens = 0\n",
        "    max_tokens = 0\n",
        "    show_examples = 3\n",
        "\n",
        "    for example in examples:\n",
        "        prompt = example.get_alpaca_prompt()\n",
        "        answer = example.sql_statement\n",
        "\n",
        "        prompt_tokens = model.tokenizer(prompt)[\"input_ids\"]\n",
        "        answer_tokens = model.tokenizer(answer)[\"input_ids\"]\n",
        "        all_tokens = prompt_tokens + answer_tokens\n",
        "\n",
        "        num_answer_tokens = len(answer_tokens)\n",
        "        num_prompt_tokens = len(prompt_tokens)\n",
        "        num_tokens = len(all_tokens)\n",
        "        assert num_tokens == num_prompt_tokens + num_answer_tokens\n",
        "\n",
        "        max_prompt_tokens = max(max_prompt_tokens, num_prompt_tokens)\n",
        "        max_answer_tokens = max(max_answer_tokens, num_answer_tokens)\n",
        "        max_tokens = max(max_tokens, num_tokens)\n",
        "\n",
        "        # Check that the model can correctly predict the question's answer\n",
        "        same, _, decoded_output = check_experiment(prompt, answer, all_tokens, num_prompt_tokens, num_answer_tokens, make_changes = False, show_same = show_examples > 0)\n",
        "\n",
        "        if not same:\n",
        "            continue\n",
        "        show_examples -= 1\n",
        "\n",
        "        if by_attention_head:\n",
        "            for layer_idx in range(N_LAYERS):\n",
        "                for head_idx in range(N_HEADS):\n",
        "                    for token_idx in range(num_tokens):\n",
        "                        # Important logic:\n",
        "                        # num_prompt_tokens and num_answer_tokens are the normal interpretation of the prompt/answer sizes.\n",
        "                        # If ablating a token in the MIDDLE of the PROMPT, we provide ALL the prompt tokens, and generate num_answer_tokens.\n",
        "                        # If ablating a token in the MIDDLE of the ANSWER, we increase the size of the \"prompt\" and decrease the generated tokens.\n",
        "                        exp_prompt_tokens = max(num_prompt_tokens, token_idx+1)\n",
        "                        exp_num_generate = num_tokens - exp_prompt_tokens\n",
        "\n",
        "                        assert num_tokens == exp_prompt_tokens + exp_num_generate\n",
        "\n",
        "                        run_list.append([layer_idx, head_idx, token_idx, all_tokens, exp_prompt_tokens, exp_num_generate])\n",
        "        else:\n",
        "            for token_idx in range(num_prompt_tokens):\n",
        "                run_list.append([prompt, answer, token_idx, prompt_tokens])\n",
        "\n",
        "    return run_list, max_prompt_tokens, max_answer_tokens, max_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wdUVEfAcXJa"
      },
      "source": [
        "# Which token positions are useful?\n",
        "This information is used to shrink the size of search spaces in following sections. For the SQL model all token positions are useful"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETOrSFQqVSG6"
      },
      "outputs": [],
      "source": [
        "def run_token_experiments( ):\n",
        "    examples = generate_batch()\n",
        "    run_list, max_prompt_tokens, max_good_answer_tokens, max_tokens = get_experiment_list(examples, False)\n",
        "    cfg.initialize_token_positions( max_prompt_tokens, max_good_answer_tokens, True )\n",
        "    num_exps = len(run_list)\n",
        "\n",
        "    print(\"batch_size=\"+str(batch_size), \"max_prompt_tokens=\"+str(max_prompt_tokens), \"max_good_answer_tokens=\"+str(max_good_answer_tokens), \"max_tokens=\"+str(max_tokens), \"num_exps=\"+str(num_exps))\n",
        "\n",
        "    try_results = np.zeros(max_tokens, dtype=int)\n",
        "    fail_results = np.zeros(max_tokens, dtype=int)\n",
        "\n",
        "    for item_num in tqdm.tqdm(range(num_exps)):\n",
        "\n",
        "        run_item = run_list[item_num]\n",
        "        run_prompt, run_answer, run_token_idx, prompt_tokens = run_item\n",
        "\n",
        "        with model.generate(prompt_tokens, max_new_tokens=max_good_answer_tokens, pad_token_id=model.tokenizer.eos_token_id) :\n",
        "\n",
        "            # Ablate the portion of the output corresponding to this token position\n",
        "            for run_layer_idx in range(N_LAYERS):\n",
        "                layer_output = model.transformer.h[run_layer_idx].output[0]\n",
        "                mean_activation = layer_output.mean(dim=1, keepdim=True)  # Mean across sequence\n",
        "                # Replace target token with mean activation\n",
        "                layer_output[:, run_token_idx, :] = mean_activation.squeeze(1)\n",
        "\n",
        "            final_output = model.generator.output.save()\n",
        "\n",
        "        decoded_output = model.tokenizer.decode(final_output[0], skip_special_tokens=True)\n",
        "\n",
        "        # Did the output change?\n",
        "        if run_prompt + run_answer != decoded_output:\n",
        "            fail_results[run_token_idx] += 1\n",
        "        try_results[run_token_idx] += 1\n",
        "\n",
        "    # Compute the failure rate as percentage\n",
        "    failure_rate = (1.0 * fail_results / (try_results + 1e-10)) * 100\n",
        "    failure_rate = np.round(failure_rate, 2)\n",
        "\n",
        "    return max_tokens, fail_results, failure_rate\n",
        "\n",
        "g_max_tokens, g_token_fail_results, g_token_failure_rate = run_token_experiments()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RT7zG1EDdA5C"
      },
      "outputs": [],
      "source": [
        "print( \"Useful token positions:\" )\n",
        "for token_idx in range(len(g_token_failure_rate)):\n",
        "    if g_token_failure_rate[token_idx] > 0 :\n",
        "        cfg.add_useful_position(token_idx)\n",
        "        print( \"Position:\", token_idx, \"    % Fails:\", g_token_failure_rate[token_idx], \"    # Fails:\", g_token_fail_results[token_idx] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMs7MpPocb6_"
      },
      "source": [
        "# Which token+layer+attention head nodes are useful?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bINHUagozRoo"
      },
      "outputs": [],
      "source": [
        "def run_attention_experiments():\n",
        "    show_diff = True\n",
        "    examples = generate_batch()\n",
        "    run_list, max_prompt_tokens, max_good_answer_tokens, max_tokens = get_experiment_list(examples, True)\n",
        "    num_exps = len(run_list)\n",
        "\n",
        "    print(\"batch_size=\"+str(batch_size), \"max_prompt_tokens=\"+str(max_prompt_tokens), \"max_good_answer_tokens=\"+str(max_good_answer_tokens), \"max_tokens=\"+str(max_tokens), \"num_exps=\"+str(num_exps))\n",
        "\n",
        "    try_results = np.zeros((N_LAYERS, N_HEADS, max_tokens), dtype=int)\n",
        "    fail_results = np.zeros((N_LAYERS, N_HEADS, max_tokens), dtype=int)\n",
        "\n",
        "    for item_num in tqdm.tqdm(range(num_exps)):\n",
        "\n",
        "        run_item = run_list[item_num]\n",
        "        run_layer_idx, run_head_idx, run_token_idx, all_tokens, num_prompt_tokens, num_generate = run_item\n",
        "\n",
        "        same, decoded_input, decoded_output = run_attention_experiment(run_layer_idx, run_head_idx, run_token_idx, all_tokens, num_prompt_tokens, num_generate)\n",
        "\n",
        "        if not same:\n",
        "            fail_results[run_layer_idx, run_head_idx, run_token_idx] += 1\n",
        "            if show_diff:\n",
        "                print(\"Failure when intervening:\", \"Layer=\"+str(run_layer_idx), \"Head=\"+str(run_head_idx), \"Pos=\"+str(run_token_idx), \"NumPrompts=\"+str(num_prompt_tokens), \"NumGenerate=\"+str(num_generate))\n",
        "                print(\"Input :\", decoded_input)\n",
        "                print(\"Output:\", decoded_output)\n",
        "                show_diff = False\n",
        "        try_results[run_layer_idx, run_head_idx, run_token_idx] += 1\n",
        "\n",
        "\n",
        "    # Compute the failure rate as percentage\n",
        "    failure_rate = (1.0 * fail_results / (try_results + 1e-10)) * 100\n",
        "    failure_rate = np.round(failure_rate, 2)\n",
        "\n",
        "    return max_tokens, fail_results, failure_rate, try_results\n",
        "\n",
        "g_max_tokens, g_attn_failure_results, g_attn_failure_rate, g_attn_try_results = run_attention_experiments()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nViXbBVDxUt"
      },
      "outputs": [],
      "source": [
        "cfg.useful_nodes = qmi.UsefulNodeList()\n",
        "for layer_idx in range(N_LAYERS):\n",
        "    for head_idx in range(N_HEADS):\n",
        "        for token_idx in range(g_max_tokens):\n",
        "            fail_perc = int(g_attn_failure_rate[layer_idx, head_idx, token_idx])\n",
        "            if fail_perc > 0 :\n",
        "                # Add percentage failure quanta\n",
        "                node_location = qmi.NodeLocation(token_idx, layer_idx, True, head_idx)\n",
        "                cfg.add_useful_node_tag( node_location, qmi.QType.FAIL.value, str(fail_perc) )\n",
        "\n",
        "cfg.useful_nodes.sort_nodes()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2lbWJrUwY7p"
      },
      "outputs": [],
      "source": [
        "print( \"Use synonyms:\", use_synonyms, \"Constant num tokens:\", constant_num_tokens )\n",
        "\n",
        "for layer_idx in range(N_LAYERS):\n",
        "    title = model_title + \" vs CS\" + str(prompt_cs_num) + \" prompts: % change, mean ablation, L\" + str(layer_idx)\n",
        "    plt.imshow(g_attn_failure_rate[layer_idx], cmap=\"viridis\", aspect=\"auto\")\n",
        "    plt.colorbar(label=\"Percentage Change\")\n",
        "    plt.xlabel(\"Token Position\")\n",
        "    plt.ylabel(\"Attention Head\")\n",
        "    plt.title(title)\n",
        "\n",
        "    plt.savefig(f\"{model_title}_layer_{layer_idx}.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAHRK6TOmMac"
      },
      "outputs": [],
      "source": [
        "# cfg.useful_nodes.print_node_tags()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCAu8QO1mNB0"
      },
      "outputs": [],
      "source": [
        "title = model_title + \" useful attention heads\"\n",
        "ax1, quanta_results, num_results = qmi.calc_quanta_map(\n",
        "    cfg, True, 6,\n",
        "    cfg.useful_nodes, qmi.QType.FAIL.value, \"\", qmi.get_quanta_fail_perc,\n",
        "    combine_identical_cells=False)\n",
        "\n",
        "if num_results > 0:\n",
        "    if cfg.graph_file_suffix > \"\":\n",
        "        print(\"Saving quanta map:\", title)\n",
        "        qmi.save_plt_to_file(cfg=cfg, full_title=title)\n",
        "    else:\n",
        "        ax1.set_title(title + ' ({} nodes)'.format(len(quanta_results)))\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_szu6Trokt2"
      },
      "outputs": [],
      "source": [
        "# Serialize and save the useful nodes list to a temporary CoLab file in JSON format. Manually download.\n",
        "useful_node_json_filename = 'tinysql_bm' + str(model_num) + \"_cs\" + str(model_cs_num) + '_useful_nodes.json'\n",
        "print( \"Saving useful node list with behavior tags:\", useful_node_json_filename)\n",
        "cfg.useful_nodes.save_nodes(useful_node_json_filename)\n",
        "\n",
        "#TODO: Auto save to Martian wandb"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}