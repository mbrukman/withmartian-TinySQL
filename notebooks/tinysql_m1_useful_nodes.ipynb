{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQAsYjSQVXHC"
      },
      "source": [
        "# M1 useful nodes\n",
        "This notebook identifies M1 nodes (attention heads and MLPs) that, when ablated, cause a decrease in model prediction accuracy. These nodes are needed (aka useful) for accurate predictions.\n",
        "\n",
        "\n",
        "This notebook was:\n",
        "- Developed on Google Colab using an **T4**\n",
        "- Runs with M1 (TinyStories) with base/CS1/CS2/CS3.\n",
        "- Requires a GITHUB_TOKEN secret to access Martian quanta_text_to_sql code repository.\n",
        "- Requires a HF_TOKEN secret to access Martian HuggingFace repository.\n",
        "\n",
        "This notebook relies on the nnsight library. Useful background:\n",
        "- https://nnsight.net/notebooks/tutorials/walkthrough/#Batching\n",
        "- https://nnsight.net/notebooks/tutorials/walkthrough/#Looping\n",
        "\n",
        "This notebook relies on the https://github.com/PhilipQuirke/quanta_mech_interp library for graphing useful nodes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1tQZgLUnojc"
      },
      "source": [
        "# Import libraries\n",
        "Imports standard libraries. Do not read."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qt2jt2bHcY2v"
      },
      "outputs": [],
      "source": [
        "# https://nnsight.net/\n",
        "# Access 0.4 prerelease version (as at Dec 2024)\n",
        "#!pip install nnsight==0.4.0.dev0\n",
        "!pip install -U nnsight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vif7qLNrlC0P"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import einops\n",
        "import torch\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = \"colab\"\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm.auto as tqdm\n",
        "\n",
        "import nnsight\n",
        "from nnsight import LanguageModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orVn0wTnosHO"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "from google.colab import userdata\n",
        "import gc\n",
        "import weakref"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade git+https://github.com/PhilipQuirke/quanta_mech_interp.git\n",
        "import QuantaMechInterp as qmi"
      ],
      "metadata": {
        "id": "i1zClTbuyFUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMWEb8TJoske"
      },
      "outputs": [],
      "source": [
        "github_token = userdata.get(\"GITHUB_TOKEN\")\n",
        "\n",
        "# Install the private repository using the token\n",
        "!pip install --upgrade git+https://{github_token}@github.com/withmartian/quanta_text_to_sql.git\n",
        "\n",
        "import QuantaTextToSql as qts"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Select model, command set and feature to investigate\n"
      ],
      "metadata": {
        "id": "s6k59y0OZhp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_num = 1                 # 1=TinyStories, 2=Qwen, 3=Llama, 4=Granite, 5=SmolLM\n",
        "cs_num = 1                    # 0=BaseModel, 1=CS1, 2=CS2 or 3=CS3"
      ],
      "metadata": {
        "id": "rl1akjeAUfoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y72URtyynvnv"
      },
      "source": [
        "# Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFaFTy6LnbxI"
      },
      "outputs": [],
      "source": [
        "model = qts.load_tinysql_model(model_num, cs_num, auth_token=userdata.get(\"HF_TOKEN\"))\n",
        "clear_output()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "N_LAYERS, N_HEADS, D_MODEL, D_HEAD = qts.get_model_sizes(model_num, model)"
      ],
      "metadata": {
        "id": "sRL4ckXZqElr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_hf_name = qts.sql_interp_model_location(model_num, cs_num)\n",
        "\n",
        "# Singleton QuantaTool \"main\" configuration class. qmi.AlgoConfig is derived from the chain qmi.UsefulConfig > qmi.ModelConfig\n",
        "cfg = qmi.AlgoConfig()\n",
        "cfg.repo_name, cfg.model_name = model_hf_name.rsplit(\"/\", 1)\n",
        "cfg.main_model = model\n",
        "cfg.n_layers = N_LAYERS\n",
        "cfg.n_heads = N_HEADS\n",
        "cfg.d_model = D_MODEL\n",
        "cfg.d_head = D_HEAD\n",
        "cfg.file_config_prefix = \"\"\n",
        "\n",
        "print(\"cfg.repo_name=\"+cfg.repo_name, \"cfg.model_name=\"+cfg.model_name)"
      ],
      "metadata": {
        "id": "uTJ2LHzo5flR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate test data and experiment runs"
      ],
      "metadata": {
        "id": "XjjpDQ2_cS2-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76YZ8VErsHjP"
      },
      "outputs": [],
      "source": [
        "# Generate a batch of prompts with 3 field names\n",
        "def generate_batch(batch_size):\n",
        "    cfg.batch_size = batch_size\n",
        "\n",
        "    return qts.generate_csn(batch_size=batch_size, csn=max(1,cs_num), min_cols=3, max_cols=3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def same_string(title, token_idx, input_tokens, output_tokens):\n",
        "  decoded_input = model.tokenizer.decode(input_tokens, skip_special_tokens=True)\n",
        "  decoded_output = model.tokenizer.decode(output_tokens, skip_special_tokens=True)\n",
        "  same = decoded_input==decoded_output\n",
        "  if not same:\n",
        "      print(title, \": token_idx=\"+str(token_idx), \": Same=\"+str(same))"
      ],
      "metadata": {
        "id": "wAhtYvAZAoml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_attention_experiment(run_layer_idx, run_head_idx, run_token_idx, all_tokens, num_prompt_tokens, num_generate, make_changes = True):\n",
        "\n",
        "    num_tokens = len(all_tokens)\n",
        "    assert num_tokens == num_prompt_tokens + num_generate\n",
        "\n",
        "    start = run_head_idx * D_HEAD\n",
        "    end = (run_head_idx + 1) * D_HEAD\n",
        "\n",
        "    if not make_changes:\n",
        "         with model.generate(all_tokens[:num_prompt_tokens], max_new_tokens=num_generate,\n",
        "                          pad_token_id=model.tokenizer.eos_token_id) as tracer:\n",
        "\n",
        "            final_output = model.generator.output.save()\n",
        "\n",
        "    elif run_token_idx < num_prompt_tokens:\n",
        "        with model.generate(all_tokens[:num_prompt_tokens], max_new_tokens=num_generate,\n",
        "                          pad_token_id=model.tokenizer.eos_token_id) as tracer:\n",
        "\n",
        "            # Zero out just the portion of the prompt corresponding to this head\n",
        "            model.transformer.h[run_layer_idx].output[0][:, run_token_idx, start:end] = 0\n",
        "\n",
        "            final_output = model.generator.output.save()\n",
        "\n",
        "    else:\n",
        "        assert num_prompt_tokens == run_token_idx\n",
        "        if False: # This doesnt seem to work\n",
        "            with model.generate(all_tokens[:num_prompt_tokens], max_new_tokens=num_generate,\n",
        "                              pad_token_id=model.tokenizer.eos_token_id) as tracer:\n",
        "\n",
        "                for next_idx in range(num_generate):\n",
        "                    model.transformer.h[run_layer_idx].next()\n",
        "\n",
        "                # Zero out just the portion of the output corresponding to this head\n",
        "                # PQR: Runs but output is never corrupted. This code run inside above for loop when next_idx == 0 crashes. Sigh\n",
        "                model.transformer.h[run_layer_idx].output[0][:, run_token_idx, start:end] = 0\n",
        "\n",
        "                final_output = model.generator.output.save()\n",
        "        elif num_generate > 1:\n",
        "            # Try this.\n",
        "            with model.generate(all_tokens[:num_prompt_tokens+1], max_new_tokens=num_generate-1,\n",
        "                              pad_token_id=model.tokenizer.eos_token_id) as tracer:\n",
        "\n",
        "                # Zero out just the portion of the prompt corresponding to this head\n",
        "                model.transformer.h[run_layer_idx].output[0][:, run_token_idx, start:end] = 0\n",
        "\n",
        "                final_output = model.generator.output.save()\n",
        "        else:\n",
        "            return True, \"\", \"\"\n",
        "\n",
        "    #final_output = final_output.detach().cpu().numpy()\n",
        "\n",
        "    # Did the output change?\n",
        "    decoded_input = model.tokenizer.decode(all_tokens, skip_special_tokens=True)\n",
        "    decoded_output = model.tokenizer.decode(final_output[0], skip_special_tokens=True)\n",
        "    same = decoded_input==decoded_output\n",
        "\n",
        "    return same, decoded_input, decoded_output"
      ],
      "metadata": {
        "id": "qBS0v8Gir-d4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Return a list of experiments to run\n",
        "def get_experiment_list(examples, by_attention_head):\n",
        "    run_list = []\n",
        "    max_prompt_tokens = 0\n",
        "    max_answer_tokens = 0\n",
        "    max_tokens = 0\n",
        "    show_one_example = True\n",
        "\n",
        "    for example in examples:\n",
        "        prompt = example.get_alpaca_prompt()\n",
        "        answer = example.sql_statement\n",
        "\n",
        "        prompt_tokens = model.tokenizer(prompt)[\"input_ids\"]\n",
        "        answer_tokens = model.tokenizer(answer)[\"input_ids\"]\n",
        "        all_tokens = prompt_tokens + answer_tokens\n",
        "\n",
        "        num_answer_tokens = len(answer_tokens)\n",
        "        num_prompt_tokens = len(prompt_tokens)\n",
        "        num_tokens = len(all_tokens)\n",
        "        assert num_tokens == num_prompt_tokens + num_answer_tokens\n",
        "\n",
        "        max_prompt_tokens = max(max_prompt_tokens, num_prompt_tokens)\n",
        "        max_answer_tokens = max(max_answer_tokens, num_answer_tokens)\n",
        "        max_tokens = max(max_tokens, num_tokens)\n",
        "\n",
        "        # Check that the model can predict the question\n",
        "        same, _, decoded_output = run_attention_experiment(0, 0, 0, all_tokens, num_prompt_tokens, num_answer_tokens, make_changes = False)\n",
        "        if not same:\n",
        "            print( \"Ignoring example that model doesn't predict correctly:\", prompt.replace('\\n', ' '), answer.replace('\\n', ' ') )\n",
        "            continue\n",
        "        elif show_one_example:\n",
        "            print(\"Example model predict correctly:\", prompt.replace('\\n', ' '), answer.replace('\\n', ' ') )\n",
        "            show_one_example = False;\n",
        "\n",
        "        if by_attention_head:\n",
        "            for layer_idx in range(N_LAYERS):\n",
        "                for head_idx in range(N_HEADS):\n",
        "                    for token_idx in range(num_tokens):\n",
        "                        exp_prompt_tokens = max(num_prompt_tokens, token_idx)\n",
        "                        exp_num_generate = num_tokens - exp_prompt_tokens\n",
        "                        assert num_tokens == exp_prompt_tokens + exp_num_generate\n",
        "\n",
        "                        run_list.append([layer_idx, head_idx, token_idx, all_tokens, exp_prompt_tokens, exp_num_generate])\n",
        "        else:\n",
        "            for token_idx in range(num_prompt_tokens):\n",
        "                run_list.append([prompt, answer, token_idx, prompt_tokens])\n",
        "\n",
        "    return run_list, max_prompt_tokens, max_answer_tokens, max_tokens"
      ],
      "metadata": {
        "id": "Ju8wtYel8xRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Which token positions are useful?\n",
        "This information is used to shrink the size of search spaces in following sections. For the SQL model all token positions are useful"
      ],
      "metadata": {
        "id": "5wdUVEfAcXJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_token_positions_useful = True"
      ],
      "metadata": {
        "id": "rDX9JLzKl2oL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_BATCH = 50\n",
        "\n",
        "def run_token_experiments( ):\n",
        "    examples = generate_batch(N_BATCH)\n",
        "    run_list, max_prompt_tokens, max_good_answer_tokens, max_tokens = get_experiment_list(examples, False)\n",
        "    cfg.initialize_token_positions( max_prompt_tokens, max_good_answer_tokens, True )\n",
        "    num_exps = len(run_list)\n",
        "\n",
        "    print(\"N_BATCH=\"+str(N_BATCH), \"max_prompt_tokens=\"+str(max_prompt_tokens), \"max_good_answer_tokens=\"+str(max_good_answer_tokens), \"max_tokens=\"+str(max_tokens), \"num_exps=\"+str(num_exps))\n",
        "\n",
        "    try_results = np.zeros(max_tokens, dtype=int)\n",
        "    fail_results = np.zeros(max_tokens, dtype=int)\n",
        "\n",
        "    if not all_token_positions_useful:\n",
        "        for item_num in tqdm.tqdm(range(num_exps)):\n",
        "\n",
        "            run_item = run_list[item_num]\n",
        "            run_prompt, run_answer, run_token_idx, prompt_tokens = run_item\n",
        "\n",
        "            with model.generate(prompt_tokens, max_new_tokens=max_good_answer_tokens,\n",
        "                              pad_token_id=model.tokenizer.eos_token_id) as tracer:\n",
        "\n",
        "                # Zero out just the portion of the output corresponding to this token position\n",
        "                for run_layer_idx in range(N_LAYERS):\n",
        "                    model.transformer.h[run_layer_idx].output[0][:, run_token_idx, :] = 0\n",
        "\n",
        "                final_output = model.generator.output.save()\n",
        "\n",
        "            final_output = final_output.detach().cpu().numpy()\n",
        "            decoded_output = model.tokenizer.decode(final_output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Did the output change?\n",
        "            try_results[run_token_idx] += 1\n",
        "            if run_prompt + run_answer != decoded_output:\n",
        "                #print(\"Input:\", item_num, run_prompt.replace('\\n', ' '), run_answer.replace('\\n', ' '))\n",
        "                #print(\"Output:\", item_num, decoded_output.replace('\\n', ' '))\n",
        "                fail_results[run_token_idx] += 1\n",
        "\n",
        "    # Compute the failure rate as percentage\n",
        "    failure_rate = (1.0 * fail_results / (try_results + 1e-10)) * 100\n",
        "    failure_rate = np.round(failure_rate, 2)\n",
        "\n",
        "    return max_tokens, fail_results, failure_rate\n",
        "\n",
        "g_max_tokens, g_token_fail_results, g_token_failure_rate = run_token_experiments()"
      ],
      "metadata": {
        "id": "ETOrSFQqVSG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if all_token_positions_useful:\n",
        "    for token_idx in range(g_max_tokens):\n",
        "        cfg.add_useful_position(token_idx)\n",
        "else:\n",
        "    print( \"Useful token positions:\" )\n",
        "    for token_idx in range(len(g_token_failure_rate)):\n",
        "        if g_token_failure_rate[token_idx] > 0 :\n",
        "            cfg.add_useful_position(token_idx)\n",
        "            print( \"Position:\", token_idx, \"% Fails:\", g_token_failure_rate[token_idx], \"# Fails:\", g_token_fail_results[token_idx] )"
      ],
      "metadata": {
        "id": "RT7zG1EDdA5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if False:\n",
        "    cfg.calc_position_failures_map(g_token_fail_results.tolist())\n",
        "    qmi.save_plt_to_file(cfg=cfg, full_title=\"Failures When Position Ablated\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "wZ0aAic8jqiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Which token+layer+attention head nodes are useful?"
      ],
      "metadata": {
        "id": "GMs7MpPocb6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_BATCH = 5\n",
        "\n",
        "def run_attention_experiments():\n",
        "    show_diff = True\n",
        "    examples = generate_batch(N_BATCH)\n",
        "    run_list, max_prompt_tokens, max_good_answer_tokens, max_tokens = get_experiment_list(examples, True)\n",
        "    num_exps = len(run_list)\n",
        "\n",
        "    print(\"N_BATCH=\"+str(N_BATCH), \"max_prompt_tokens=\"+str(max_prompt_tokens), \"max_answer_tokens=\"+str(max_good_answer_tokens), \"max_tokens=\"+str(max_tokens), \"num_exps=\"+str(num_exps))\n",
        "\n",
        "    try_results = np.zeros((N_LAYERS, N_HEADS, max_tokens), dtype=int)\n",
        "    fail_results = np.zeros((N_LAYERS, N_HEADS, max_tokens), dtype=int)\n",
        "\n",
        "    for item_num in tqdm.tqdm(range(num_exps)):\n",
        "\n",
        "        run_item = run_list[item_num]\n",
        "        run_layer_idx, run_head_idx, run_token_idx, all_tokens, num_prompt_tokens, num_generate = run_item\n",
        "\n",
        "        same, decoded_input, decoded_output = run_attention_experiment(run_layer_idx, run_head_idx, run_token_idx, all_tokens, num_prompt_tokens, num_generate, make_changes=False)\n",
        "        if not same:\n",
        "            print(\"Unexpected failure when no intervention\", run_layer_idx, run_head_idx, run_token_idx, num_prompt_tokens, num_generate)\n",
        "            print(\"Input :\", decoded_input.replace('\\n', ' '))\n",
        "            print(\"Output:\", decoded_output.replace('\\n', ' '))\n",
        "            assert False\n",
        "\n",
        "        same, decoded_input, decoded_output = run_attention_experiment(run_layer_idx, run_head_idx, run_token_idx, all_tokens, num_prompt_tokens, num_generate)\n",
        "\n",
        "        if not same:\n",
        "            fail_results[run_layer_idx, run_head_idx, run_token_idx] += 1\n",
        "            if show_diff:\n",
        "                print(\"Failure when intervening\", run_layer_idx, run_head_idx, run_token_idx, num_prompt_tokens, num_generate)\n",
        "                print(\"Input :\", decoded_input.replace('\\n', ' '))\n",
        "                print(\"Output:\", decoded_output.replace('\\n', ' '))\n",
        "                show_diff = False\n",
        "        try_results[run_layer_idx, run_head_idx, run_token_idx] += 1\n",
        "\n",
        "\n",
        "    # Compute the failure rate as percentage\n",
        "    failure_rate = (1.0 * fail_results / (try_results + 1e-10)) * 100\n",
        "    failure_rate = np.round(failure_rate, 2)\n",
        "\n",
        "    return max_tokens, fail_results, failure_rate, try_results\n",
        "\n",
        "g_max_tokens, g_attn_failure_results, g_attn_failure_rate, g_attn_try_results = run_attention_experiments()"
      ],
      "metadata": {
        "id": "bINHUagozRoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg.useful_nodes = qmi.UsefulNodeList()\n",
        "for layer_idx in range(N_LAYERS):\n",
        "    for head_idx in range(N_HEADS):\n",
        "        for token_idx in range(g_max_tokens):\n",
        "            fail_perc = int(g_attn_failure_rate[layer_idx, head_idx, token_idx])\n",
        "            if fail_perc > 0 :\n",
        "                # Add percentage failure quanta\n",
        "                node_location = qmi.NodeLocation(token_idx, layer_idx, True, head_idx)\n",
        "                cfg.add_useful_node_tag( node_location, qmi.QType.FAIL.value, str(fail_perc) )\n",
        "\n",
        "cfg.useful_nodes.sort_nodes()"
      ],
      "metadata": {
        "id": "1nViXbBVDxUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer_idx in range(N_LAYERS):\n",
        "    plt.imshow(g_attn_failure_rate[layer_idx], cmap=\"viridis\", aspect=\"auto\")\n",
        "    plt.colorbar(label=\"Percentage Change\")\n",
        "    plt.xlabel(\"Token Position\")\n",
        "    plt.ylabel(\"Attention Head\")\n",
        "    plt.title(\"Percentage of Output Changes by Zeroing Activations in Layer \" + str(layer_idx))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "O2lbWJrUwY7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg.useful_nodes.print_node_tags()"
      ],
      "metadata": {
        "id": "CAHRK6TOmMac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title = \"Useful attention heads\"\n",
        "ax1, quanta_results, num_results = qmi.calc_quanta_map(\n",
        "    cfg, True, 6,\n",
        "    cfg.useful_nodes, qmi.QType.FAIL.value, \"\", qmi.get_quanta_fail_perc,\n",
        "    combine_identical_cells=False)\n",
        "\n",
        "if num_results > 0:\n",
        "    if cfg.graph_file_suffix > \"\":\n",
        "        print(\"Saving quanta map:\", title)\n",
        "        qmi.save_plt_to_file(cfg=cfg, full_title=title)\n",
        "    else:\n",
        "        ax1.set_title(title + ' ({} nodes)'.format(len(quanta_results)))\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "DCAu8QO1mNB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Serialize and save the useful nodes list to a temporary CoLab file in JSON format\n",
        "main_fname_behavior_json = cfg.model_name + '_behavior.json'\n",
        "print( \"Saving useful node list with behavior tags:\", main_fname_behavior_json)\n",
        "cfg.useful_nodes.save_nodes(main_fname_behavior_json)"
      ],
      "metadata": {
        "id": "2_szu6Trokt2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "z1tQZgLUnojc",
        "5wdUVEfAcXJa"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}