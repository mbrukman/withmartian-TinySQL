{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQAsYjSQVXHC"
      },
      "source": [
        "# M1 useful nodes\n",
        "This notebook identifies M1 nodes (attention heads and MLPs) that, when ablated, cause a decrease in model prediction accuracy. These nodes are needed (aka useful) for accurate predictions.\n",
        "\n",
        "\n",
        "This notebook was:\n",
        "- Developed on Google Colab using an **T4**\n",
        "- Runs with M1 (TinyStories) with base/CS1/CS2/CS3.\n",
        "- Requires a GITHUB_TOKEN secret to access Martian quanta_text_to_sql code repository.\n",
        "- Requires a HF_TOKEN secret to access Martian HuggingFace repository.\n",
        "\n",
        "This notebook relies on the nnsight library. Useful background:\n",
        "- https://nnsight.net/notebooks/tutorials/walkthrough/#Batching\n",
        "- https://nnsight.net/notebooks/tutorials/walkthrough/#Looping\n",
        "\n",
        "This notebook relies on the https://github.com/PhilipQuirke/quanta_mech_interp library for graphing useful nodes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1tQZgLUnojc"
      },
      "source": [
        "# Import libraries\n",
        "Imports standard libraries. Do not read."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qt2jt2bHcY2v"
      },
      "outputs": [],
      "source": [
        "# https://nnsight.net/\n",
        "# Access 0.4 prerelease version (as at Dec 2024)\n",
        "#!pip install nnsight==0.4.0.dev0\n",
        "!pip install -U nnsight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vif7qLNrlC0P"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import einops\n",
        "import torch\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = \"colab\"\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm.auto as tqdm\n",
        "\n",
        "import nnsight\n",
        "from nnsight import LanguageModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orVn0wTnosHO"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "from google.colab import userdata\n",
        "import gc\n",
        "import weakref"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade git+https://github.com/PhilipQuirke/quanta_mech_interp.git\n",
        "import QuantaMechInterp as qmi"
      ],
      "metadata": {
        "id": "i1zClTbuyFUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMWEb8TJoske"
      },
      "outputs": [],
      "source": [
        "github_token = userdata.get(\"GITHUB_TOKEN\")\n",
        "\n",
        "# Install the private repository using the token\n",
        "!pip install --upgrade git+https://{github_token}@github.com/withmartian/quanta_text_to_sql.git\n",
        "\n",
        "import QuantaTextToSql as qts"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Select model, command set and feature to investigate\n"
      ],
      "metadata": {
        "id": "s6k59y0OZhp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_num = 1                 # 1=TinyStories, 2=Qwen, 3=Llama, 4=Granite, 5=SmolLM\n",
        "cs_num = 1                    # 0=BaseModel, 1=CS1, 2=CS2 or 3=CS3"
      ],
      "metadata": {
        "id": "rl1akjeAUfoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y72URtyynvnv"
      },
      "source": [
        "# Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFaFTy6LnbxI"
      },
      "outputs": [],
      "source": [
        "model_hf_name = qts.sql_interp_model_location(model_num, cs_num)\n",
        "\n",
        "if model_num == 1:\n",
        "    the_tokenizer, the_model = qts.load_sql_interp_model(model_num, cs_num, auth_token=userdata.get(\"HF_TOKEN\"), use_flash_attention=False)\n",
        "    model = LanguageModel(the_model, the_tokenizer)\n",
        "    model.tokenizer = the_tokenizer\n",
        "else:\n",
        "    model = LanguageModel(model_hf_name, device_map=\"auto\")\n",
        "\n",
        "clear_output()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model_num == 1:\n",
        "    N_LAYERS = len(model.transformer.h)\n",
        "else:\n",
        "    N_LAYERS = len(model.model.layers)\n",
        "N_HEADS = 16 if model_num == 1 else 7 if model_num == 2 else 16\n",
        "D_MODEL = model.transformer.wte.embedding_dim if model_num == 1 else model.config.hidden_size\n",
        "D_HEAD = D_MODEL // N_HEADS\n",
        "\n",
        "print(\"N_LAYERS=\"+str(N_LAYERS), \"N_HEADS=\"+str(N_HEADS), \"D_MODEL=\"+str(D_MODEL), \"D_HEAD=\"+str(D_HEAD))"
      ],
      "metadata": {
        "id": "sRL4ckXZqElr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Singleton QuantaTool \"main\" configuration class. MathsConfig is derived from the chain AlgoConfig > UsefulConfig > ModelConfig\n",
        "cfg = qmi.AlgoConfig()\n",
        "cfg.model_name = model_hf_name\n",
        "cfg.main_model = model\n",
        "cfg.n_layers = N_LAYERS\n",
        "cfg.n_heads = N_HEADS\n",
        "cfg.d_model = D_MODEL\n",
        "cfg.d_head = D_HEAD\n",
        "cfg.file_config_prefix = \"\""
      ],
      "metadata": {
        "id": "uTJ2LHzo5flR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate test data and experiment runs"
      ],
      "metadata": {
        "id": "XjjpDQ2_cS2-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76YZ8VErsHjP"
      },
      "outputs": [],
      "source": [
        "# Generate a batch of prompts with 3 field names\n",
        "def generate_batch(batch_size):\n",
        "    cfg.batch_size = N_BATCH\n",
        "\n",
        "    if cs_num == 0 or cs_num == 1:\n",
        "      examples = qts.generate_cs1(batch_size=N_BATCH, min_cols=3, max_cols=3)\n",
        "    elif cs_num == 2:\n",
        "      examples = qts.generate_cs2(batch_size=N_BATCH, min_cols=3, max_cols=3)\n",
        "    elif cs_num == 3:\n",
        "      examples = qts.generate_cs3(batch_size=N_BATCH, min_cols=3, max_cols=3)\n",
        "\n",
        "    return examples"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Return a list of experiments to run. Also the max num tokens in the prompts, the\n",
        "# max num tokens in the (ground truth) answer, and max num tokens in prompt+answer.\n",
        "def get_experiment_list(examples, by_attention_head):\n",
        "    run_list = []\n",
        "    max_prompt_tokens = 0\n",
        "    max_good_answer_tokens = 0\n",
        "    max_tokens = 0\n",
        "\n",
        "    for example in examples:\n",
        "        prompt = example.get_alpaca_prompt()\n",
        "        answer = example.sql_statement\n",
        "\n",
        "        prompt_tokens = model.tokenizer(prompt)[\"input_ids\"]\n",
        "        num_prompt_tokens = len(prompt_tokens)\n",
        "        num_good_answer_tokens = len(model.tokenizer(answer)[\"input_ids\"])\n",
        "        num_tokens = num_prompt_tokens + num_good_answer_tokens\n",
        "\n",
        "        max_prompt_tokens = max(max_prompt_tokens, num_prompt_tokens)\n",
        "        max_good_answer_tokens = max(max_good_answer_tokens, num_good_answer_tokens)\n",
        "        max_tokens = max(max_tokens, num_tokens)\n",
        "\n",
        "        if by_attention_head:\n",
        "            for layer_idx in range(N_LAYERS):\n",
        "                for head_idx in range(N_HEADS):\n",
        "                    for token_idx in range(num_prompt_tokens):\n",
        "                        run_list.append([prompt, answer, layer_idx, head_idx, token_idx, prompt_tokens])\n",
        "        else:\n",
        "            for token_idx in range(num_prompt_tokens):\n",
        "                run_list.append([prompt, answer, token_idx, prompt_tokens])\n",
        "\n",
        "    return run_list, max_prompt_tokens, max_good_answer_tokens, max_tokens"
      ],
      "metadata": {
        "id": "Ju8wtYel8xRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Which token positions are useful?\n",
        "This information is used to shrink the size of search spaces in following sections."
      ],
      "metadata": {
        "id": "5wdUVEfAcXJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_BATCH = 50\n",
        "\n",
        "def run_token_experiments():\n",
        "    examples = generate_batch(N_BATCH)\n",
        "    run_list, max_prompt_tokens, max_good_answer_tokens, max_tokens = get_experiment_list(examples, False)\n",
        "    cfg.initialize_token_positions( max_prompt_tokens, max_good_answer_tokens, True )\n",
        "    num_exps = len(run_list)\n",
        "\n",
        "    print(\"N_BATCH=\"+str(N_BATCH), \"max_prompt_tokens=\"+str(max_prompt_tokens), \"max_good_answer_tokens=\"+str(max_good_answer_tokens), \"max_tokens=\"+str(max_tokens), \"num_exps=\"+str(num_exps))\n",
        "\n",
        "    try_results = np.zeros(max_tokens, dtype=int)\n",
        "    fail_results = np.zeros(max_tokens, dtype=int)\n",
        "\n",
        "    for item_num in tqdm.tqdm(range(num_exps)):\n",
        "\n",
        "        run_item = run_list[item_num]\n",
        "        run_prompt, run_answer, run_token_idx, prompt_tokens = run_item\n",
        "\n",
        "        with model.generate(prompt_tokens, max_new_tokens=max_good_answer_tokens,\n",
        "                          pad_token_id=model.tokenizer.eos_token_id) as tracer:\n",
        "\n",
        "            # Zero out just the portion of the output corresponding to this token position\n",
        "            for run_layer_idx in range(N_LAYERS):\n",
        "                model.transformer.h[run_layer_idx].output[0][:, run_token_idx, :] = 0\n",
        "\n",
        "            final_output = model.generator.output.save()\n",
        "\n",
        "        final_output = final_output.detach().cpu().numpy()\n",
        "        decoded_output = model.tokenizer.decode(final_output[0], skip_special_tokens=True)\n",
        "\n",
        "        # Did the output change?\n",
        "        try_results[run_token_idx] += 1\n",
        "        if run_prompt + run_answer != decoded_output:\n",
        "            #print(\"Input:\", item_num, run_prompt.replace('\\n', ' '), run_answer.replace('\\n', ' '))\n",
        "            #print(\"Output:\", item_num, decoded_output.replace('\\n', ' '))\n",
        "            fail_results[run_token_idx] += 1\n",
        "\n",
        "        # Compute the failure rate as percentage\n",
        "        failure_rate = (1.0 * fail_results / (try_results + 1e-10)) * 100\n",
        "        failure_rate = np.round(failure_rate, 2)\n",
        "\n",
        "    return max_tokens, fail_results, failure_rate\n",
        "\n",
        "g_max_tokens, g_token_fail_results, g_token_failure_rate = run_token_experiments()"
      ],
      "metadata": {
        "id": "ETOrSFQqVSG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print( \"Useful token positions:\" )\n",
        "for token_idx in range(len(g_token_failure_rate)):\n",
        "    if g_token_failure_rate[token_idx] > 0 :\n",
        "        cfg.add_useful_position(token_idx)\n",
        "        print( \"Position:\", token_idx, \"% Fails:\", g_token_failure_rate[token_idx], \"# Fails:\", g_token_fail_results[token_idx] )"
      ],
      "metadata": {
        "id": "RT7zG1EDdA5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if False:\n",
        "    cfg.calc_position_failures_map(g_token_fail_results.tolist())\n",
        "    qmi.save_plt_to_file(cfg=cfg, full_title=\"Failures When Position Ablated\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "wZ0aAic8jqiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Which token+layer+attention head nodes are useful?"
      ],
      "metadata": {
        "id": "GMs7MpPocb6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_BATCH = 10\n",
        "\n",
        "def run_attention_experiments():\n",
        "\n",
        "    examples = generate_batch(N_BATCH)\n",
        "    run_list, max_prompt_tokens, max_good_answer_tokens, max_tokens = get_experiment_list(examples, True)\n",
        "    num_exps = len(run_list)\n",
        "\n",
        "    print(\"N_BATCH=\"+str(N_BATCH), \"max_prompt_tokens=\"+str(max_prompt_tokens), \"max_good_answer_tokens=\"+str(max_good_answer_tokens), \"max_tokens=\"+str(max_tokens), \"num_exps=\"+str(num_exps))\n",
        "\n",
        "    try_results = np.zeros((N_LAYERS, N_HEADS, max_tokens), dtype=int)\n",
        "    fail_results = np.zeros((N_LAYERS, N_HEADS, max_tokens), dtype=int)\n",
        "\n",
        "    for item_num in tqdm.tqdm(range(num_exps)):\n",
        "\n",
        "        run_item = run_list[item_num]\n",
        "        run_prompt, run_answer, run_layer_idx, run_head_idx, run_token_idx, prompt_tokens = run_item\n",
        "\n",
        "        start = run_head_idx * D_HEAD\n",
        "        end = (run_head_idx + 1) * D_HEAD\n",
        "\n",
        "        with model.generate(prompt_tokens, max_new_tokens=max_good_answer_tokens,\n",
        "                          pad_token_id=model.tokenizer.eos_token_id) as tracer:\n",
        "\n",
        "            # Zero out just the portion of the output corresponding to this head\n",
        "            model.transformer.h[run_layer_idx].output[0][:, run_token_idx, start:end] = 0\n",
        "\n",
        "            final_output = model.generator.output.save()\n",
        "\n",
        "        final_output = final_output.detach().cpu().numpy()\n",
        "        decoded_output = model.tokenizer.decode(final_output[0], skip_special_tokens=True)\n",
        "\n",
        "        # Did the output change?\n",
        "        try_results[run_layer_idx, run_head_idx, run_token_idx] += 1\n",
        "        if run_prompt + run_answer != decoded_output:\n",
        "            fail_results[run_layer_idx, run_head_idx, run_token_idx] += 1\n",
        "\n",
        "        # Compute the failure rate as percentage\n",
        "        failure_rate = (1.0 * fail_results / (try_results + 1e-10)) * 100\n",
        "        failure_rate = np.round(failure_rate, 2)\n",
        "\n",
        "    return max_tokens, fail_results, failure_rate, try_results\n",
        "\n",
        "print(g_attn_failure_rate.shape)\n",
        "g_max_tokens, g_attn_failure_results, g_attn_failure_rate, g_attn_try_results = run_attention_experiments()"
      ],
      "metadata": {
        "id": "bINHUagozRoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg.useful_nodes = qmi.UsefulNodeList()\n",
        "for layer_idx in range(N_LAYERS):\n",
        "    for head_idx in range(N_HEADS):\n",
        "        for token_idx in range(g_max_tokens):\n",
        "            fail_perc = int(g_attn_failure_rate[ layer_idx, head_idx, token_idx])\n",
        "            if fail_perc > 0 :\n",
        "                # Add percentage failure quanta\n",
        "                node_location = qmi.NodeLocation(token_idx, layer_idx, True, head_idx)\n",
        "                cfg.add_useful_node_tag( node_location, qmi.QType.FAIL.value, str(fail_perc) )\n",
        "\n",
        "cfg.useful_nodes.sort_nodes()"
      ],
      "metadata": {
        "id": "1nViXbBVDxUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer_idx in range(N_LAYERS):\n",
        "    plt.imshow(g_attn_failure_rate[layer_idx], cmap=\"viridis\", aspect=\"auto\")\n",
        "    plt.colorbar(label=\"Percentage Change\")\n",
        "    plt.xlabel(\"Token Position\")\n",
        "    plt.ylabel(\"Attention Head\")\n",
        "    plt.title(\"Percentage of Output Changes by Zeroing Activations in Layer \" + str(layer_idx))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "O2lbWJrUwY7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg.useful_nodes.print_node_tags()"
      ],
      "metadata": {
        "id": "CAHRK6TOmMac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title = \"Useful attention heads\"\n",
        "ax1, quanta_results, num_results = qmi.calc_quanta_map(\n",
        "    cfg, True, 6,\n",
        "    cfg.useful_nodes, qmi.QType.FAIL.value, \"\", qmi.get_quanta_fail_perc)\n",
        "\n",
        "if num_results > 0:\n",
        "    if cfg.graph_file_suffix > \"\":\n",
        "        print(\"Saving quanta map:\", title)\n",
        "        qmi.save_plt_to_file(cfg=cfg, full_title=title)\n",
        "    else:\n",
        "        ax1.set_title(title + ' ({} nodes)'.format(len(quanta_results)))\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "DCAu8QO1mNB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Serialize and save the useful nodes list to a temporary CoLab file in JSON format\n",
        "#main_fname_behavior_json = cfg.model_name + '_behavior.json'\n",
        "main_fname_behavior_json = 'behavior.json'\n",
        "print( \"Saving useful node list with behavior tags:\", main_fname_behavior_json)\n",
        "cfg.useful_nodes.save_nodes(main_fname_behavior_json)"
      ],
      "metadata": {
        "id": "2_szu6Trokt2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "z1tQZgLUnojc"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}