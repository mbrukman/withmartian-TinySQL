{
<<<<<<< HEAD
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQAsYjSQVXHC"
   },
   "source": [
    "# M1 useful nodes\n",
    "This notebook identifies M1 nodes (attention heads and MLPs) that, when ablated, cause a decrease in model prediction accuracy. These nodes are needed (aka useful) for accurate predictions.\n",
    "\n",
    "\n",
    "This notebook was:\n",
    "- Developed on Google Colab using an **T4**\n",
    "- Runs with M1 (TinyStories) with base/CS1/CS2/CS3.\n",
    "- Requires a GITHUB_TOKEN secret to access Martian quanta_text_to_sql code repository.\n",
    "- Requires a HF_TOKEN secret to access Martian HuggingFace repository.\n",
    "\n",
    "This notebook relies on the nnsight library. Useful background:\n",
    "- https://nnsight.net/notebooks/tutorials/walkthrough/#Batching\n",
    "- https://nnsight.net/notebooks/tutorials/walkthrough/#Looping\n",
    "\n",
    "This notebook relies on the https://github.com/PhilipQuirke/quanta_mech_interp library for graphing useful nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1tQZgLUnojc"
   },
   "source": [
    "# Import libraries\n",
    "Imports standard libraries. Do not read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (24.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pip --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "qt2jt2bHcY2v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nnsight in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (0.3.7)\n",
      "Requirement already satisfied: pandas in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: plotly in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (5.24.1)\n",
      "Requirement already satisfied: matplotlib in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (3.9.3)\n",
      "Requirement already satisfied: transformers in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from nnsight) (4.44.0)\n",
      "Requirement already satisfied: protobuf in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from nnsight) (5.29.1)\n",
      "Requirement already satisfied: python-socketio[client] in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from nnsight) (5.11.4)\n",
      "Requirement already satisfied: tokenizers>=0.13.0 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from nnsight) (0.19.1)\n",
      "Requirement already satisfied: pydantic>=2.9.0 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from nnsight) (2.10.3)\n",
      "Requirement already satisfied: torch>=2.4.0 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from nnsight) (2.5.1)\n",
      "Requirement already satisfied: sentencepiece in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from nnsight) (0.2.0)\n",
      "Requirement already satisfied: torchvision in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from nnsight) (0.20.1)\n",
      "Requirement already satisfied: accelerate in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from nnsight) (0.33.0)\n",
      "Requirement already satisfied: diffusers in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from nnsight) (0.31.0)\n",
      "Requirement already satisfied: einops in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from nnsight) (0.8.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from plotly) (9.0.0)\n",
      "Requirement already satisfied: packaging in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from plotly) (24.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from pydantic>=2.9.0->nnsight) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from pydantic>=2.9.0->nnsight) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from pydantic>=2.9.0->nnsight) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from tokenizers>=0.13.0->nnsight) (0.24.5)\n",
      "Requirement already satisfied: filelock in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=2.4.0->nnsight) (3.15.4)\n",
      "Requirement already satisfied: networkx in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=2.4.0->nnsight) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=2.4.0->nnsight) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=2.4.0->nnsight) (2024.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=2.4.0->nnsight) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=2.4.0->nnsight) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=2.4.0->nnsight) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=2.4.0->nnsight) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=2.4.0->nnsight) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=2.4.0->nnsight) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=2.4.0->nnsight) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=2.4.0->nnsight) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=2.4.0->nnsight) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=2.4.0->nnsight) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=2.4.0->nnsight) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=2.4.0->nnsight) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=2.4.0->nnsight) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=2.4.0->nnsight) (72.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=2.4.0->nnsight) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.4.0->nnsight) (1.3.0)\n",
      "Requirement already satisfied: psutil in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from accelerate->nnsight) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from accelerate->nnsight) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from accelerate->nnsight) (0.4.4)\n",
      "Requirement already satisfied: importlib-metadata in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages/setuptools/_vendor (from diffusers->nnsight) (8.0.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from diffusers->nnsight) (2024.7.24)\n",
      "Requirement already satisfied: requests in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from diffusers->nnsight) (2.32.3)\n",
      "Requirement already satisfied: bidict>=0.21.0 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from python-socketio[client]->nnsight) (0.23.1)\n",
      "Requirement already satisfied: python-engineio>=4.8.0 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from python-socketio[client]->nnsight) (4.10.1)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from python-socketio[client]->nnsight) (1.8.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from transformers->nnsight) (4.66.5)\n",
      "Requirement already satisfied: simple-websocket>=0.10.0 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from python-engineio>=4.8.0->python-socketio[client]->nnsight) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from requests->diffusers->nnsight) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from requests->diffusers->nnsight) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from requests->diffusers->nnsight) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from requests->diffusers->nnsight) (2024.7.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages/setuptools/_vendor (from importlib-metadata->diffusers->nnsight) (3.19.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from jinja2->torch>=2.4.0->nnsight) (2.1.5)\n",
      "Requirement already satisfied: wsproto in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from simple-websocket>=0.10.0->python-engineio>=4.8.0->python-socketio[client]->nnsight) (1.2.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from wsproto->simple-websocket>=0.10.0->python-engineio>=4.8.0->python-socketio[client]->nnsight) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "# https://nnsight.net/\n",
    "# Access 0.4 prerelease version (as at Dec 2024)\n",
    "#!pip install nnsight==0.4.0.dev0\n",
    "!pip install nnsight pandas plotly matplotlib --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: plotly\n",
      "Version: 5.24.1\n",
      "Summary: An open-source, interactive data visualization library for Python\n",
      "Home-page: https://plotly.com/python/\n",
      "Author: Chris P\n",
      "Author-email: chris@plot.ly\n",
      "License: MIT\n",
      "Location: /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages\n",
      "Requires: packaging, tenacity\n",
      "Required-by: QuantaMechInterp\n"
     ]
    }
   ],
   "source": [
    "!pip show plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Vif7qLNrlC0P"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import einops\n",
    "import torch\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"colab\"\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm.auto as tqdm\n",
    "\n",
    "import nnsight\n",
    "from nnsight import LanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "orVn0wTnosHO"
   },
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "#from google.colab import userdata\n",
    "import gc\n",
    "import weakref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "i1zClTbuyFUG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/PhilipQuirke/quanta_mech_interp.git\n",
      "  Cloning https://github.com/PhilipQuirke/quanta_mech_interp.git to /tmp/pip-req-build-be_655yl\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/PhilipQuirke/quanta_mech_interp.git /tmp/pip-req-build-be_655yl\n",
      "  Resolved https://github.com/PhilipQuirke/quanta_mech_interp.git to commit a05298e344510c9afe41b1e9e231aa8e49d59a6f\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.1 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from QuantaMechInterp==1.0) (1.26.4)\n",
      "Requirement already satisfied: plotly in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from QuantaMechInterp==1.0) (5.24.1)\n",
      "Requirement already satisfied: matplotlib in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from QuantaMechInterp==1.0) (3.9.3)\n",
      "Requirement already satisfied: scikit-learn in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from QuantaMechInterp==1.0) (1.6.0)\n",
      "Requirement already satisfied: scikit-optimize in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from QuantaMechInterp==1.0) (0.10.2)\n",
      "Requirement already satisfied: transformers in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from QuantaMechInterp==1.0) (4.44.0)\n",
      "Requirement already satisfied: typeguard>=3.0.2 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages/setuptools/_vendor (from QuantaMechInterp==1.0) (4.3.0)\n",
      "Requirement already satisfied: torch>=1.10 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from QuantaMechInterp==1.0) (2.5.1)\n",
      "Requirement already satisfied: torchtyping>=0.1.4 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from QuantaMechInterp==1.0) (0.1.4)\n",
      "Requirement already satisfied: filelock in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=1.10->QuantaMechInterp==1.0) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=1.10->QuantaMechInterp==1.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=1.10->QuantaMechInterp==1.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=1.10->QuantaMechInterp==1.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=1.10->QuantaMechInterp==1.0) (2024.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=1.10->QuantaMechInterp==1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=1.10->QuantaMechInterp==1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=1.10->QuantaMechInterp==1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=1.10->QuantaMechInterp==1.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=1.10->QuantaMechInterp==1.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=1.10->QuantaMechInterp==1.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=1.10->QuantaMechInterp==1.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=1.10->QuantaMechInterp==1.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=1.10->QuantaMechInterp==1.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=1.10->QuantaMechInterp==1.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=1.10->QuantaMechInterp==1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=1.10->QuantaMechInterp==1.0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=1.10->QuantaMechInterp==1.0) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=1.10->QuantaMechInterp==1.0) (72.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from torch>=1.10->QuantaMechInterp==1.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.10->QuantaMechInterp==1.0) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from matplotlib->QuantaMechInterp==1.0) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from matplotlib->QuantaMechInterp==1.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from matplotlib->QuantaMechInterp==1.0) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from matplotlib->QuantaMechInterp==1.0) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from matplotlib->QuantaMechInterp==1.0) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from matplotlib->QuantaMechInterp==1.0) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from matplotlib->QuantaMechInterp==1.0) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from matplotlib->QuantaMechInterp==1.0) (2.9.0.post0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from plotly->QuantaMechInterp==1.0) (9.0.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from scikit-learn->QuantaMechInterp==1.0) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from scikit-learn->QuantaMechInterp==1.0) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from scikit-learn->QuantaMechInterp==1.0) (3.5.0)\n",
      "Requirement already satisfied: pyaml>=16.9 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from scikit-optimize->QuantaMechInterp==1.0) (24.12.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from transformers->QuantaMechInterp==1.0) (0.24.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from transformers->QuantaMechInterp==1.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from transformers->QuantaMechInterp==1.0) (2024.7.24)\n",
      "Requirement already satisfied: requests in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from transformers->QuantaMechInterp==1.0) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from transformers->QuantaMechInterp==1.0) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from transformers->QuantaMechInterp==1.0) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from transformers->QuantaMechInterp==1.0) (4.66.5)\n",
      "Requirement already satisfied: six>=1.5 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->QuantaMechInterp==1.0) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from jinja2->torch>=1.10->QuantaMechInterp==1.0) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from requests->transformers->QuantaMechInterp==1.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from requests->transformers->QuantaMechInterp==1.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from requests->transformers->QuantaMechInterp==1.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from requests->transformers->QuantaMechInterp==1.0) (2024.7.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade git+https://github.com/PhilipQuirke/quanta_mech_interp.git\n",
    "import QuantaMechInterp as qmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "BMWEb8TJoske"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://****@github.com/withmartian/quanta_text_to_sql.git\n",
      "  Cloning https://****@github.com/withmartian/quanta_text_to_sql.git to /tmp/pip-req-build-2eplgdcb\n",
      "  Running command git clone --filter=blob:none --quiet 'https://****@github.com/withmartian/quanta_text_to_sql.git' /tmp/pip-req-build-2eplgdcb\n",
      "  warning: redirecting to https://github.com/withmartian/quanta_text_to_sql.git/\n",
      "  Resolved https://****@github.com/withmartian/quanta_text_to_sql.git to commit f4e42c0da4088c4b445b61aa75693f199e629da9\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.1 in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages (from QuantaTextToSql==1.0) (1.26.4)\n",
      "Requirement already satisfied: wheel in /home/dhruv_gretel_ai/.pyenv/versions/text-to-sql/lib/python3.12/site-packages/setuptools/_vendor (from QuantaTextToSql==1.0) (0.43.0)\n"
     ]
    }
   ],
   "source": [
    "#github_token = userdata.get(\"GITHUB_TOKEN\")\n",
    "#github_token = getpass(\"Enter your GitHub token: \")\n",
    "# Install the private repository using the token\n",
    "!pip install --upgrade git+https://@github.com/withmartian/quanta_text_to_sql.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6k59y0OZhp7"
   },
   "source": [
    "# Select model, command set and feature to investigate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import QuantaTextToSql as qts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "rl1akjeAUfoV"
   },
   "outputs": [],
   "source": [
    "model_num = 1                 # 1=TinyStories, 2=Qwen, 3=Llama, 4=Granite, 5=SmolLM\n",
    "cs_num = 1                    # 0=BaseModel, 1=CS1, 2=CS2 or 3=CS3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y72URtyynvnv"
   },
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "oFaFTy6LnbxI"
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "data did not match any variant of untagged enum ModelWrapper at line 250326 column 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mqts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_tinysql_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcs_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhf_VFejYwPmVSEbCTkoECaONtTmCosfmRwDgd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m clear_output()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n",
      "File \u001b[0;32m~/.pyenv/versions/text-to-sql/lib/python3.12/site-packages/QuantaTextToSql/load_data/load_model.py:112\u001b[0m, in \u001b[0;36mload_tinysql_model\u001b[0;34m(model_num, cs_num, auth_token)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_tinysql_model\u001b[39m( model_num : \u001b[38;5;28mint\u001b[39m, cs_num : \u001b[38;5;28mint\u001b[39m, auth_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_num \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 112\u001b[0m         the_tokenizer, the_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_sql_interp_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcs_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_flash_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m         model \u001b[38;5;241m=\u001b[39m LanguageModel(the_model, the_tokenizer)\n\u001b[1;32m    114\u001b[0m         model\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m the_tokenizer\n",
      "File \u001b[0;32m~/.pyenv/versions/text-to-sql/lib/python3.12/site-packages/QuantaTextToSql/load_data/load_model.py:94\u001b[0m, in \u001b[0;36mload_sql_interp_model\u001b[0;34m(model_num, cs_num, auth_token, use_flash_attention, device_map)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_sql_interp_model\u001b[39m( model_num : \u001b[38;5;28mint\u001b[39m, cs_num : \u001b[38;5;28mint\u001b[39m, auth_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, use_flash_attention\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     92\u001b[0m     model_location \u001b[38;5;241m=\u001b[39m sql_interp_model_location(model_num, cs_num)\n\u001b[0;32m---> 94\u001b[0m     tokenizer, model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_flash_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_flash_attention\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_num \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     97\u001b[0m         tokenizer\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/text-to-sql/lib/python3.12/site-packages/QuantaTextToSql/load_data/load_model.py:66\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_location, auth_token, use_flash_attention, device_map)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     auth_token \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHF_TOKEN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 66\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_flash_attention:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# qwen model and llama model with flash attention\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# Prerequisite: pip install flash-attn==2.0.2\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# From https://github.com/Dao-AILab/flash-attention\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     75\u001b[0m         model_location,\n\u001b[1;32m     76\u001b[0m         torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16,\n\u001b[1;32m     77\u001b[0m         device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[1;32m     78\u001b[0m         attn_implementation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     79\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/text-to-sql/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:897\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    894\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    895\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    896\u001b[0m         )\n\u001b[0;32m--> 897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[0;32m~/.pyenv/versions/text-to-sql/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2271\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2268\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2269\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2274\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2275\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2279\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2280\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2282\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/text-to-sql/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2505\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2503\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2504\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2505\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2506\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   2507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   2508\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2509\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2510\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/text-to-sql/lib/python3.12/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py:99\u001b[0m, in \u001b[0;36mGPT2TokenizerFast.__init__\u001b[0;34m(self, vocab_file, merges_file, tokenizer_file, unk_token, bos_token, eos_token, add_prefix_space, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     90\u001b[0m     vocab_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     98\u001b[0m ):\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmerges_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43munk_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_prefix_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_prefix_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_bos_token \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd_bos_token\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    112\u001b[0m     pre_tok_state \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend_tokenizer\u001b[38;5;241m.\u001b[39mpre_tokenizer\u001b[38;5;241m.\u001b[39m__getstate__())\n",
      "File \u001b[0;32m~/.pyenv/versions/text-to-sql/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:115\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(tokenizer_object)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fast_tokenizer_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m from_slow:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# We have a serialization from tokenizers which let us directly build the backend\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mTokenizerFast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfast_tokenizer_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# We need to convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n",
      "\u001b[0;31mException\u001b[0m: data did not match any variant of untagged enum ModelWrapper at line 250326 column 3"
     ]
    }
   ],
   "source": [
    "model = qts.load_tinysql_model(model_num, cs_num, auth_token=\"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\")\n",
    "\n",
    "clear_output()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sRL4ckXZqElr"
   },
   "outputs": [],
   "source": [
    "N_LAYERS, N_HEADS, D_MODEL, D_HEAD = qts.get_model_sizes(model_num, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uTJ2LHzo5flR"
   },
   "outputs": [],
   "source": [
    "model_hf_name = qts.sql_interp_model_location(model_num, cs_num)\n",
    "\n",
    "# Singleton QuantaTool \"main\" configuration class. qmi.AlgoConfig is derived from the chain qmi.UsefulConfig > qmi.ModelConfig\n",
    "cfg = qmi.AlgoConfig()\n",
    "cfg.repo_name, cfg.model_name = model_hf_name.rsplit(\"/\", 1)\n",
    "cfg.main_model = model\n",
    "cfg.n_layers = N_LAYERS\n",
    "cfg.n_heads = N_HEADS\n",
    "cfg.d_model = D_MODEL\n",
    "cfg.d_head = D_HEAD\n",
    "cfg.file_config_prefix = \"\"\n",
    "\n",
    "print(\"cfg.repo_name=\"+cfg.repo_name, \"cfg.model_name=\"+cfg.model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjjpDQ2_cS2-"
   },
   "source": [
    "# Generate test data and experiment runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "76YZ8VErsHjP"
   },
   "outputs": [],
   "source": [
    "# Generate a batch of prompts with 3 field names\n",
    "def generate_batch(batch_size):\n",
    "    cfg.batch_size = N_BATCH\n",
    "\n",
    "    return qts.generate_csn(batch_size=N_BATCH, csn=max(1,cs_num), min_cols=3, max_cols=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ju8wtYel8xRm"
   },
   "outputs": [],
   "source": [
    "# Return a list of experiments to run\n",
    "def get_experiment_list(examples, by_attention_head):\n",
    "    run_list = []\n",
    "    max_prompt_tokens = 0\n",
    "    max_answer_tokens = 0\n",
    "    max_tokens = 0\n",
    "\n",
    "    for example in examples:\n",
    "        prompt = example.get_alpaca_prompt()\n",
    "        answer = example.sql_statement\n",
    "\n",
    "        prompt_tokens = model.tokenizer(prompt)[\"input_ids\"]\n",
    "        answer_tokens = model.tokenizer(answer)[\"input_ids\"]\n",
    "        all_tokens = prompt_tokens + answer_tokens\n",
    "\n",
    "        num_answer_tokens = len(answer_tokens)\n",
    "        num_prompt_tokens = len(prompt_tokens)\n",
    "        num_tokens = len(all_tokens)\n",
    "        assert num_tokens == num_prompt_tokens + num_answer_tokens\n",
    "\n",
    "        max_prompt_tokens = max(max_prompt_tokens, num_prompt_tokens)\n",
    "        max_answer_tokens = max(max_answer_tokens, num_answer_tokens)\n",
    "        max_tokens = max(max_tokens, num_tokens)\n",
    "\n",
    "        if by_attention_head:\n",
    "            for layer_idx in range(N_LAYERS):\n",
    "                for head_idx in range(N_HEADS):\n",
    "                    for token_idx in range(num_tokens):\n",
    "                        exp_prompt_tokens = max(num_prompt_tokens, token_idx)\n",
    "                        exp_num_generate = num_tokens - exp_prompt_tokens\n",
    "                        assert num_tokens == exp_prompt_tokens + exp_num_generate\n",
    "\n",
    "                        run_list.append([layer_idx, head_idx, token_idx, all_tokens, exp_prompt_tokens, exp_num_generate])\n",
    "        else:\n",
    "            for token_idx in range(num_prompt_tokens):\n",
    "                run_list.append([prompt, answer, token_idx, prompt_tokens])\n",
    "\n",
    "    return run_list, max_prompt_tokens, max_answer_tokens, max_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wdUVEfAcXJa"
   },
   "source": [
    "# Which token positions are useful?\n",
    "This information is used to shrink the size of search spaces in following sections. For the SQL model all token positions are useful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rDX9JLzKl2oL"
   },
   "outputs": [],
   "source": [
    "all_token_positions_useful = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ETOrSFQqVSG6"
   },
   "outputs": [],
   "source": [
    "N_BATCH = 50\n",
    "\n",
    "def run_token_experiments( ):\n",
    "    examples = generate_batch(N_BATCH)\n",
    "    run_list, max_prompt_tokens, max_good_answer_tokens, max_tokens = get_experiment_list(examples, False)\n",
    "    cfg.initialize_token_positions( max_prompt_tokens, max_good_answer_tokens, True )\n",
    "    num_exps = len(run_list)\n",
    "\n",
    "    print(\"N_BATCH=\"+str(N_BATCH), \"max_prompt_tokens=\"+str(max_prompt_tokens), \"max_good_answer_tokens=\"+str(max_good_answer_tokens), \"max_tokens=\"+str(max_tokens), \"num_exps=\"+str(num_exps))\n",
    "\n",
    "    try_results = np.zeros(max_tokens, dtype=int)\n",
    "    fail_results = np.zeros(max_tokens, dtype=int)\n",
    "\n",
    "    if not all_token_positions_useful:\n",
    "        for item_num in tqdm.tqdm(range(num_exps)):\n",
    "\n",
    "            run_item = run_list[item_num]\n",
    "            run_prompt, run_answer, run_token_idx, prompt_tokens = run_item\n",
    "\n",
    "            with model.generate(prompt_tokens, max_new_tokens=max_good_answer_tokens,\n",
    "                              pad_token_id=model.tokenizer.eos_token_id) as tracer:\n",
    "\n",
    "                # Zero out just the portion of the output corresponding to this token position\n",
    "                for run_layer_idx in range(N_LAYERS):\n",
    "                    model.transformer.h[run_layer_idx].output[0][:, run_token_idx, :] = 0\n",
    "\n",
    "                final_output = model.generator.output.save()\n",
    "\n",
    "            final_output = final_output.detach().cpu().numpy()\n",
    "            decoded_output = model.tokenizer.decode(final_output[0], skip_special_tokens=True)\n",
    "\n",
    "            # Did the output change?\n",
    "            try_results[run_token_idx] += 1\n",
    "            if run_prompt + run_answer != decoded_output:\n",
    "                #print(\"Input:\", item_num, run_prompt.replace('\\n', ' '), run_answer.replace('\\n', ' '))\n",
    "                #print(\"Output:\", item_num, decoded_output.replace('\\n', ' '))\n",
    "                fail_results[run_token_idx] += 1\n",
    "\n",
    "    # Compute the failure rate as percentage\n",
    "    failure_rate = (1.0 * fail_results / (try_results + 1e-10)) * 100\n",
    "    failure_rate = np.round(failure_rate, 2)\n",
    "\n",
    "    return max_tokens, fail_results, failure_rate\n",
    "\n",
    "g_max_tokens, g_token_fail_results, g_token_failure_rate = run_token_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RT7zG1EDdA5C"
   },
   "outputs": [],
   "source": [
    "if all_token_positions_useful:\n",
    "    for token_idx in range(g_max_tokens):\n",
    "        cfg.add_useful_position(token_idx)\n",
    "else:\n",
    "    print( \"Useful token positions:\" )\n",
    "    for token_idx in range(len(g_token_failure_rate)):\n",
    "        if g_token_failure_rate[token_idx] > 0 :\n",
    "            cfg.add_useful_position(token_idx)\n",
    "            print( \"Position:\", token_idx, \"% Fails:\", g_token_failure_rate[token_idx], \"# Fails:\", g_token_fail_results[token_idx] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZ0aAic8jqiV"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    cfg.calc_position_failures_map(g_token_fail_results.tolist())\n",
    "    qmi.save_plt_to_file(cfg=cfg, full_title=\"Failures When Position Ablated\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMs7MpPocb6_"
   },
   "source": [
    "# Which token+layer+attention head nodes are useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bINHUagozRoo"
   },
   "outputs": [],
   "source": [
    "N_BATCH = 5\n",
    "\n",
    "def run_attention_experiments():\n",
    "\n",
    "    examples = generate_batch(N_BATCH)\n",
    "    run_list, max_prompt_tokens, max_good_answer_tokens, max_tokens = get_experiment_list(examples, True)\n",
    "    num_exps = len(run_list)\n",
    "\n",
    "    print(\"N_BATCH=\"+str(N_BATCH), \"max_prompt_tokens=\"+str(max_prompt_tokens), \"max_answer_tokens=\"+str(max_good_answer_tokens), \"max_tokens=\"+str(max_tokens), \"num_exps=\"+str(num_exps))\n",
    "\n",
    "    try_results = np.zeros((N_LAYERS, N_HEADS, max_tokens), dtype=int)\n",
    "    fail_results = np.zeros((N_LAYERS, N_HEADS, max_tokens), dtype=int)\n",
    "\n",
    "    for item_num in tqdm.tqdm(range(num_exps)):\n",
    "\n",
    "        run_item = run_list[item_num]\n",
    "        run_layer_idx, run_head_idx, run_token_idx, all_tokens, num_prompt_tokens, num_generate = run_item\n",
    "        assert len(all_tokens) == num_prompt_tokens + num_generate\n",
    "\n",
    "        start = run_head_idx * D_HEAD\n",
    "        end = (run_head_idx + 1) * D_HEAD\n",
    "\n",
    "        if run_token_idx < num_prompt_tokens:\n",
    "            with model.generate(all_tokens[:num_prompt_tokens], max_new_tokens=num_generate,\n",
    "                              pad_token_id=model.tokenizer.eos_token_id) as tracer:\n",
    "\n",
    "                # Zero out just the portion of the prompt corresponding to this head\n",
    "                model.transformer.h[run_layer_idx].output[0][:, run_token_idx, start:end] = 0\n",
    "\n",
    "                final_output = model.generator.output.save()\n",
    "\n",
    "        else:\n",
    "            assert num_prompt_tokens == run_token_idx\n",
    "            with model.generate(all_tokens[:num_prompt_tokens], max_new_tokens=num_generate,\n",
    "                              pad_token_id=model.tokenizer.eos_token_id) as tracer:\n",
    "\n",
    "                for next_idx in range(num_generate):\n",
    "                    model.transformer.h[run_layer_idx].next()\n",
    "\n",
    "                # Zero out just the portion of the output corresponding to this head\n",
    "                # PQR: Runs but output is never corrupted. This code run inside above for loop when next_idx == 0 crashes. Sigh\n",
    "                model.transformer.h[run_layer_idx].output[0][:, run_token_idx, start:end] = 0\n",
    "\n",
    "                final_output = model.generator.output.save()\n",
    "\n",
    "        final_output = final_output.detach().cpu().numpy()\n",
    "\n",
    "        # Did the output change?\n",
    "        if (len(final_output[0]) != len(all_tokens)) or not (all_tokens == final_output[0]).all():\n",
    "            fail_results[run_layer_idx, run_head_idx, run_token_idx] += 1\n",
    "        try_results[run_layer_idx, run_head_idx, run_token_idx] += 1\n",
    "\n",
    "    # Compute the failure rate as percentage\n",
    "    failure_rate = (1.0 * fail_results / (try_results + 1e-10)) * 100\n",
    "    failure_rate = np.round(failure_rate, 2)\n",
    "\n",
    "    return max_tokens, fail_results, failure_rate, try_results\n",
    "\n",
    "g_max_tokens, g_attn_failure_results, g_attn_failure_rate, g_attn_try_results = run_attention_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1nViXbBVDxUt"
   },
   "outputs": [],
   "source": [
    "cfg.useful_nodes = qmi.UsefulNodeList()\n",
    "for layer_idx in range(N_LAYERS):\n",
    "    for head_idx in range(N_HEADS):\n",
    "        for token_idx in range(g_max_tokens):\n",
    "            fail_perc = int(g_attn_failure_rate[layer_idx, head_idx, token_idx])\n",
    "            if fail_perc > 0 :\n",
    "                # Add percentage failure quanta\n",
    "                node_location = qmi.NodeLocation(token_idx, layer_idx, True, head_idx)\n",
    "                cfg.add_useful_node_tag( node_location, qmi.QType.FAIL.value, str(fail_perc) )\n",
    "\n",
    "cfg.useful_nodes.sort_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O2lbWJrUwY7p"
   },
   "outputs": [],
   "source": [
    "for layer_idx in range(N_LAYERS):\n",
    "    plt.imshow(g_attn_failure_rate[layer_idx], cmap=\"viridis\", aspect=\"auto\")\n",
    "    plt.colorbar(label=\"Percentage Change\")\n",
    "    plt.xlabel(\"Token Position\")\n",
    "    plt.ylabel(\"Attention Head\")\n",
    "    plt.title(\"Percentage of Output Changes by Zeroing Activations in Layer \" + str(layer_idx))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CAHRK6TOmMac"
   },
   "outputs": [],
   "source": [
    "cfg.useful_nodes.print_node_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DCAu8QO1mNB0"
   },
   "outputs": [],
   "source": [
    "title = \"Useful attention heads\"\n",
    "ax1, quanta_results, num_results = qmi.calc_quanta_map(\n",
    "    cfg, True, 6,\n",
    "    cfg.useful_nodes, qmi.QType.FAIL.value, \"\", qmi.get_quanta_fail_perc,\n",
    "    combine_identical_cells=False)\n",
    "\n",
    "if num_results > 0:\n",
    "    if cfg.graph_file_suffix > \"\":\n",
    "        print(\"Saving quanta map:\", title)\n",
    "        qmi.save_plt_to_file(cfg=cfg, full_title=title)\n",
    "    else:\n",
    "        ax1.set_title(title + ' ({} nodes)'.format(len(quanta_results)))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2_szu6Trokt2"
   },
   "outputs": [],
   "source": [
    "# Serialize and save the useful nodes list to a temporary CoLab file in JSON format\n",
    "main_fname_behavior_json = cfg.model_name + '_behavior.json'\n",
    "print( \"Saving useful node list with behavior tags:\", main_fname_behavior_json)\n",
    "cfg.useful_nodes.save_nodes(main_fname_behavior_json)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "z1tQZgLUnojc",
    "5wdUVEfAcXJa"
   ],
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "text-to-sql",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
=======
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQAsYjSQVXHC"
      },
      "source": [
        "# M1 useful nodes\n",
        "This notebook identifies M1 nodes (attention heads and MLPs) that, when ablated, cause a decrease in model prediction accuracy. These nodes are needed (aka useful) for accurate predictions.\n",
        "\n",
        "\n",
        "This notebook was:\n",
        "- Developed on Google Colab using an **T4**\n",
        "- Runs with M1 (TinyStories) with base/CS1/CS2/CS3 models.\n",
        "- Requires a GITHUB_TOKEN secret to access Martian TinySQL code repository.\n",
        "- Requires a HF_TOKEN secret to access Martian HuggingFace repository.\n",
        "\n",
        "This notebook relies on the nnsight library. Useful background:\n",
        "- https://nnsight.net/notebooks/tutorials/walkthrough/#Batching\n",
        "- https://nnsight.net/notebooks/tutorials/walkthrough/#Looping\n",
        "\n",
        "This notebook relies on the https://github.com/PhilipQuirke/quanta_mech_interp library for graphing useful nodes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1tQZgLUnojc"
      },
      "source": [
        "# Import libraries\n",
        "Imports standard libraries. Do not read."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qt2jt2bHcY2v",
        "outputId": "a4023824-37dd-42ea-9577-e8eb626ec430"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nnsight in /usr/local/lib/python3.10/dist-packages (0.3.7)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from nnsight) (4.47.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from nnsight) (4.25.5)\n",
            "Requirement already satisfied: python-socketio[client] in /usr/local/lib/python3.10/dist-packages (from nnsight) (5.12.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from nnsight) (0.21.0)\n",
            "Requirement already satisfied: pydantic>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from nnsight) (2.10.3)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from nnsight) (2.5.1+cu121)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from nnsight) (0.2.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from nnsight) (0.20.1+cu121)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from nnsight) (1.2.1)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.10/dist-packages (from nnsight) (0.31.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from nnsight) (0.8.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.0->nnsight) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.0->nnsight) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.0->nnsight) (4.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.0->nnsight) (0.27.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->nnsight) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->nnsight) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->nnsight) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->nnsight) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->nnsight) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.4.0->nnsight) (1.3.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate->nnsight) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->nnsight) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->nnsight) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate->nnsight) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate->nnsight) (0.4.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers->nnsight) (8.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers->nnsight) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers->nnsight) (2.32.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers->nnsight) (11.0.0)\n",
            "Requirement already satisfied: bidict>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from python-socketio[client]->nnsight) (0.23.1)\n",
            "Requirement already satisfied: python-engineio>=4.11.0 in /usr/local/lib/python3.10/dist-packages (from python-socketio[client]->nnsight) (4.11.2)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from python-socketio[client]->nnsight) (1.8.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->nnsight) (4.67.1)\n",
            "Requirement already satisfied: simple-websocket>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from python-engineio>=4.11.0->python-socketio[client]->nnsight) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers->nnsight) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers->nnsight) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers->nnsight) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers->nnsight) (2024.12.14)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers->nnsight) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.4.0->nnsight) (3.0.2)\n",
            "Requirement already satisfied: wsproto in /usr/local/lib/python3.10/dist-packages (from simple-websocket>=0.10.0->python-engineio>=4.11.0->python-socketio[client]->nnsight) (1.2.0)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto->simple-websocket>=0.10.0->python-engineio>=4.11.0->python-socketio[client]->nnsight) (0.14.0)\n"
          ]
        }
      ],
      "source": [
        "# https://nnsight.net/\n",
        "# Access 0.4 prerelease version (as at Dec 2024)\n",
        "#!pip install nnsight==0.4.0.dev0\n",
        "!pip install -U nnsight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Vif7qLNrlC0P"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import einops\n",
        "import torch\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = \"colab\"\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm.auto as tqdm\n",
        "\n",
        "import nnsight\n",
        "from nnsight import LanguageModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "orVn0wTnosHO"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "from google.colab import userdata\n",
        "import gc\n",
        "import weakref"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1zClTbuyFUG",
        "outputId": "8f77b71a-4220-415d-8b94-cf40d78544f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/PhilipQuirke/quanta_mech_interp.git\n",
            "  Cloning https://github.com/PhilipQuirke/quanta_mech_interp.git to /tmp/pip-req-build-yaa0wjir\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/PhilipQuirke/quanta_mech_interp.git /tmp/pip-req-build-yaa0wjir\n",
            "  Resolved https://github.com/PhilipQuirke/quanta_mech_interp.git to commit 7a53d2b4cf91a41532b2f8437fe43e58febca204\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from QuantaMechInterp==1.0) (1.26.4)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from QuantaMechInterp==1.0) (5.24.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from QuantaMechInterp==1.0) (3.8.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from QuantaMechInterp==1.0) (1.6.0)\n",
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.10/dist-packages (from QuantaMechInterp==1.0) (0.10.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from QuantaMechInterp==1.0) (4.47.1)\n",
            "Requirement already satisfied: typeguard>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from QuantaMechInterp==1.0) (4.4.1)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from QuantaMechInterp==1.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchtyping>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from QuantaMechInterp==1.0) (0.1.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->QuantaMechInterp==1.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->QuantaMechInterp==1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->QuantaMechInterp==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->QuantaMechInterp==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->QuantaMechInterp==1.0) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->QuantaMechInterp==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10->QuantaMechInterp==1.0) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->QuantaMechInterp==1.0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->QuantaMechInterp==1.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->QuantaMechInterp==1.0) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->QuantaMechInterp==1.0) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->QuantaMechInterp==1.0) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->QuantaMechInterp==1.0) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->QuantaMechInterp==1.0) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->QuantaMechInterp==1.0) (2.8.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->QuantaMechInterp==1.0) (9.0.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->QuantaMechInterp==1.0) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->QuantaMechInterp==1.0) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->QuantaMechInterp==1.0) (3.5.0)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize->QuantaMechInterp==1.0) (24.12.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers->QuantaMechInterp==1.0) (0.27.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->QuantaMechInterp==1.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->QuantaMechInterp==1.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->QuantaMechInterp==1.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->QuantaMechInterp==1.0) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->QuantaMechInterp==1.0) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->QuantaMechInterp==1.0) (4.67.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->QuantaMechInterp==1.0) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->QuantaMechInterp==1.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->QuantaMechInterp==1.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->QuantaMechInterp==1.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->QuantaMechInterp==1.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->QuantaMechInterp==1.0) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade git+https://github.com/PhilipQuirke/quanta_mech_interp.git\n",
        "import QuantaMechInterp as qmi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMWEb8TJoske",
        "outputId": "7fe4f345-dd29-47bf-e498-236d9932bae8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://****@github.com/withmartian/TinySQL.git\n",
            "  Cloning https://****@github.com/withmartian/TinySQL.git to /tmp/pip-req-build-9wgo544c\n",
            "  Running command git clone --filter=blob:none --quiet 'https://****@github.com/withmartian/TinySQL.git' /tmp/pip-req-build-9wgo544c\n",
            "  Resolved https://****@github.com/withmartian/TinySQL.git to commit 8c4ec093c1f14305ee081a3f1b54b7cf44c7cda2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from TinySQL==1.0) (1.26.4)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from TinySQL==1.0) (0.45.1)\n"
          ]
        }
      ],
      "source": [
        "github_token = userdata.get(\"GITHUB_TOKEN\")\n",
        "\n",
        "# Install the private repository using the token\n",
        "!pip install --upgrade git+https://{github_token}@github.com/withmartian/TinySQL.git\n",
        "\n",
        "import TinySQL as qts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6k59y0OZhp7"
      },
      "source": [
        "# Select model and command set to investigate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "rl1akjeAUfoV"
      },
      "outputs": [],
      "source": [
        "model_num = 1                 # 1=TinyStories, 2=Qwen, 3=Llama, 4=Granite, 5=SmolLM\n",
        "cs_num = 1                    # 0=BaseModel, 1=CS1, 2=CS2 or 3=CS3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y72URtyynvnv"
      },
      "source": [
        "# Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFaFTy6LnbxI"
      },
      "outputs": [],
      "source": [
        "model = qts.load_tinysql_model(model_num, cs_num, auth_token=userdata.get(\"HF_TOKEN\"))\n",
        "clear_output()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRL4ckXZqElr"
      },
      "outputs": [],
      "source": [
        "N_LAYERS, N_HEADS, D_MODEL, D_HEAD = qts.get_model_sizes(model_num, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTJ2LHzo5flR"
      },
      "outputs": [],
      "source": [
        "model_hf_name = qts.sql_interp_model_location(model_num, cs_num)\n",
        "\n",
        "# Singleton QuantaTool \"main\" configuration class. qmi.AlgoConfig is derived from the chain qmi.UsefulConfig > qmi.ModelConfig\n",
        "cfg = qmi.AlgoConfig()\n",
        "cfg.repo_name, cfg.model_name = model_hf_name.rsplit(\"/\", 1)\n",
        "cfg.main_model = model\n",
        "cfg.n_layers = N_LAYERS\n",
        "cfg.n_heads = N_HEADS\n",
        "cfg.d_model = D_MODEL\n",
        "cfg.d_head = D_HEAD\n",
        "cfg.set_seed(673023)\n",
        "\n",
        "print(\"cfg.repo_name=\"+cfg.repo_name, \"cfg.model_name=\"+cfg.model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjjpDQ2_cS2-"
      },
      "source": [
        "# Generate test data and experiment runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76YZ8VErsHjP"
      },
      "outputs": [],
      "source": [
        "# Generate a batch of prompts with 3 field names\n",
        "def generate_batch(batch_size):\n",
        "    cfg.batch_size = batch_size\n",
        "\n",
        "    return qts.generate_csn(batch_size=batch_size, csn=max(1,cs_num), min_cols=3, max_cols=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBS0v8Gir-d4"
      },
      "outputs": [],
      "source": [
        "# Check if the model can correctly predict the question's answer\n",
        "def run_attention_experiment(run_layer_idx, run_head_idx, run_token_idx, all_tokens, run_prompt_tokens, run_generate, make_changes = True, extra_tokens=0):\n",
        "\n",
        "    assert len(all_tokens) == run_prompt_tokens + run_generate\n",
        "\n",
        "    if run_generate == 0:\n",
        "        return True, \"\", \"\"\n",
        "\n",
        "    start = run_head_idx * D_HEAD\n",
        "    end = (run_head_idx + 1) * D_HEAD\n",
        "\n",
        "    with model.generate(all_tokens[:run_prompt_tokens], max_new_tokens=run_generate + extra_tokens, pad_token_id=model.tokenizer.eos_token_id):\n",
        "\n",
        "        if make_changes:\n",
        "            model.transformer.h[run_layer_idx].output[0][:, run_token_idx, start:end] = 0\n",
        "\n",
        "        final_output = model.generator.output.save()\n",
        "\n",
        "    # Did the output change?\n",
        "    decoded_input = model.tokenizer.decode(all_tokens, skip_special_tokens=True)\n",
        "    decoded_output = model.tokenizer.decode(final_output[0], skip_special_tokens=True)\n",
        "    same = decoded_input==decoded_output\n",
        "\n",
        "    return same, decoded_input, decoded_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the model can correctly predict the question's answer and print result\n",
        "def check_experiment(prompt, answer, all_tokens, run_prompt_tokens, run_generate, make_changes = True, show_same = True):\n",
        "\n",
        "    same, decoded_input, decoded_output = run_attention_experiment(0, 0, 0, all_tokens, run_prompt_tokens, run_generate, make_changes, extra_tokens=100)\n",
        "\n",
        "    if not same:\n",
        "        print( \"Ignoring example that model doesn't predict correctly:\")\n",
        "\n",
        "    if (not same) or show_same:\n",
        "        print(\"Prompt (\"+str(run_prompt_tokens)+\") :\", prompt.replace('\\n', '\\\\n'))\n",
        "        print(\"Answer (\"+str(run_generate)+\") :\", answer.replace('\\n', '\\\\n'))\n",
        "        print(\"Output:\", decoded_output.replace('\\n', '\\\\n'))\n",
        "        print()\n",
        "\n",
        "    return same, decoded_input, decoded_output"
      ],
      "metadata": {
        "id": "e1Ze2PY7Nber"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check that generators work\n",
        "def check_generator_on_clean_input(examples):\n",
        "    num_good = 0\n",
        "    for example in examples:\n",
        "        prompt = example.get_alpaca_prompt()\n",
        "        answer = example.sql_statement\n",
        "\n",
        "        prompt_tokens = model.tokenizer(prompt)[\"input_ids\"]\n",
        "        answer_tokens = model.tokenizer(answer)[\"input_ids\"]\n",
        "        all_tokens = prompt_tokens + answer_tokens\n",
        "\n",
        "        num_answer_tokens = len(answer_tokens)\n",
        "        num_prompt_tokens = len(prompt_tokens)\n",
        "\n",
        "        same, _, decoded_output = check_experiment(prompt, answer, all_tokens, num_prompt_tokens, num_answer_tokens, make_changes = False, show_same = False)\n",
        "        if same:\n",
        "            num_good += 1\n",
        "\n",
        "    # Almost all examples should succeed\n",
        "    assert num_good >= len(examples) - 1\n",
        "\n",
        "\n",
        "generator = qts.CorruptFeatureTestGenerator(model_num=model_num, cs_num=cs_num, tokenizer=model.tokenizer, use_novel_names=True)\n",
        "check_generator_on_clean_input( generator.generate_feature_examples(qts.ENGFIELDNAME, 6) )\n",
        "\n",
        "check_generator_on_clean_input( qts.generate_csn(batch_size=6, csn=max(1,cs_num), min_cols=3, max_cols=3) )"
      ],
      "metadata": {
        "id": "XuCHN1hWfeBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ju8wtYel8xRm"
      },
      "outputs": [],
      "source": [
        "# Return a list of experiments to run\n",
        "def get_experiment_list(examples, by_attention_head):\n",
        "    run_list = []\n",
        "    max_prompt_tokens = 0\n",
        "    max_answer_tokens = 0\n",
        "    max_tokens = 0\n",
        "    show_examples = 3\n",
        "\n",
        "    for example in examples:\n",
        "        prompt = example.get_alpaca_prompt()\n",
        "        answer = example.sql_statement\n",
        "\n",
        "        prompt_tokens = model.tokenizer(prompt)[\"input_ids\"]\n",
        "        answer_tokens = model.tokenizer(answer)[\"input_ids\"]\n",
        "        all_tokens = prompt_tokens + answer_tokens\n",
        "\n",
        "        num_answer_tokens = len(answer_tokens)\n",
        "        num_prompt_tokens = len(prompt_tokens)\n",
        "        num_tokens = len(all_tokens)\n",
        "        assert num_tokens == num_prompt_tokens + num_answer_tokens\n",
        "\n",
        "        max_prompt_tokens = max(max_prompt_tokens, num_prompt_tokens)\n",
        "        max_answer_tokens = max(max_answer_tokens, num_answer_tokens)\n",
        "        max_tokens = max(max_tokens, num_tokens)\n",
        "\n",
        "        # Check that the model can correctly predict the question's answer\n",
        "        same, _, decoded_output = check_experiment(prompt, answer, all_tokens, num_prompt_tokens, num_answer_tokens, make_changes = False, show_same = show_examples > 0)\n",
        "\n",
        "        if not same:\n",
        "            continue\n",
        "        show_examples -= 1\n",
        "\n",
        "        if by_attention_head:\n",
        "            for layer_idx in range(N_LAYERS):\n",
        "                for head_idx in range(N_HEADS):\n",
        "                    for token_idx in range(num_tokens):\n",
        "                        # Important logic:\n",
        "                        # num_prompt_tokens and num_answer_tokens are the normal interpretation of the prompt/answer sizes.\n",
        "                        # If ablating a token in the MIDDLE of the PROMPT, we provide ALL the prompt tokens, and generate num_answer_tokens.\n",
        "                        # If ablating a token in the MIDDLE of the ANSWER, we increase the size of the \"prompt\" and decrease the generated tokens.\n",
        "                        exp_prompt_tokens = max(num_prompt_tokens, token_idx+1)\n",
        "                        exp_num_generate = num_tokens - exp_prompt_tokens\n",
        "\n",
        "                        assert num_tokens == exp_prompt_tokens + exp_num_generate\n",
        "\n",
        "                        run_list.append([layer_idx, head_idx, token_idx, all_tokens, exp_prompt_tokens, exp_num_generate])\n",
        "        else:\n",
        "            for token_idx in range(num_prompt_tokens):\n",
        "                run_list.append([prompt, answer, token_idx, prompt_tokens])\n",
        "\n",
        "    return run_list, max_prompt_tokens, max_answer_tokens, max_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wdUVEfAcXJa"
      },
      "source": [
        "# Which token positions are useful?\n",
        "This information is used to shrink the size of search spaces in following sections. For the SQL model all token positions are useful"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETOrSFQqVSG6"
      },
      "outputs": [],
      "source": [
        "N_BATCH = 50\n",
        "\n",
        "def run_token_experiments( ):\n",
        "    examples = generate_batch(N_BATCH)\n",
        "    run_list, max_prompt_tokens, max_good_answer_tokens, max_tokens = get_experiment_list(examples, False)\n",
        "    cfg.initialize_token_positions( max_prompt_tokens, max_good_answer_tokens, True )\n",
        "    num_exps = len(run_list)\n",
        "\n",
        "    print(\"N_BATCH=\"+str(N_BATCH), \"max_prompt_tokens=\"+str(max_prompt_tokens), \"max_good_answer_tokens=\"+str(max_good_answer_tokens), \"max_tokens=\"+str(max_tokens), \"num_exps=\"+str(num_exps))\n",
        "\n",
        "    try_results = np.zeros(max_tokens, dtype=int)\n",
        "    fail_results = np.zeros(max_tokens, dtype=int)\n",
        "\n",
        "    for item_num in tqdm.tqdm(range(num_exps)):\n",
        "\n",
        "        run_item = run_list[item_num]\n",
        "        run_prompt, run_answer, run_token_idx, prompt_tokens = run_item\n",
        "\n",
        "        with model.generate(prompt_tokens, max_new_tokens=max_good_answer_tokens, pad_token_id=model.tokenizer.eos_token_id) :\n",
        "\n",
        "            # Zero out just the portion of the output corresponding to this token position\n",
        "            for run_layer_idx in range(N_LAYERS):\n",
        "                model.transformer.h[run_layer_idx].output[0][:, run_token_idx, :] = 0\n",
        "\n",
        "            final_output = model.generator.output.save()\n",
        "\n",
        "        decoded_output = model.tokenizer.decode(final_output[0], skip_special_tokens=True)\n",
        "\n",
        "        # Did the output change?\n",
        "        if run_prompt + run_answer != decoded_output:\n",
        "            fail_results[run_token_idx] += 1\n",
        "        try_results[run_token_idx] += 1\n",
        "\n",
        "    # Compute the failure rate as percentage\n",
        "    failure_rate = (1.0 * fail_results / (try_results + 1e-10)) * 100\n",
        "    failure_rate = np.round(failure_rate, 2)\n",
        "\n",
        "    return max_tokens, fail_results, failure_rate\n",
        "\n",
        "g_max_tokens, g_token_fail_results, g_token_failure_rate = run_token_experiments()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RT7zG1EDdA5C"
      },
      "outputs": [],
      "source": [
        "print( \"Useful token positions:\" )\n",
        "for token_idx in range(len(g_token_failure_rate)):\n",
        "    if g_token_failure_rate[token_idx] > 0 :\n",
        "        cfg.add_useful_position(token_idx)\n",
        "        print( \"Position:\", token_idx, \"    % Fails:\", g_token_failure_rate[token_idx], \"    # Fails:\", g_token_fail_results[token_idx] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZ0aAic8jqiV"
      },
      "outputs": [],
      "source": [
        "#cfg.calc_position_failures_map(g_token_fail_results.tolist())\n",
        "#qmi.save_plt_to_file(cfg=cfg, full_title=\"Failures When Position Ablated\")\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMs7MpPocb6_"
      },
      "source": [
        "# Which token+layer+attention head nodes are useful?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bINHUagozRoo"
      },
      "outputs": [],
      "source": [
        "N_BATCH = 10\n",
        "\n",
        "def run_attention_experiments():\n",
        "    show_diff = True\n",
        "    examples = generate_batch(N_BATCH)\n",
        "    run_list, max_prompt_tokens, max_good_answer_tokens, max_tokens = get_experiment_list(examples, True)\n",
        "    num_exps = len(run_list)\n",
        "\n",
        "    print(\"N_BATCH=\"+str(N_BATCH), \"max_prompt_tokens=\"+str(max_prompt_tokens), \"max_answer_tokens=\"+str(max_good_answer_tokens), \"max_tokens=\"+str(max_tokens), \"num_exps=\"+str(num_exps))\n",
        "\n",
        "    try_results = np.zeros((N_LAYERS, N_HEADS, max_tokens), dtype=int)\n",
        "    fail_results = np.zeros((N_LAYERS, N_HEADS, max_tokens), dtype=int)\n",
        "\n",
        "    for item_num in tqdm.tqdm(range(num_exps)):\n",
        "\n",
        "        run_item = run_list[item_num]\n",
        "        run_layer_idx, run_head_idx, run_token_idx, all_tokens, num_prompt_tokens, num_generate = run_item\n",
        "\n",
        "        same, decoded_input, decoded_output = run_attention_experiment(run_layer_idx, run_head_idx, run_token_idx, all_tokens, num_prompt_tokens, num_generate)\n",
        "\n",
        "        if not same:\n",
        "            fail_results[run_layer_idx, run_head_idx, run_token_idx] += 1\n",
        "            if show_diff:\n",
        "                print(\"Failure when intervening:\", \"Layer=\"+str(run_layer_idx), \"Head=\"+str(run_head_idx), \"Pos=\"+str(run_token_idx), \"NumPrompts=\"+str(num_prompt_tokens), \"NumGenerate=\"+str(num_generate))\n",
        "                print(\"Input :\", decoded_input.replace('\\n', '\\\\n'))\n",
        "                print(\"Output:\", decoded_output.replace('\\n', '\\\\n'))\n",
        "                show_diff = False\n",
        "        try_results[run_layer_idx, run_head_idx, run_token_idx] += 1\n",
        "\n",
        "\n",
        "    # Compute the failure rate as percentage\n",
        "    failure_rate = (1.0 * fail_results / (try_results + 1e-10)) * 100\n",
        "    failure_rate = np.round(failure_rate, 2)\n",
        "\n",
        "    return max_tokens, fail_results, failure_rate, try_results\n",
        "\n",
        "g_max_tokens, g_attn_failure_results, g_attn_failure_rate, g_attn_try_results = run_attention_experiments()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nViXbBVDxUt"
      },
      "outputs": [],
      "source": [
        "cfg.useful_nodes = qmi.UsefulNodeList()\n",
        "for layer_idx in range(N_LAYERS):\n",
        "    for head_idx in range(N_HEADS):\n",
        "        for token_idx in range(g_max_tokens):\n",
        "            fail_perc = int(g_attn_failure_rate[layer_idx, head_idx, token_idx])\n",
        "            if fail_perc > 0 :\n",
        "                # Add percentage failure quanta\n",
        "                node_location = qmi.NodeLocation(token_idx, layer_idx, True, head_idx)\n",
        "                cfg.add_useful_node_tag( node_location, qmi.QType.FAIL.value, str(fail_perc) )\n",
        "\n",
        "cfg.useful_nodes.sort_nodes()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2lbWJrUwY7p"
      },
      "outputs": [],
      "source": [
        "for layer_idx in range(N_LAYERS):\n",
        "    title = \"TinySQL Percentage of Output Changes by Zeroing Activations in Layer \" + str(layer_idx)\n",
        "    plt.imshow(g_attn_failure_rate[layer_idx], cmap=\"viridis\", aspect=\"auto\")\n",
        "    plt.colorbar(label=\"Percentage Change\")\n",
        "    plt.xlabel(\"Token Position\")\n",
        "    plt.ylabel(\"Attention Head\")\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "    qmi.save_plt_to_file(cfg=cfg, full_title=title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAHRK6TOmMac"
      },
      "outputs": [],
      "source": [
        "# cfg.useful_nodes.print_node_tags()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCAu8QO1mNB0"
      },
      "outputs": [],
      "source": [
        "title = \"TinySQL useful attention heads\"\n",
        "ax1, quanta_results, num_results = qmi.calc_quanta_map(\n",
        "    cfg, True, 6,\n",
        "    cfg.useful_nodes, qmi.QType.FAIL.value, \"\", qmi.get_quanta_fail_perc,\n",
        "    combine_identical_cells=False)\n",
        "\n",
        "if num_results > 0:\n",
        "    if cfg.graph_file_suffix > \"\":\n",
        "        print(\"Saving quanta map:\", title)\n",
        "        qmi.save_plt_to_file(cfg=cfg, full_title=title)\n",
        "    else:\n",
        "        ax1.set_title(title + ' ({} nodes)'.format(len(quanta_results)))\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_szu6Trokt2"
      },
      "outputs": [],
      "source": [
        "# Serialize and save the useful nodes list to a temporary CoLab file in JSON format. Manually download.\n",
        "useful_node_json_filename = 'tinysql_bm' + str(model_num) + \"_cs\" + str(cs_num) + '_useful_nodes.json'\n",
        "print( \"Saving useful node list with behavior tags:\", useful_node_json_filename)\n",
        "cfg.useful_nodes.save_nodes(useful_node_json_filename)\n",
        "\n",
        "#TODO: Auto save to Martian wandb"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "z1tQZgLUnojc"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
>>>>>>> 04117cf358b0a2af3bc2a803665fd2deb82dee0a
