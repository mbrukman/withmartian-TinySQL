{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80d6b3b0-a4c8-46ca-b785-bf521ce91b8d",
   "metadata": {},
   "source": [
    "### Load in the SAEs from Huggingface Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c06caf-8e84-46f1-9a55-d9a16f0bae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def install_dependencies():\n",
    "    ! rm -rf sae || True\n",
    "    ! git clone https://github.com/amirabdullah19852020/sae.git\n",
    "    ! cd sae && pip install .\n",
    "    ! git clone https://github.com/withmartian/TinySQL.git\n",
    "    ! cd TinySQL && pip install .\n",
    "\n",
    "install_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52ead171-4c94-4fd6-8d84-90e1e47649a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triton disabled, using eager implementation of SAE decoder.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "os.environ[\"SAE_DISABLE_TRITON\"] = \"1\"\n",
    "\n",
    "import psutil\n",
    "import re\n",
    "\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from IPython.display import display, HTML\n",
    "from typing import Callable\n",
    "from math import ceil\n",
    "from pathlib import Path\n",
    "\n",
    "import nnsight\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import sae\n",
    "import torch\n",
    "import torch.fx\n",
    "\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import snapshot_download\n",
    "import matplotlib.pyplot as plt\n",
    "from nnsight import NNsight, LanguageModel\n",
    "from plotly.subplots import make_subplots\n",
    "from sae import Sae\n",
    "from sae.sae_interp import GroupedSaeOutput, SaeOutput, SaeCollector, LoadedSAES, sql_tagger\n",
    "from sae.sae_plotting import plot_layer_curves, plot_layer_features\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from TinySQL import sql_interp_model_location\n",
    "from TinySQL.training_data.fragments import field_names, table_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c1b085-52e8-446e-8b90-f9c59f2ff9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current process\n",
    "def process_info():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    \n",
    "    # Memory usage in MB\n",
    "    memory_info = process.memory_info()\n",
    "    print(f\"RSS: {memory_info.rss / (1024 ** 2):.2f} MB\")  # Resident Set Size\n",
    "    print(f\"VMS: {memory_info.vms / (1024 ** 2):.2f} MB\") \n",
    "\n",
    "process_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2e5fa6-fcd2-4e48-9d15-f75ffebec6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "repo = \"sql_interp_saes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9215a96-a9b6-42cd-86f5-5e663ae3cdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = \"withmartian/sql_interp_saes\"\n",
    "cache_dir = \"working_directory\"\n",
    "\n",
    "syn=True\n",
    "\n",
    "full_model_name = sql_interp_model_location(model_num=1, cs_num=1, synonym=syn)\n",
    "model_alias = f\"saes_{full_model_name.split('/')[1]}_syn={syn}\"\n",
    "print(model_alias)\n",
    "\n",
    "# Change this to work with another model alias.\n",
    "seed = 42\n",
    "\n",
    "process_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f5596f-ff6f-43fe-be3a-61f457715482",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = Path(\n",
    "    snapshot_download(repo_name, allow_patterns=f\"{model_alias}/*\", local_dir=cache_dir)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b84292-a200-4ccf-b71e-c89228d7289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ff27fe-0c74-46a2-acf2-d16a49e02794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(example):\n",
    "    alpaca_prompt = \"### Instruction: {} ### Context: {} ### Response: {}\"\n",
    "    example['prompt'] = alpaca_prompt.format(example['english_prompt'], example['create_statement'], example['sql_statement'])\n",
    "    example['response'] = example['sql_statement']\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f0a3dd-0975-419f-8480-bd978044d4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_saes = LoadedSAES.load_from_path(\n",
    "    model_alias=model_alias, k=128, cache_dir=cache_dir, dataset_mapper=format_example, \n",
    "    store_activations=False, function_tagger=sql_tagger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3769553c-217f-4314-908b-e88b48404566",
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_collector = SaeCollector(loaded_saes, seed=seed, sample_size=100, averaged_representations_only=False)\n",
    "# sae_collector.get_texts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a74708-dbb6-4499-a593-e96beab82ca9",
   "metadata": {},
   "source": [
    "### Maximally activating latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715075f3-5a76-43aa-a5d6-ac1b987aed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_and_sort_weights(acts, indices):\n",
    "    \"\"\"\n",
    "    Compute the summed weights of each index and sort them in descending order.\n",
    "\n",
    "    Parameters:\n",
    "    acts (list of list of float): Nested list of scores.\n",
    "    indices (list of list of int): Nested list of indices corresponding to scores.\n",
    "\n",
    "    Returns:\n",
    "    list of tuple: Sorted elements by summed weights in descending order.\n",
    "    \"\"\"\n",
    "    # Dictionary to store summed weights for each index\n",
    "    weights = {}\n",
    "    numel = 0\n",
    "\n",
    "    for act_row, idx_row in zip(acts, indices):\n",
    "        numel+=1\n",
    "        for score, idx in zip(act_row, idx_row):\n",
    "            weights[idx] = weights.get(idx, 0) + score\n",
    "\n",
    "    for element in weights:\n",
    "        weights[element]/=(numel or 1)\n",
    "\n",
    "    # Sort by summed weight in descending order\n",
    "    sorted_weights = sorted(weights.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return sorted_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc71c14f-4bf8-42a3-84ab-4d8fee92e8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_element = sae_collector.encoded_set[4][\"encoding\"]\n",
    "single_sae_output = single_element.sae_outputs_by_layer['transformer.h.0.attn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c9f9d0-179e-4f47-a0a5-1ff2c8f09b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(single_sae_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd89f1ee-200c-481e-a402-75453693ce1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(single_element.text)\n",
    "print(single_element.search_indices_with_tag(\"TABLE\"))\n",
    "single_element.tags_by_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6ced6f-87f3-41fe-be6f-6e44715f4e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(single_sae_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc4e93c-7fcf-4fb2-a904-988e5f6a1215",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model = loaded_saes.language_model\n",
    "one_prompt = sae_collector.mapped_dataset['train']['prompt'][0]\n",
    "tokenizer = loaded_saes.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e945316d-9a76-4dbf-89c3-4ae89e8ceee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(one_prompt, return_tensors=\"pt\")\n",
    "\n",
    "with language_model.trace() as tracer:\n",
    "    with tracer.invoke(inputs) as invoker:\n",
    "        layer_output = language_model.transformer.h[0].output[0].save()\n",
    "\n",
    "layer_output\n",
    "\n",
    "# model.transformer.h[layer_idx].output = (modified_output,) + model.transformer.h[layer_idx].output[1:]\n",
    "# final_output = model.lm_head.output.argmax(dim=-1).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e12cce8-3366-4051-8174-8b7e960ced90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_probs(language_model, tokenizer, text):\n",
    "    tokenized = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    input_ids = tokenized[\"input_ids\"].cpu()\n",
    "\n",
    "    tokens = tokenizer.tokenize(one_prompt)\n",
    "    simple_tokens = [token.replace(\"Ä \", \"\").lower() for token in tokens]\n",
    "    response_index = simple_tokens.index(\"response\")\n",
    "    input_len = len(tokens)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = language_model._model(**tokenized).logits.cpu()\n",
    "        logprobs = F.log_softmax(logits, dim=-1).squeeze(0)\n",
    "        correct_logprobs = logprobs[torch.arange(input_len), input_ids][0]\n",
    "        \n",
    "        response_logprobs = correct_logprobs[response_index:]\n",
    "        num_element = response_logprobs.numel()\n",
    "        total_logprobs = response_logprobs.sum()/num_element\n",
    "\n",
    "    return total_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602337a1-6f1f-4233-93f2-c44163cac28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_saes.language_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874720fb-ae80-47f4-927d-db96cf76b26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = \"RESPONSE_FIELD\"\n",
    "\n",
    "def get_sorted_weights_by_layer(sae_collector, tag):\n",
    "    results = sae_collector.get_all_sae_outputs_for_tag(tag)\n",
    "    aggregated_sae_features = {}\n",
    "    layers = sae_collector.layers\n",
    "    for layer in layers:\n",
    "        all_top_acts = []\n",
    "        all_top_indices = []\n",
    "        for element in tqdm(results):\n",
    "            all_top_acts.extend(element[layer].top_acts)\n",
    "            all_top_indices.extend(element[layer].top_indices)\n",
    "\n",
    "    \n",
    "        sorted_weights = compute_and_sort_weights(all_top_acts, all_top_indices)\n",
    "        aggregated_sae_features[layer] = {\"top_acts\": all_top_acts, \"top_indices\": all_top_indices, \"sorted_weights\": sorted_weights}\n",
    "    return aggregated_sae_features\n",
    "\n",
    "\n",
    "sorted_weights = get_sorted_weights_by_layer(sae_collector, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0884d023-8570-4299-b276-74dad62a2a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_features(sorted_weights, tag, top_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9457e6a-f52d-4f61-8867-f4f822177de1",
   "metadata": {},
   "source": [
    "**Get the maximum weight of a feature in an element.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a043b4-bc48-46e0-bb3f-a1a28fc89a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "element = sae_collector.encoded_set[2][\"encoding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba952236-11ef-4c6c-a3ec-c43c4faf3a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_feature = 11707\n",
    "target_layer = \"transformer.h.0.attn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7d8724-73f3-4c8f-9680-3b60ce3d8a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_element = element.sae_outputs_by_layer[target_layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a29116-df8a-48f0-b11e-174f8b03b17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "element.get_max_weight_of_feature(target_layer, target_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3088186c-aa4e-41d8-b295-8f219ea582dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = loaded_saes.map_to_attention_head(layer_name=target_layer, feature_num=target_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0013b15-43b8-4bb9-8bd2-f30716e8df37",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, visualizations = sae_collector.get_maximally_activating_datasets(target_layer, target_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26077dab-e86b-40ba-9b4f-46e49787a4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(texts[4][0][\"encoding\"].tags_by_index)\n",
    "texts[4][0][\"encoding\"].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2044bc14-a42f-4dfa-aeb9-92e76d0fe966",
   "metadata": {},
   "outputs": [],
   "source": [
    "for visualization in visualizations:\n",
    "    display(visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d160bc6f-4c99-4370-a5ca-228a038f8398",
   "metadata": {},
   "source": [
    "### Monitor reconstruction Errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f34e36-f869-47e4-90c5-c133ce4259d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_error_by_k_and_layer = sae_collector.get_avg_reconstruction_error_for_all_k_and_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9c8c48-9e04-4696-923f-0a5184a2c159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruction_error_by_k_and_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a1d6ef-3e6b-4da0-8386-7c9ba7064232",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_curves(reconstruction_error_by_k_and_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ab33a6-d645-4d78-a04d-ca22bffd53f7",
   "metadata": {},
   "source": [
    "### Monitor Ablation Errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85d7ccf-f329-48ba-ab2d-44503dbd6266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_heads(model, prompt_text, target_layer):\n",
    "    N_HEADS = 16\n",
    "    inputs = model.tokenizer(prompt_text, return_tensors=\"pt\")\n",
    "\n",
    "    with model.trace() as tracer:\n",
    "        with tracer.invoke(inputs) as invoker:\n",
    "            for layer_idx in target_layers:\n",
    "                layer_output = model.transformer.h[layer_idx].output[0]\n",
    "                target_heads = heads_per_layer[layer_idx]\n",
    "\n",
    "                output_reshaped = einops.rearrange(\n",
    "                    layer_output,\n",
    "                    'b s (nh dh) -> b s nh dh',\n",
    "                    nh=N_HEADS\n",
    "                )\n",
    "\n",
    "                for head_idx in range(N_HEADS):\n",
    "                    if head_idx not in target_heads:\n",
    "                        output_reshaped[:, :, head_idx, :] = 0\n",
    "\n",
    "                modified_output = einops.rearrange(\n",
    "                    output_reshaped,\n",
    "                    'b s nh dh -> b s (nh dh)',\n",
    "                    nh=N_HEADS\n",
    "                )\n",
    "\n",
    "                model.transformer.h[layer_idx].output = (modified_output,) + model.transformer.h[layer_idx].output[1:]\n",
    "\n",
    "            final_output = model.lm_head.output.argmax(dim=-1).save()\n",
    "\n",
    "    print(\"Modified Output:\", model.tokenizer.decode(final_output[0][-1]))\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e3f5ec-5696-461c-b1f8-376a7c3961ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_heads(model, prompt_text, target_layer):\n",
    "    N_HEADS = 16\n",
    "    inputs = model.tokenizer(prompt_text, return_tensors=\"pt\")\n",
    "\n",
    "    with model.trace() as tracer:\n",
    "        with tracer.invoke(inputs) as invoker:\n",
    "            for layer_idx in target_layers:\n",
    "                layer_output = model.transformer.h[layer_idx].output[0]\n",
    "                target_heads = heads_per_layer[layer_idx]\n",
    "\n",
    "                output_reshaped = einops.rearrange(\n",
    "                    layer_output,\n",
    "                    'b s (nh dh) -> b s nh dh',\n",
    "                    nh=N_HEADS\n",
    "                )\n",
    "\n",
    "                for head_idx in range(N_HEADS):\n",
    "                    if head_idx not in target_heads:\n",
    "                        output_reshaped[:, :, head_idx, :] = 0\n",
    "\n",
    "                modified_output = einops.rearrange(\n",
    "                    output_reshaped,\n",
    "                    'b s nh dh -> b s (nh dh)',\n",
    "                    nh=N_HEADS\n",
    "                )\n",
    "\n",
    "                model.transformer.h[layer_idx].output = (modified_output,) + model.transformer.h[layer_idx].output[1:]\n",
    "\n",
    "            final_output = model.lm_head.output.argmax(dim=-1).save()\n",
    "\n",
    "    print(\"Modified Output:\", model.tokenizer.decode(final_output[0][-1]))\n",
    "    return final_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
