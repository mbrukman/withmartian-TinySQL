{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80d6b3b0-a4c8-46ca-b785-bf521ce91b8d",
   "metadata": {},
   "source": [
    "### Load in the SAEs from Huggingface Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5343ca6b-38c9-46ab-aadd-a60a3f4f19f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip uninstall triton -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ead171-4c94-4fd6-8d84-90e1e47649a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import psutil\n",
    "import re\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import nnsight\n",
    "import numpy as np\n",
    "import sae\n",
    "import torch\n",
    "import torch.fx\n",
    "\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import snapshot_download\n",
    "from nnsight import NNsight, LanguageModel\n",
    "from sae import Sae\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from TinySQL import sql_interp_model_location\n",
    "from TinySQL.training_data.fragments import field_names, table_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c1b085-52e8-446e-8b90-f9c59f2ff9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current process\n",
    "def process_info():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    \n",
    "    # Memory usage in MB\n",
    "    memory_info = process.memory_info()\n",
    "    print(f\"RSS: {memory_info.rss / (1024 ** 2):.2f} MB\")  # Resident Set Size\n",
    "    print(f\"VMS: {memory_info.vms / (1024 ** 2):.2f} MB\") \n",
    "\n",
    "process_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1e3940-f7c3-4362-a390-8d103ab55998",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(table_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2e5fa6-fcd2-4e48-9d15-f75ffebec6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_table_names = table_names.get_TableInfo()\n",
    "dir(field_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9215a96-a9b6-42cd-86f5-5e663ae3cdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = \"withmartian/sql_interp_saes\"\n",
    "\n",
    "# Change this to work with another model alias.\n",
    "model_alias = \"saes_bm1_cs1\"\n",
    "cache_dir = \"working_directory\"\n",
    "seed = 42\n",
    "\n",
    "process_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91567a0a-9f42-4597-ac28-6cbfa56ee7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelDatasetFullName:\n",
    "    model_id: str\n",
    "    dataset_id: str\n",
    "    full_dataset_name: str\n",
    "    full_model_name: str\n",
    "\n",
    "    def load_model(self):\n",
    "        model = AutoModelForCausalLM.from_pretrained(full_model_name)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3878fa9-d799-4503-b84f-2d03fc689567",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SaeOutput:\n",
    "    sae_name: str\n",
    "    sae: Sae\n",
    "    text: str\n",
    "    tokens: list[str]\n",
    "    raw_acts: list[float]\n",
    "    top_acts: list[float]\n",
    "    top_indices: list[int]\n",
    "\n",
    "    def zero_out_except_top_n(self, scores, indices, n):\n",
    "        \"\"\"\n",
    "        Zero out all but the top n scores in the scores vector, preserving the order.\n",
    "        \"\"\"\n",
    "        if len(scores) != len(indices):\n",
    "            raise ValueError(\"Scores and indices lists must have the same length.\")\n",
    "    \n",
    "        if n <= 0:\n",
    "            return [0] * len(scores), indices\n",
    "    \n",
    "        # Pair scores with their original indices\n",
    "        paired = list(zip(scores, range(len(scores))))\n",
    "    \n",
    "        # Sort by score in descending order\n",
    "        paired.sort(key=lambda x: -x[0])\n",
    "    \n",
    "        # Get the indices of the top n scores\n",
    "        top_n_indices = [index for _, index in paired[:n]]\n",
    "    \n",
    "        # Create a new scores list with only the top n scores retained\n",
    "        filtered_scores = [scores[i] if i in top_n_indices else 0 for i in range(len(scores))]\n",
    "    \n",
    "        return filtered_scores, indices\n",
    "\n",
    "    def zero_out_except_top_n_for_multiple(self, scores_list, indices_list, n):\n",
    "        if len(scores_list) != len(indices_list):\n",
    "            raise ValueError(\"Scores and indices lists must have the same length.\")\n",
    "    \n",
    "        filtered_scores_list = []\n",
    "        filtered_indices_list = []\n",
    "    \n",
    "        for scores, indices in zip(scores_list, indices_list):\n",
    "            filtered_scores, filtered_indices = self.zero_out_except_top_n(scores, indices, n)\n",
    "            filtered_scores_list.append(filtered_scores)\n",
    "            filtered_indices_list.append(filtered_indices)\n",
    "    \n",
    "        return filtered_scores_list, filtered_indices_list\n",
    "\n",
    "    def reconstruction_error(self, k=128):\n",
    "        decoded_activations = self.decode_to_activations(k)\n",
    "        raw_acts = torch.tensor(self.raw_acts).cuda()\n",
    "\n",
    "        difference = decoded_activations - raw_acts\n",
    "\n",
    "        reconstruction_error = torch.norm(difference) / torch.norm(raw_acts)\n",
    "        return reconstruction_error.item()\n",
    "\n",
    "    def decode_to_activations(self, k=128):\n",
    "        filtered_acts, top_k_indices = self.zero_out_except_top_n_for_multiple(self.top_acts.copy(), self.top_indices.copy(), n=k)\n",
    "        return self.sae.decode(top_acts=torch.tensor(filtered_acts).cuda(), top_indices=torch.tensor(top_k_indices).cuda())\n",
    "\n",
    "@dataclass \n",
    "class GroupedSaeOutput:\n",
    "    sae_outputs_by_layer: dict[str, SaeOutput]\n",
    "    text: str\n",
    "    tokens: list[str]\n",
    "    tags: list[str]\n",
    "    tags_by_index: list[str]\n",
    "\n",
    "    def apply_tags(self):\n",
    "        self.tags = []\n",
    "        for token in self.tokens:\n",
    "            if token in table_names:\n",
    "                self.tags.append(\"TABLE\")\n",
    "            elif token in field_name:\n",
    "                self.tags.append(\"FIELD\")\n",
    "            else:\n",
    "                self.tags.append(\"NONE\")\n",
    "\n",
    "    def get_reconstruction_error_by_layer(self, layer, k):\n",
    "        return sae_outputs_by_layer[layer].recontruction_error(k=k)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f5596f-ff6f-43fe-be3a-61f457715482",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = Path(\n",
    "    snapshot_download(repo_name, allow_patterns=f\"{model_alias}/*\", local_dir=cache_dir)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ff27fe-0c74-46a2-acf2-d16a49e02794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(example):\n",
    "    alpaca_prompt = \"### Instruction: {} ### Context: {} ### Response: \"\n",
    "    example['prompt'] = alpaca_prompt.format(example['english_prompt'], example['create_statement'])\n",
    "    example['response'] = example['sql_statement']\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83cbdd3-5fee-459f-b5ab-e1ea381227aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadedSAES:\n",
    "    def __init__(self, dataset_name: str, full_model_name: str, model_alias: str,\n",
    "        tokenizer: AutoTokenizer, language_model: LanguageModel, layers: list[str],\n",
    "        layer_to_directory: dict, k: str, base_path: str, layer_to_saes: dict):\n",
    "\n",
    "        self.dataset_name = dataset_name\n",
    "        self.full_model_name = full_model_name\n",
    "        self.model_alias = model_alias\n",
    "        self.tokenizer = tokenizer\n",
    "        self.language_model = language_model\n",
    "        self.layers = layers\n",
    "        self.layer_to_directory = layer_to_directory\n",
    "        self.k = k\n",
    "        self.base_path = base_path\n",
    "        self.layer_to_saes = layer_to_saes\n",
    "\n",
    "        self.dataset = self.get_dataset()\n",
    "        self.mapped_dataset = self.dataset.map(format_example)\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def get_all_subdirectories(path):\n",
    "        subdirectories = [\n",
    "            os.path.join(path, name) for name in os.listdir(path) \n",
    "            if os.path.isdir(os.path.join(path, name)) and not name.startswith(\".\")\n",
    "        ]\n",
    "        return subdirectories\n",
    "\n",
    "    def nnsight_eval_string_for_layer(self, layer: str):\n",
    "        \"\"\"\n",
    "        Converts transformer.h[0].mlp into self.language_model.transformer.h[0].mlp.output.save() for nnsight\n",
    "        \"\"\"\n",
    "        subbed_layer = re.sub(r'\\.([0-9]+)\\.', r'[\\1].', layer)\n",
    "        return f\"self.language_model.{subbed_layer}.output.save()\"\n",
    "\n",
    "    def encode_to_activations_for_layer(self, text: str, layer: str):\n",
    "        if \"bm1\" in self.full_model_name:\n",
    "            with self.language_model.trace() as tracer:\n",
    "                with tracer.invoke(text) as invoker:\n",
    "                    eval_string = self.nnsight_eval_string_for_layer(layer)\n",
    "                    my_output = eval(eval_string)\n",
    "        if len(my_output) > 1:\n",
    "            return my_output[0]\n",
    "        else:\n",
    "            return my_output\n",
    "\n",
    "    def encode_to_sae_for_layer(self, text: str, layer: str):\n",
    "        activations = self.encode_to_activations_for_layer(text, layer).cuda()\n",
    "        raw_acts = activations[0].cpu().detach().numpy().tolist()\n",
    "        relevant_sae = self.layer_to_saes[layer]\n",
    "        sae_acts_and_features = relevant_sae.encode(activations)\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "\n",
    "        top_acts = sae_acts_and_features.top_acts[0].cpu().detach().numpy().tolist()\n",
    "        top_indices = sae_acts_and_features.top_indices[0].cpu().detach().numpy().tolist()\n",
    "\n",
    "        sae_output = SaeOutput(\n",
    "            sae_name=layer, text=text, tokens=tokens, top_acts=top_acts, top_indices=top_indices, raw_acts=raw_acts, sae=relevant_sae\n",
    "        )\n",
    "        \n",
    "        return sae_output\n",
    "\n",
    "    def encode_to_all_saes(self, text: str):\n",
    "        sae_outputs_by_layer = {layer: self.encode_to_sae_for_layer(text=text, layer=layer) for layer in self.layers}\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        result = GroupedSaeOutput(sae_outputs_by_layer=sae_outputs_by_layer, text=text, tokens=tokens, tags=[], tags_by_index=[])\n",
    "        result.apply_tags()\n",
    "\n",
    "    @staticmethod\n",
    "    def load_from_path(model_alias: str, k: str):\n",
    "        k = str(k)\n",
    "        \n",
    "        base_path = f\"{cache_dir}/{model_alias}/k={k}\"\n",
    "        \n",
    "        print(f\"Loading from path {base_path}\")\n",
    "        subdirectories = LoadedSAES.get_all_subdirectories(base_path)\n",
    "        \n",
    "        layer_to_directory = {\n",
    "            directory.split(\"/\")[-1] : directory for directory in subdirectories\n",
    "        }\n",
    "\n",
    "        layer_to_directory = {layer: directory for layer, directory in layer_to_directory.items() if \"1\" in layer}\n",
    "        layers = sorted(list(layer_to_directory.keys()))\n",
    "\n",
    "        with open(f\"{base_path}/model_config.json\",  \"r\") as f_in:\n",
    "            model_config = json.load(f_in)\n",
    "\n",
    "            dataset_name = model_config[\"dataset_name\"]\n",
    "            full_model_name = model_config[\"model_name\"]\n",
    "            language_model = LanguageModel(full_model_name)\n",
    "            tokenizer = language_model.tokenizer\n",
    "\n",
    "        layer_to_saes = {layer: Sae.load_from_disk(directory).cuda() for layer, directory in layer_to_directory.items()}\n",
    "\n",
    "        return LoadedSAES(dataset_name=dataset_name, full_model_name=full_model_name,\n",
    "                          model_alias=model_alias, layers=layers, layer_to_directory=layer_to_directory,\n",
    "                          tokenizer=tokenizer, k=k, base_path=base_path,\n",
    "                          layer_to_saes=layer_to_saes, language_model=language_model)\n",
    "\n",
    "    def get_dataset(self):\n",
    "        return load_dataset(self.dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f0a3dd-0975-419f-8480-bd978044d4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_saes = LoadedSAES.load_from_path(model_alias=model_alias, k=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b903ab-c4fe-40c4-82d5-3a0705845e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaeCollector:\n",
    "    \"\"\"\n",
    "    This class is responsible for collecting a large amount of text,\n",
    "    assigning tags to each token for a feature name, and also SAE outputs.\n",
    "    These can then be used for probes and feature analysis.\n",
    "    (Still to add: ablations.)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, loaded_saes, sample_size=2, restricted_tags=None):\n",
    "        self.loaded_saes = loaded_saes\n",
    "        self.restricted_tags = restricted_tags or []\n",
    "        self.sample_size = sample_size\n",
    "        self.mapped_dataset = loaded_saes.mapped_dataset\n",
    "        self.mapped_dataset.shuffle(seed=seed)\n",
    "        self.tokenizer = self.loaded_saes.tokenizer\n",
    "        self.layers = self.loaded_saes.layers\n",
    "        self.encoded_set = self.create_and_load_random_subset(sample_size=self.sample_size)\n",
    "\n",
    "    def get_prompt_and_encoding_for_text(self, feature):\n",
    "        prompt = feature[\"prompt\"]\n",
    "        response = feature[\"response\"]\n",
    "        encoding = self.loaded_saes.encode_to_all_saes(prompt)\n",
    "\n",
    "        return_dict = {\n",
    "            \"prompt\": prompt,\n",
    "            \"response\": response,\n",
    "            \"encoding\": encoding\n",
    "        }\n",
    "        return return_dict\n",
    "\n",
    "    def get_avg_reconstruction_error_for_all_k_and_layers(self):\n",
    "        all_reconstruction_errors = {layer: self.get_avg_reconstruction_error_for_all_k(layer) for layer in self.layers}\n",
    "        return all_reconstruction_errors\n",
    "\n",
    "    def get_avg_reconstruction_error_for_all_k(self, layer, min_range=0, max_range=128):\n",
    "        all_reconstruction_errors = {}\n",
    "        for element in tqdm(self.encoded_set):\n",
    "            encoding = element[\"encoding\"]\n",
    "            for k in range(min_range, max_range):\n",
    "                recon_error = encoding.sae_outputs_by_layer[layer].reconstruction_error(k)\n",
    "                curr_reconstruction_error_list = all_reconstruction_errors.get(k, [])\n",
    "                curr_reconstruction_error_list.append(recon_error)\n",
    "                all_reconstruction_errors[k] = curr_reconstruction_error_list\n",
    "\n",
    "        average_reconstruction_errors = {k: np.average(error_list) for k, error_list in all_reconstruction_errors.items()}\n",
    "        return average_reconstruction_errors\n",
    "\n",
    "    def create_and_load_random_subset(self, sample_size: int):\n",
    "        sampled_set = self.mapped_dataset['train'].select(range(sample_size))\n",
    "        encoded_set = []\n",
    "        for element in tqdm(sampled_set):\n",
    "            encoded_element = self.get_prompt_and_encoding_for_text(element)\n",
    "            encoded_set.append(encoded_element)\n",
    "        return encoded_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3769553c-217f-4314-908b-e88b48404566",
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_collector = SaeCollector(loaded_saes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a74708-dbb6-4499-a593-e96beab82ca9",
   "metadata": {},
   "source": [
    "### Maximally activating datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874720fb-ae80-47f4-927d-db96cf76b26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "element = sae_collector.encoded_set[0][\"encoding\"]\n",
    "element.tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d160bc6f-4c99-4370-a5ca-228a038f8398",
   "metadata": {},
   "source": [
    "### Monitor reconstruction Errors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f34e36-f869-47e4-90c5-c133ce4259d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_error_by_k_and_layer = sae_collector.get_avg_reconstruction_error_for_all_k_and_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9c8c48-9e04-4696-923f-0a5184a2c159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruction_error_by_k_and_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ab33a6-d645-4d78-a04d-ca22bffd53f7",
   "metadata": {},
   "source": [
    "### Monitor Ablation Errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e7e998-46c3-406c-a861-5a151cde3a9c",
   "metadata": {},
   "source": [
    "### Add taggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b34d6c-7cbb-434f-a992-a5d7363ae6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TinySQL.training_data.fragments import field_names, table_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c88527-e09e-4ca8-9cbb-d1c1bed4a884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import TinySQL\n",
    "help(TinySQL.fragments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9542d3ea-7145-475d-a2e5-4f298c34e8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_element = encoded_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f882da43-92c1-44ae-a629-c1dd9469fb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cdfb13-272b-4555-84f8-34050a44e8b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae3c7f1-5bb6-42bb-ac58-a77cf0ba22c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
