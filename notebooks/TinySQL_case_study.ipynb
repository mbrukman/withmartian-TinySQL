{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F01CPc6-1hAg",
        "outputId": "514b221f-a5c3-4427-c69e-0add550c5b7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U datasets kaleido\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from collections import defaultdict\n",
        "from typing import List, Dict\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "from datasets import load_dataset\n",
        "import random\n",
        "import numpy as np\n",
        "from huggingface_hub import hf_hub_download\n",
        "import os\n",
        "import plotly.graph_objects as go\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BtwoERstIFx"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "#\"withmartian/sql_interp_bm2_cs3_experiment_6.3\"  withmartian/sql_interp_bm3_cs3_experiment_9.3\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"withmartian/sql_interp_bm2_cs3_experiment_6.3\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"withmartian/sql_interp_bm2_cs3_experiment_6.3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8z41QYXotIFy",
        "outputId": "10ed8792-b188-4a94-846b-264a4b7efeb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://****@github.com/withmartian/TinySQL.git@abir_min_circuits\n",
            "  Cloning https://****@github.com/withmartian/TinySQL.git (to revision abir_min_circuits) to /private/var/folders/sh/7x5lnyb57z30b_6spb2t8w9m0000gn/T/pip-req-build-wu68u0wg\n",
            "  Running command git clone --filter=blob:none --quiet 'https://****@github.com/withmartian/TinySQL.git' /private/var/folders/sh/7x5lnyb57z30b_6spb2t8w9m0000gn/T/pip-req-build-wu68u0wg\n",
            "  Running command git checkout -b abir_min_circuits --track origin/abir_min_circuits\n",
            "  Switched to a new branch 'abir_min_circuits'\n",
            "  branch 'abir_min_circuits' set up to track 'origin/abir_min_circuits'.\n",
            "  Resolved https://****@github.com/withmartian/TinySQL.git to commit eac048722d0a6588b86673f6cc8164d83b330ade\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.1 in /opt/homebrew/lib/python3.11/site-packages (from TinySQL==1.3) (1.26.4)\n",
            "Requirement already satisfied: wheel in /opt/homebrew/lib/python3.11/site-packages (from TinySQL==1.3) (0.45.1)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "github_token = 'github-'\n",
        "\n",
        "# !pip install --upgrade git+https://{github_token}@github.com/withmartian/TinySQL.git\n",
        "!pip install --upgrade git+https://{github_token}@github.com/withmartian/TinySQL.git@abir_min_circuits\n",
        "import TinySQL as qts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1x1MxpXDtIFy"
      },
      "outputs": [],
      "source": [
        "model_num = 2                 # 0=GPT2, 1=TinyStories, 2=Qwen, 3=Llama, 4=Granite, 5=SmolLM\n",
        "cs_num = 3                       # 0=BaseModel, 1=CS1, 2=CS2 or 3=CS3\n",
        "feature_name = qts.ENGTABLENAME   # ENGTABLENAME, ENGFIELDNAME, DEFTABLESTART, DEFTABLENAME, DEFFIELDNAME, DEFFIELDSEPARATOR\n",
        "use_novel_names = False           # If True, we corrupt using words not found in the clean prompt or create sql e.g. \"little\" or \"hammer\"\n",
        "use_synonyms_table = False\n",
        "use_synonyms_field = False\n",
        "batch_size = 10\n",
        "\n",
        "\n",
        "# Key global \"input\" variables\n",
        "clean_prompt = \"\"\n",
        "corrupt_prompt = \"\"\n",
        "clean_tokenizer_index = qts.UNKNOWN_VALUE # Tokenizer vocab index for clean word\n",
        "corrupt_tokenizer_index = qts.UNKNOWN_VALUE # Tokenizer vocab index for corrupted word\n",
        "answer_token_index = qts.UNKNOWN_VALUE # Token index in sql command answer of clean/corrupt word\n",
        "\n",
        "# Key global \"results\" variables\n",
        "clean_logit_diff = qts.UNKNOWN_VALUE\n",
        "corrupt_logit_diff = qts.UNKNOWN_VALUE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFFiJw5btIFy"
      },
      "outputs": [],
      "source": [
        "generator = qts.CorruptFeatureTestGenerator(model_num, cs_num, tokenizer, use_novel_names=use_novel_names, use_synonyms_field=use_synonyms_field, use_synonyms_table=use_synonyms_table, use_order_by=True)\n",
        "examples = generator.generate_feature_examples(feature_name, batch_size)\n",
        "\n",
        "# Each examples is corrupted at prompt_token_index. A resulting impact is expected at answer_token_index\n",
        "prompts = []\n",
        "ref_answers = []\n",
        "for i, example in enumerate(examples):\n",
        "    clean_tokenizer_index = example.clean_tokenizer_index\n",
        "    corrupt_tokenizer_index = example.corrupt_tokenizer_index\n",
        "    answer_token_index = example.answer_token_index\n",
        "\n",
        "    # Truncate the clean_prompt at answer_token_index\n",
        "    clean_prompt = example.clean_BatchItem.get_alpaca_prompt() + example.clean_BatchItem.sql_statement\n",
        "    clean_tokens = tokenizer(clean_prompt)[\"input_ids\"]\n",
        "    prompts.append(clean_prompt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlFKlrgPtIFz"
      },
      "outputs": [],
      "source": [
        "target_token = \" duration\" #\" inventory\" #\"COUNT\"\n",
        "target_token_id = tokenizer.encode(target_token, add_special_tokens=False)[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQ-wzgn1tIFz"
      },
      "outputs": [],
      "source": [
        "prompt = '### Instruction: show me the duration and size from the orders table ordered by duration in ascending order ### Context: CREATE TABLE orders ( duration CHAR, size TIME ) ### Response: SELECT duration, size FROM orders ORDER BY'# duration ASC'\n",
        "\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "input_ids = inputs[\"input_ids\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1YrxQU-tIFz"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    outputs = model(input_ids=input_ids, output_hidden_states=True, return_dict=True)\n",
        "    hidden_states = outputs.hidden_states\n",
        "\n",
        "unembedding_matrix = model.lm_head.weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoRDjmcGtIF0",
        "outputId": "922e731e-9344-48fe-cad4-c75cbcf660a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 0: duration logit = 0.01, rank = 100345, top token = ' BY'\n",
            "Layer 1: duration logit = -0.01, rank = 102185, top token = '皿'\n",
            "Layer 2: duration logit = 0.19, rank = 22403, top token = '皿'\n",
            "Layer 3: duration logit = 0.14, rank = 51071, top token = ' locales'\n",
            "Layer 4: duration logit = 0.26, rank = 37460, top token = '正宗'\n",
            "Layer 5: duration logit = 0.53, rank = 14360, top token = ':';\n",
            "'\n",
            "Layer 6: duration logit = 0.60, rank = 12475, top token = '一族'\n",
            "Layer 7: duration logit = 0.57, rank = 18494, top token = '\n",
            "                        \n",
            "'\n",
            "Layer 8: duration logit = 0.54, rank = 11113, top token = 'izione'\n",
            "Layer 9: duration logit = 0.35, rank = 30377, top token = '激活'\n",
            "Layer 10: duration logit = 0.40, rank = 31730, top token = 'ordination'\n",
            "Layer 11: duration logit = 0.40, rank = 40074, top token = '\".\n",
            "\n",
            "\n",
            "\n",
            "'\n",
            "Layer 12: duration logit = 0.41, rank = 49843, top token = '\".\n",
            "\n",
            "\n",
            "\n",
            "'\n",
            "Layer 13: duration logit = 0.33, rank = 61366, top token = '\".\n",
            "\n",
            "\n",
            "\n",
            "'\n",
            "Layer 14: duration logit = 0.60, rank = 27184, top token = '\n",
            "   \n",
            "'\n",
            "Layer 15: duration logit = 0.78, rank = 35105, top token = '\".\n",
            "\n",
            "\n",
            "\n",
            "'\n",
            "Layer 16: duration logit = 0.62, rank = 41964, top token = ' ....\n",
            "\n",
            "'\n",
            "Layer 17: duration logit = 1.15, rank = 2090, top token = '\tvolatile'\n",
            "Layer 18: duration logit = 1.54, rank = 5068, top token = ' consulate'\n",
            "Layer 19: duration logit = 2.28, rank = 3335, top token = ' consulate'\n",
            "Layer 20: duration logit = 2.82, rank = 1699, top token = ' consulate'\n",
            "Layer 21: duration logit = 4.13, rank = 1815, top token = ' Hulu'\n",
            "Layer 22: duration logit = 6.75, rank = 167, top token = ' Hulu'\n",
            "Layer 23: duration logit = 8.04, rank = 23, top token = ' />)\n",
            "'\n",
            "Layer 24: duration logit = 31.48, rank = 1, top token = ' duration'\n"
          ]
        }
      ],
      "source": [
        "for layer_idx, layer_hidden in enumerate(hidden_states):\n",
        "    # Choose position to probe — usually the last token of the prompt\n",
        "    token_position = -1  # or -2 to go earlier\n",
        "    residual = layer_hidden[0, token_position]  # shape: [hidden_dim]\n",
        "\n",
        "    logits = torch.matmul(unembedding_matrix, residual)  # [vocab_size]\n",
        "\n",
        "    # Logit and rank for COUNT\n",
        "    count_logit = logits[target_token_id].item()\n",
        "    rank = torch.argsort(logits, descending=True).tolist().index(target_token_id) + 1\n",
        "\n",
        "    # Top predicted token\n",
        "    top_token_id = torch.argmax(logits).item()\n",
        "    top_token = tokenizer.decode([top_token_id])\n",
        "\n",
        "    print(f\"Layer {layer_idx}: duration logit = {count_logit:.2f}, rank = {rank}, top token = '{top_token}'\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}