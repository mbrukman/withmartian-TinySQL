{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQAsYjSQVXHC"
      },
      "source": [
        "# Ablation of non-EAP circuits\n",
        "\n",
        "**Background:** A \"TinySQL\" model takes as input 1) An Instruction, which is an english data request sentence and 2) A Context, which is a SQL table create statement. The model outputs a Response, which is a SQL select statement.  \n",
        "\n",
        "**Notebook purpose:** Do mean or zero ablation on the circuits of the models that were not selected through EAP.\n",
        "\n",
        "**Notebook details:** This notebook:\n",
        "- Was developed on Google Colab using an A100\n",
        "- Runs with M1 (TinyStories) with base/CS1/CS2/CS3 models.\n",
        "- Requires a GITHUB_TOKEN secret to access Martian TinySQL code repository.\n",
        "- Requires a HF_TOKEN secret to access Martian HuggingFace repository.\n",
        "- Was developed under a grant provided by withmartian.com ( https://withmartian.com )\n",
        "- Relies on the nnsight library. Also refer the https://nnsight.net/notebooks/tutorials/activation_patching/ tutorial\n",
        "- Relies on the https://github.com/PhilipQuirke/quanta_mech_interp library for graphing useful nodes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1tQZgLUnojc"
      },
      "source": [
        "# Import libraries\n",
        "Imports standard libraries. Do not read."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qt2jt2bHcY2v"
      },
      "outputs": [],
      "source": [
        "# https://nnsight.net/\n",
        "# !pip install -U nnsight -q\n",
        "!pip install nnsight==0.3.7 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZQv5mkuGjYX"
      },
      "outputs": [],
      "source": [
        "!pip install pandas plotly -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vif7qLNrlC0P"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import einops\n",
        "import torch\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "# pio.renderers.default = \"colab\"\n",
        "\n",
        "import nnsight\n",
        "from nnsight import LanguageModel, util"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orVn0wTnosHO"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "# from google.colab import userdata\n",
        "import gc\n",
        "import weakref"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKy4emg-xMp6"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o16WOSYmyxC2"
      },
      "outputs": [],
      "source": [
        "!pip install datasets -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMWEb8TJoske"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "github_token = userdata.get(\"GITHUB_TOKEN\")\n",
        "\n",
        "!pip install --upgrade git+https://{github_token}@github.com/withmartian/TinySQL.git -q\n",
        "\n",
        "import TinySQL as qts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbgIvuWuzMRu"
      },
      "outputs": [],
      "source": [
        "clean_tokens = []\n",
        "patching_results = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54KJeeB0Sjjc"
      },
      "outputs": [],
      "source": [
        "# Key global \"input\" variables\n",
        "clean_prompt = \"\"\n",
        "corrupt_prompt = \"\"\n",
        "clean_tokenizer_index = qts.UNKNOWN_VALUE # Tokenizer vocab index for clean word\n",
        "corrupt_tokenizer_index = qts.UNKNOWN_VALUE # Tokenizer vocab index for corrupted word\n",
        "answer_token_index = qts.UNKNOWN_VALUE # Token index in sql command answer of clean/corrupt word\n",
        "\n",
        "# Key global \"results\" variables\n",
        "clean_logit_diff = qts.UNKNOWN_VALUE\n",
        "corrupt_logit_diff = qts.UNKNOWN_VALUE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6k59y0OZhp7"
      },
      "source": [
        "# Select model, command set and feature to investigate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rl1akjeAUfoV"
      },
      "outputs": [],
      "source": [
        "model_num = 1                     # 0=GPT2, 1=TinyStories, 2=Qwen, 3=Llama, 4=Granite, 5=SmolLM\n",
        "cs_num = 1                        # 0=BaseModel, 1=CS1, 2=CS2 or 3=CS3\n",
        "feature_name = qts.ENGTABLENAME   # ENGTABLENAME, ENGFIELDNAME, DEFTABLESTART, DEFTABLENAME, DEFFIELDNAME, DEFFIELDSEPARATOR\n",
        "use_novel_names = False           # If True, we corrupt using words not found in the clean prompt or create sql e.g. \"little\" or \"hammer\"\n",
        "use_synonyms_table = False\n",
        "use_synonyms_field = False\n",
        "batch_size = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y72URtyynvnv"
      },
      "source": [
        "# Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFaFTy6LnbxI"
      },
      "outputs": [],
      "source": [
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "\n",
        "model = qts.load_tinysql_model(model_num, cs_num, auth_token=hf_token, synonym=True)\n",
        "model_hf = qts.sql_interp_model_location(model_num, cs_num)\n",
        "clear_output()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoZWM0rQxduM"
      },
      "outputs": [],
      "source": [
        "N_LAYERS, N_HEADS, D_MODEL, D_HEAD = qts.get_model_sizes(model_num, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YifzjHokxoeg"
      },
      "source": [
        "# Generate clean and corrupt data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nuwPxHY8ayA"
      },
      "outputs": [],
      "source": [
        "model.pad_token_id = model.tokenizer.pad_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76YZ8VErsHjP"
      },
      "outputs": [],
      "source": [
        "generator = qts.CorruptFeatureTestGenerator(model_num, cs_num, model.tokenizer, use_novel_names=use_novel_names, use_synonyms_field=use_synonyms_field, use_synonyms_table=use_synonyms_table )\n",
        "examples = generator.generate_feature_examples(feature_name, batch_size)\n",
        "\n",
        "# Each examples is corrupted at prompt_token_index. A resulting impact is expected at answer_token_index\n",
        "prompts = []\n",
        "ref_answers = []\n",
        "for example in examples:\n",
        "    clean_tokenizer_index = example.clean_tokenizer_index\n",
        "    corrupt_tokenizer_index = example.corrupt_tokenizer_index\n",
        "    answer_token_index = example.answer_token_index\n",
        "\n",
        "    # Truncate the clean_prompt at answer_token_index\n",
        "    clean_prompt = example.clean_BatchItem.get_alpaca_prompt() + example.clean_BatchItem.sql_statement\n",
        "    clean_tokens = model.tokenizer(clean_prompt)[\"input_ids\"]\n",
        "    clean_tokens = clean_tokens[:answer_token_index+1]\n",
        "    clean_prompt = model.tokenizer.decode(clean_tokens)\n",
        "\n",
        "    prompts.append(clean_prompt.split('Response: ')[0] + 'Response: ')\n",
        "    ref_answers.append(clean_prompt.split('Response: ')[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLZSoPA08ayA"
      },
      "outputs": [],
      "source": [
        "batch_size_mean = 200\n",
        "\n",
        "generator = qts.CorruptFeatureTestGenerator(model_num, cs_num, model.tokenizer, use_novel_names=use_novel_names, use_synonyms_field=use_synonyms_field, use_synonyms_table=use_synonyms_table )\n",
        "examples = generator.generate_feature_examples(feature_name, batch_size_mean)\n",
        "\n",
        "# Each examples is corrupted at prompt_token_index. A resulting impact is expected at answer_token_index\n",
        "prompts_mean = []\n",
        "ref_answers_mean = []\n",
        "for example in examples:\n",
        "    clean_tokenizer_index = example.clean_tokenizer_index\n",
        "    corrupt_tokenizer_index = example.corrupt_tokenizer_index\n",
        "    answer_token_index = example.answer_token_index\n",
        "\n",
        "    # Truncate the clean_prompt at answer_token_index\n",
        "    clean_prompt = example.clean_BatchItem.get_alpaca_prompt() + example.clean_BatchItem.sql_statement\n",
        "    clean_tokens = model.tokenizer(clean_prompt)[\"input_ids\"]\n",
        "    clean_tokens = clean_tokens[:answer_token_index+1]\n",
        "    clean_prompt = model.tokenizer.decode(clean_tokens)\n",
        "\n",
        "    prompts_mean.append(clean_prompt.split('Response: ')[0] + 'Response: ')\n",
        "    ref_answers_mean.append(clean_prompt.split('Response: ')[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsmCqeNziwOv"
      },
      "source": [
        "# Selective ablations whole model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N763EOoI8ayB"
      },
      "outputs": [],
      "source": [
        "def calculate_similarity(text1, text2):\n",
        "   def extract_sql_parts(text):\n",
        "       select_part = text[text.find(\"SELECT\") + 7:text.find(\"FROM\")].strip()\n",
        "       from_part = text[text.find(\"FROM\") + 5:].strip()\n",
        "       columns = [c.strip() for c in select_part.split(',')]\n",
        "       return columns, from_part\n",
        "   cols1, from1 = extract_sql_parts(text1)\n",
        "   cols2, from2 = extract_sql_parts(text2)\n",
        "\n",
        "   score = 0\n",
        "   if \"SELECT\" in text1 and \"SELECT\" in text2: score += 0.2\n",
        "   if \"FROM\" in text1 and \"FROM\" in text2: score += 0.2\n",
        "   if from1 == from2: score += 0.2\n",
        "   if cols1[0] == cols2[0]: score += 0.2\n",
        "   if len(cols1) >= 2 and len(cols2) >= 2:\n",
        "       if cols1[1] == cols2[1]: score += 0.2\n",
        "   return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6XoU_cCm7cG"
      },
      "outputs": [],
      "source": [
        "def compute_head_means(model, prompt_text):\n",
        "    layer_means = []\n",
        "\n",
        "    with model.generate(prompt_text, max_new_tokens=7) as tracer:\n",
        "        for layer_idx in range(len(model.transformer.h)):\n",
        "            layer_output = model.transformer.h[layer_idx].attn.output[0]\n",
        "\n",
        "            output_reshaped = einops.rearrange(\n",
        "                layer_output,\n",
        "                'b s (nh dh) -> b s nh dh',\n",
        "                nh=N_HEADS\n",
        "            )\n",
        "\n",
        "            # Calculate mean across batch dimension\n",
        "            head_means = output_reshaped.mean(dim=0)  # Shape: [s, nh, dh]\n",
        "            layer_means.append(head_means.save())\n",
        "\n",
        "    return layer_means\n",
        "\n",
        "def mean_heads_ablation(model, prompt_text, target_layers, heads_per_layer, mean=True):\n",
        "    N_HEADS = 16\n",
        "\n",
        "    # First compute the means for all layers\n",
        "    layer_means = compute_head_means(model, prompt_text)\n",
        "\n",
        "    with model.generate(prompt_text, max_new_tokens=7) as tracer:\n",
        "        hidden_states = []\n",
        "        for layer_idx in target_layers:\n",
        "            # Get initial hidden states\n",
        "            layer_output = model.transformer.h[layer_idx].attn.output[0]\n",
        "            target_heads = heads_per_layer[layer_idx]\n",
        "\n",
        "            # Reshape for head manipulation\n",
        "            output_reshaped = einops.rearrange(\n",
        "                layer_output,\n",
        "                'b s (nh dh) -> b s nh dh',\n",
        "                nh=N_HEADS\n",
        "            )\n",
        "\n",
        "            head_means = layer_means[layer_idx]  # Shape: [s, nh, dh]\n",
        "\n",
        "            # Replace non-target heads with their means\n",
        "            for head_idx in range(N_HEADS):\n",
        "                if head_idx not in target_heads:\n",
        "                    # Replace with saved means while preserving sequence position\n",
        "                    if mean:\n",
        "                        output_reshaped[:, :, head_idx, :] = head_means[:, head_idx, :].unsqueeze(0)\n",
        "                    else:\n",
        "                        output_reshaped[:, :, head_idx, :] = torch.zeros_like(head_means[:, head_idx, :].unsqueeze(0))\n",
        "\n",
        "            # Reshape back and modify layer output\n",
        "            modified_output = einops.rearrange(\n",
        "                output_reshaped,\n",
        "                'b s nh dh -> b s (nh dh)',\n",
        "                nh=N_HEADS\n",
        "            )\n",
        "            model.transformer.h[layer_idx].attn.output = (modified_output,) + model.transformer.h[layer_idx].attn.output[1:]\n",
        "\n",
        "            # Save hidden states for each step\n",
        "            hidden_states.append(model.transformer.h[layer_idx].output[0].save())\n",
        "\n",
        "\n",
        "        out = model.generator.output.save()\n",
        "\n",
        "    return hidden_states, out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJ_1kID48ayB"
      },
      "outputs": [],
      "source": [
        "heads_per_layer = {\n",
        "   0: [11, 3, 1, 8, 15, 14, 13, 7],\n",
        "   1: [10, 13, 3, 7, 14, 15, 11, 2, 1, 12, 5]\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "7E1836rW8ayC"
      },
      "outputs": [],
      "source": [
        "target_layers = [0, 1]\n",
        "# heads_per_layer = {\n",
        "#    0: [1, 2, 3, 7, 8, 11, 13, 14, 15],\n",
        "#    1: [13, 3, 7, 14, 15, 11]\n",
        "# }\n",
        "\n",
        "heads_per_layer = {\n",
        "   0: list(range(16)),\n",
        "   1: list(range(16))\n",
        "}\n",
        "\n",
        "results = []\n",
        "for i, prompt in enumerate(prompts):\n",
        "   hidden_states, output = mean_heads_ablation(model, prompt, target_layers, heads_per_layer, mean=True)\n",
        "   gen_text = model.tokenizer.decode(output[0]).split('Response: ')[1]\n",
        "   similarity = calculate_similarity(gen_text, ref_answers[i])\n",
        "   results.append({\n",
        "       'output': gen_text,\n",
        "       'similarity': similarity\n",
        "   })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gS0ErZXe8ayC"
      },
      "outputs": [],
      "source": [
        "total_similarity = sum(r['similarity'] for r in results)\n",
        "avg_similarity = total_similarity / len(results)\n",
        "print(f\"Average similarity: {avg_similarity}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "mqpT_vkT8ayD"
      },
      "outputs": [],
      "source": [
        "base_heads_per_layer = {\n",
        "    0: [1, 2, 3, 7, 8, 11, 13, 14, 15],\n",
        "    1: [13, 3, 7, 14, 15, 11]\n",
        "}\n",
        "\n",
        "target_layers = [0, 1]\n",
        "\n",
        "layer_0_mean_results = []\n",
        "layer_0_zero_results = []\n",
        "layer_1_mean_results = []\n",
        "layer_1_zero_results = []\n",
        "\n",
        "# Test Layer 0 (keeping Layer 1 intact)\n",
        "print(\"\\nTesting Layer 0:\")\n",
        "heads_0 = base_heads_per_layer[0]\n",
        "for num_heads in range(1, len(heads_0) + 1):\n",
        "    current_heads_0 = heads_0[:num_heads]\n",
        "    print(f\"\\nTesting with Layer 0 heads: {current_heads_0}\")\n",
        "\n",
        "    # Keep all Layer 1 heads active while testing Layer 0\n",
        "    current_heads_dict = {\n",
        "        0: current_heads_0,\n",
        "        1: base_heads_per_layer[1]  # All heads in layer 1\n",
        "    }\n",
        "\n",
        "    # Test with mean ablation\n",
        "    results = []\n",
        "    for i, prompt in enumerate(prompts):\n",
        "        hidden_states, output = mean_heads_ablation(model, prompt, target_layers, current_heads_dict, mean=True)\n",
        "        gen_text = model.tokenizer.decode(output[0]).split('Response: ')[1]\n",
        "        similarity = calculate_similarity(gen_text, ref_answers[i])\n",
        "        results.append({\n",
        "            'output': gen_text,\n",
        "            'similarity': similarity\n",
        "        })\n",
        "\n",
        "    avg_similarity = sum(r['similarity'] for r in results) / len(results)\n",
        "    layer_0_mean_results.append({\n",
        "        'num_heads': num_heads,\n",
        "        'heads': current_heads_0,\n",
        "        'avg_similarity': avg_similarity,\n",
        "        'detailed_results': results\n",
        "    })\n",
        "\n",
        "    # Test with zero ablation\n",
        "    results = []\n",
        "    for i, prompt in enumerate(prompts):\n",
        "        hidden_states, output = mean_heads_ablation(model, prompt, target_layers, current_heads_dict, mean=False)\n",
        "        gen_text = model.tokenizer.decode(output[0]).split('Response: ')[1]\n",
        "        similarity = calculate_similarity(gen_text, ref_answers[i])\n",
        "        results.append({\n",
        "            'output': gen_text,\n",
        "            'similarity': similarity\n",
        "        })\n",
        "\n",
        "    avg_similarity = sum(r['similarity'] for r in results) / len(results)\n",
        "    layer_0_zero_results.append({\n",
        "        'num_heads': num_heads,\n",
        "        'heads': current_heads_0,\n",
        "        'avg_similarity': avg_similarity,\n",
        "        'detailed_results': results\n",
        "    })\n",
        "\n",
        "# Test Layer 1 (keeping Layer 0 intact)\n",
        "print(\"\\nTesting Layer 1:\")\n",
        "heads_1 = base_heads_per_layer[1]\n",
        "for num_heads in range(1, len(heads_1) + 1):\n",
        "    current_heads_1 = heads_1[:num_heads]\n",
        "    print(f\"\\nTesting with Layer 1 heads: {current_heads_1}\")\n",
        "\n",
        "    # Keep all Layer 0 heads active while testing Layer 1\n",
        "    current_heads_dict = {\n",
        "        0: base_heads_per_layer[0],  # All heads in layer 0\n",
        "        1: current_heads_1\n",
        "    }\n",
        "\n",
        "    # Test with mean ablation\n",
        "    results = []\n",
        "    for i, prompt in enumerate(prompts):\n",
        "        hidden_states, output = mean_heads_ablation(model, prompt, target_layers, current_heads_dict, mean=True)\n",
        "        gen_text = model.tokenizer.decode(output[0]).split('Response: ')[1]\n",
        "        similarity = calculate_similarity(gen_text, ref_answers[i])\n",
        "        results.append({\n",
        "            'output': gen_text,\n",
        "            'similarity': similarity\n",
        "        })\n",
        "\n",
        "    avg_similarity = sum(r['similarity'] for r in results) / len(results)\n",
        "    layer_1_mean_results.append({\n",
        "        'num_heads': num_heads,\n",
        "        'heads': current_heads_1,\n",
        "        'avg_similarity': avg_similarity,\n",
        "        'detailed_results': results\n",
        "    })\n",
        "\n",
        "    # Test with zero ablation\n",
        "    results = []\n",
        "    for i, prompt in enumerate(prompts):\n",
        "        hidden_states, output = mean_heads_ablation(model, prompt, target_layers, current_heads_dict, mean=False)\n",
        "        gen_text = model.tokenizer.decode(output[0]).split('Response: ')[1]\n",
        "        similarity = calculate_similarity(gen_text, ref_answers[i])\n",
        "        results.append({\n",
        "            'output': gen_text,\n",
        "            'similarity': similarity\n",
        "        })\n",
        "\n",
        "    avg_similarity = sum(r['similarity'] for r in results) / len(results)\n",
        "    layer_1_zero_results.append({\n",
        "        'num_heads': num_heads,\n",
        "        'heads': current_heads_1,\n",
        "        'avg_similarity': avg_similarity,\n",
        "        'detailed_results': results\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJQjxnYe8ayE"
      },
      "outputs": [],
      "source": [
        "!pip install seaborn -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avZbRAOA8ayE"
      },
      "outputs": [],
      "source": [
        "import seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "dd_z0r478ayF"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "sns.set_context(\"paper\")\n",
        "\n",
        "colors = sns.color_palette(\"deep\")\n",
        "mean_color = colors[2]  # Deep teal\n",
        "zero_color = colors[3]  # Deep purple\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot for Layer 0\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot([r['num_heads'] for r in layer_0_mean_results],\n",
        "         [r['avg_similarity'] for r in layer_0_mean_results],\n",
        "         color=mean_color, marker='o', linestyle='-',\n",
        "         label='Mean Ablation', linewidth=2.5, markersize=8)\n",
        "plt.plot([r['num_heads'] for r in layer_0_zero_results],\n",
        "         [r['avg_similarity'] for r in layer_0_zero_results],\n",
        "         color=zero_color, marker='s', linestyle='-',\n",
        "         label='Zero Ablation', linewidth=2.5, markersize=8)\n",
        "plt.axhline(y=0.7839, linestyle='--', linewidth=1.5, label='Model Without Ablation')  # Horizontal line in red\n",
        "\n",
        "plt.xlabel('Number of Active Heads in Layer 0', fontsize=11)\n",
        "plt.ylabel('Average Similarity', fontsize=11)\n",
        "plt.title('Layer 0 Head Ablation\\n(Layer 1 Heads Active)', fontsize=12, pad=10)\n",
        "plt.legend(frameon=True, fontsize=10, loc='center right')  # Changed legend location\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot for Layer 1\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot([r['num_heads'] for r in layer_1_mean_results],\n",
        "         [r['avg_similarity'] for r in layer_1_mean_results],\n",
        "         color=mean_color, marker='o', linestyle='-',\n",
        "         label='Mean Ablation', linewidth=2.5, markersize=8)\n",
        "plt.plot([r['num_heads'] for r in layer_1_zero_results],\n",
        "         [r['avg_similarity'] for r in layer_1_zero_results],\n",
        "         color=zero_color, marker='s', linestyle='-',\n",
        "         label='Zero Ablation', linewidth=2.5, markersize=8)\n",
        "plt.axhline(y=0.7839, linestyle='--', linewidth=1.5, label='Model Without Ablation')  # Horizontal line in blue\n",
        "\n",
        "plt.xlabel('Number of Active Heads in Layer 1', fontsize=11)\n",
        "plt.ylabel('Average Similarity', fontsize=11)\n",
        "plt.title('Layer 1 Head Ablation\\n(Layer 0 Heads Active)', fontsize=12, pad=10)\n",
        "plt.legend(frameon=True, fontsize=10, loc='center right')  # Changed legend location\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the figure before showing it\n",
        "fig = plt.gcf()\n",
        "fig.savefig('ablation_results.pdf', format='pdf', dpi=300, bbox_inches='tight')\n",
        "fig.savefig('ablation_results.png', dpi=300, bbox_inches='tight')\n",
        "\n",
        "# Now show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwZiVsxf8ayF"
      },
      "outputs": [],
      "source": [
        "fig.savefig('ablation_results.png', dpi=300, bbox_inches='tight')  # PNG for web/screen\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grPB79Gj8ayF"
      },
      "source": [
        "# global ablation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvgC2s9w8ayG"
      },
      "outputs": [],
      "source": [
        "def compute_layer_means_all(model, prompt_texts):\n",
        "    \"\"\"Compute means for both attention and MLP outputs across samples.\"\"\"\n",
        "    attn_means = []\n",
        "    mlp_means = []\n",
        "    with model.generate(prompt_texts, max_new_tokens=7) as tracer:\n",
        "        for layer_idx in range(N_LAYERS):\n",
        "            mlp_output = model.transformer.h[layer_idx].mlp.output\n",
        "            mlp_mean = mlp_output.mean(dim=0)  # Mean across batch dim\n",
        "            mlp_means.append(mlp_mean.save())\n",
        "\n",
        "    return attn_means, mlp_means\n",
        "\n",
        "def mean_ablation_all(model, prompt_text, target_layers, layer_means):\n",
        "    \"\"\"Apply mean ablation only to layers NOT in target_layers.\"\"\"\n",
        "    attn_means, mlp_means = layer_means\n",
        "\n",
        "    with model.generate(prompt_text, max_new_tokens=7) as tracer:\n",
        "        hidden_states = []\n",
        "        for layer_idx in range(N_LAYERS):\n",
        "            original_mlp = model.transformer.h[layer_idx].mlp.output\n",
        "\n",
        "            if layer_idx not in target_layers:\n",
        "                mlp_mean = mlp_means[layer_idx]\n",
        "\n",
        "                modified_mlp = mlp_mean.unsqueeze(0).expand_as(original_mlp)\n",
        "\n",
        "                model.transformer.h[layer_idx].mlp.output = modified_mlp\n",
        "\n",
        "            hidden_states.append(model.transformer.h[layer_idx].output[0].save())\n",
        "\n",
        "        out = model.generator.output.save()\n",
        "\n",
        "    return hidden_states, out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-e4BY0t8ayH"
      },
      "outputs": [],
      "source": [
        "def zero_ablation_all(model, prompt_text, target_layers):\n",
        "    \"\"\"Apply zero ablation only to layers NOT in target_layers.\"\"\"\n",
        "    with model.generate(prompt_text, max_new_tokens=7) as tracer:\n",
        "        hidden_states = []\n",
        "        for layer_idx in range(N_LAYERS):\n",
        "            original_mlp = model.transformer.h[layer_idx].mlp.output\n",
        "\n",
        "            if layer_idx not in target_layers:\n",
        "                zero_mlp = torch.zeros_like(original_mlp)\n",
        "\n",
        "                model.transformer.h[layer_idx].mlp.output = zero_mlp\n",
        "\n",
        "            hidden_states.append(model.transformer.h[layer_idx].output[0].save())\n",
        "\n",
        "        out = model.generator.output.save()\n",
        "\n",
        "    return hidden_states, out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "GA_OefgV8ayH"
      },
      "outputs": [],
      "source": [
        "target_layers = [1]\n",
        "\n",
        "\n",
        "results = []\n",
        "for i, prompt in enumerate(prompts):\n",
        "   hidden_states, output = zero_ablation_all(model, prompt, target_layers)\n",
        "   gen_text = model.tokenizer.decode(output[0]).split('Response: ')[1]\n",
        "   similarity = calculate_similarity(gen_text, ref_answers[i])\n",
        "   results.append({\n",
        "       'output': gen_text,\n",
        "       'similarity': similarity\n",
        "   })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evBngoFd8ayI"
      },
      "outputs": [],
      "source": [
        "total_similarity = sum(r['similarity'] for r in results)\n",
        "avg_similarity = total_similarity / len(results)\n",
        "print(f\"Average similarity: {avg_similarity}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "0QRr4pg_8ayI"
      },
      "outputs": [],
      "source": [
        "layers_all = compute_layer_means_all(model, prompts_mean)\n",
        "\n",
        "target_layers = [0]\n",
        "\n",
        "\n",
        "results = []\n",
        "for i, prompt in enumerate(prompts):\n",
        "   hidden_states, output = mean_ablation_all(model, prompt, target_layers, layers_all)\n",
        "   gen_text = model.tokenizer.decode(output[0]).split('Response: ')[1]\n",
        "   similarity = calculate_similarity(gen_text, ref_answers[i])\n",
        "   results.append({\n",
        "       'output': gen_text,\n",
        "       'similarity': similarity\n",
        "   })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CulSkxv28ayJ"
      },
      "outputs": [],
      "source": [
        "total_similarity = sum(r['similarity'] for r in results)\n",
        "avg_similarity = total_similarity / len(results)\n",
        "print(f\"Average similarity: {avg_similarity}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "o-IPoeVs8ayJ"
      },
      "source": [
        "# Difference vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "_yQCMJ_68ayT"
      },
      "outputs": [],
      "source": [
        "def compute_batch_means(model, prompts):\n",
        "    \"\"\"First compute means across multiple prompts.\"\"\"\n",
        "    N_HEADS = 16\n",
        "    attn_means = []\n",
        "    mlp_means = []\n",
        "\n",
        "    with model.generate(prompts, max_new_tokens=7) as tracer:\n",
        "        for layer_idx in range(len(model.transformer.h)):\n",
        "            # Get attention and MLP outputs\n",
        "            attn_output = model.transformer.h[layer_idx].attn.output[0]\n",
        "            mlp_output = model.transformer.h[layer_idx].mlp.output\n",
        "\n",
        "            # For attention, reshape to expose heads\n",
        "            attn_reshaped = einops.rearrange(\n",
        "                attn_output,\n",
        "                'b s (nh dh) -> b s nh dh',\n",
        "                nh=N_HEADS\n",
        "            )\n",
        "\n",
        "            # Compute means across batch dimension\n",
        "            attn_mean = attn_reshaped.mean(dim=0)  # Shape: [s, nh, dh]\n",
        "            mlp_mean = mlp_output.mean(dim=0)      # Shape: [s, hidden_dim]\n",
        "\n",
        "            attn_means.append(attn_mean.save())\n",
        "            mlp_means.append(mlp_mean.save())\n",
        "\n",
        "    return attn_means, mlp_means\n",
        "\n",
        "def compute_residual_difference(model, prompt_text, attn_means, mlp_means):\n",
        "    \"\"\"Run mean ablation and compute difference between final and initial vectors.\"\"\"\n",
        "    N_HEADS = 16\n",
        "\n",
        "    with model.generate(prompt_text, max_new_tokens=7) as tracer:\n",
        "        # Save initial embeddings\n",
        "        initial_embeddings = model.transformer.wte.output\n",
        "\n",
        "        # Run through transformer with pre-computed means\n",
        "        for layer_idx in range(len(model.transformer.h)):\n",
        "            # Replace attention outputs with means\n",
        "            original_attn = model.transformer.h[layer_idx].attn.output\n",
        "            attn_mean = attn_means[layer_idx]\n",
        "            attn_expanded = attn_mean.unsqueeze(0)  # Add batch dim\n",
        "            modified_attn = einops.rearrange(\n",
        "                attn_expanded,\n",
        "                'b s nh dh -> b s (nh dh)',\n",
        "                nh=N_HEADS\n",
        "            )\n",
        "            model.transformer.h[layer_idx].attn.output = (modified_attn,) + original_attn[1:]\n",
        "\n",
        "            # Replace MLP outputs with means\n",
        "            mlp_mean = mlp_means[layer_idx]\n",
        "            mlp_expanded = mlp_mean.unsqueeze(0)  # Add batch dim\n",
        "            model.transformer.h[layer_idx].mlp.output = mlp_expanded\n",
        "\n",
        "        # Get final vectors after last MLP\n",
        "        final_vectors = model.transformer.h[-1].mlp.output\n",
        "\n",
        "        # Compute and save difference vectors\n",
        "        difference_vectors = (final_vectors - initial_embeddings).save()\n",
        "\n",
        "    return difference_vectors\n",
        "\n",
        "def apply_difference_vectors(model, prompt_text, difference_vectors):\n",
        "    \"\"\"Apply saved difference vectors directly to embeddings.\"\"\"\n",
        "    with model.generate(prompt_text, max_new_tokens=7) as tracer:\n",
        "        # Get initial embeddings\n",
        "        embeddings = model.transformer.wte.output\n",
        "\n",
        "        # Add difference vectors\n",
        "        modified_embeddings = embeddings + difference_vectors.expand_as(embeddings)\n",
        "\n",
        "        # Apply final layer norm\n",
        "        normalized = model.transformer.ln_f(modified_embeddings)\n",
        "\n",
        "        # Now pass to language model head\n",
        "        logits = model.lm_head(normalized)\n",
        "\n",
        "        out = logits.save()\n",
        "\n",
        "    return out\n",
        "\n",
        "# First compute means across batch of prompts\n",
        "print(\"Computing batch means...\")\n",
        "attn_means, mlp_means = compute_batch_means(model, prompts)\n",
        "\n",
        "# Then compute difference vectors using mean ablation\n",
        "print(\"Computing residual difference...\")\n",
        "difference_vectors = compute_residual_difference(model, prompts[0], attn_means, mlp_means)\n",
        "\n",
        "# Finally, apply to new prompts\n",
        "print(\"Testing on prompts...\")\n",
        "results = []\n",
        "for i, prompt in enumerate(prompts):\n",
        "    output = apply_difference_vectors(model, prompt, difference_vectors)\n",
        "    gen_text = model.tokenizer.decode(torch.argmax(output[0], dim=-1))\n",
        "    similarity = calculate_similarity(gen_text, ref_answers[i])\n",
        "    results.append({\n",
        "        'output': gen_text,\n",
        "        'similarity': similarity\n",
        "    })\n",
        "\n",
        "# Print results\n",
        "for i, result in enumerate(results[:3]):\n",
        "    print(f\"\\nPrompt {i}:\")\n",
        "    print(f\"Generated: {result['output']}\")\n",
        "    print(f\"Similarity: {result['similarity']}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "z1tQZgLUnojc",
        "YifzjHokxoeg"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}