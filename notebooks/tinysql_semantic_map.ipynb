{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQAsYjSQVXHC"
   },
   "source": [
    "# Activation Patching on GPT2, M1, M2 and M3 models using nnsight\n",
    "- Developed on Google Colab using an A100 with 40GB GPU and 80GB system RAM.\n",
    "- Runs with GPT2/TinyStories/Qwen/Llama with base/CS1/CS2/CS3.  \n",
    "- Requires a GITHUB_TOKEN secret to access Martian quanta_text_to_sql code repository.\n",
    "- Requires a HF_TOKEN secret to access Martian HuggingFace repository.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6k59y0OZhp7"
   },
   "source": [
    "# Select model and command set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rl1akjeAUfoV"
   },
   "outputs": [],
   "source": [
    "model_num = 3   # 0=GPT2, 1=TinyStories, 2=Qwen or 3=Llama\n",
    "cs_num = 1      # 0=BaseModel, 1=CS1, 2=CS2 or 3=CS3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1tQZgLUnojc"
   },
   "source": [
    "# Import libraries\n",
    "Imports standard libraries. Do not read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qt2jt2bHcY2v",
    "outputId": "be4cc720-6401-426f-b328-fdd4bebe349b"
   },
   "outputs": [],
   "source": [
    "# https://nnsight.net/\n",
    "!pip install -U nnsight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Vif7qLNrlC0P"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/clementneo/projects/quanta_text_to_sql/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning:\n",
      "\n",
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import einops\n",
    "import torch\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"colab\"\n",
    "\n",
    "import nnsight\n",
    "from nnsight import LanguageModel, util\n",
    "from nnsight.tracing.Proxy import Proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BMWEb8TJoske"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from getpass import getpass\n",
    "    from google.colab import userdata\n",
    "    import gc\n",
    "    import weakref\n",
    "    github_token = userdata.get(\"GITHUB_TOKEN\")\n",
    "except:\n",
    "    import os\n",
    "    github_token = os.getenv(\"GITHUB_TOKEN\")\n",
    "\n",
    "# # Install the private repository using the token\n",
    "!pip install --upgrade git+https://{github_token}@github.com/withmartian/quanta_text_to_sql.git\n",
    "\n",
    "import TinySQL as qts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y72URtyynvnv"
   },
   "source": [
    "# Run on m1, m2 and m3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oFaFTy6LnbxI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048, padding_idx=128004)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      "  (generator): WrapperModule()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if model_num > 0:\n",
    "\n",
    "    if model_num == 1:\n",
    "        the_tokenizer, the_model = qts.load_sql_interp_model(model_num, cs_num, auth_token=userdata.get(\"HF_TOKEN\"))\n",
    "\n",
    "        model = LanguageModel(the_model, the_tokenizer)\n",
    "        model.tokenizer = the_tokenizer\n",
    "    else:\n",
    "        if torch.backends.mps.is_available():\n",
    "            model = LanguageModel(qts.sql_interp_model_location(model_num, cs_num), device_map=\"mps\")\n",
    "        else:\n",
    "            model = LanguageModel(qts.sql_interp_model_location(model_num, cs_num), device_map=\"auto\")\n",
    "\n",
    "    clear_output()\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run prompts to see which one the model gets right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 3\n",
      "Prompt 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 5\n",
      "Prompt 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 7\n",
      "Prompt 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 9\n",
      "Prompt 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 11\n",
      "0.5454545454545454\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "(\"### Instruction: How much do employees earn? ### Context: CREATE TABLE employees (name TEXT, salary INTEGER, department TEXT) ### Response: SELECT\", 'salary'),\n",
    "(\"### Instruction: How much does each product cost? ### Context: CREATE TABLE products (name TEXT, price INTEGER, category TEXT) ### Response: SELECT\", 'price'),\n",
    "(\"### Instruction: How tall are all the buildings? ### Context: CREATE TABLE buildings (address TEXT, height INTEGER, year_built INTEGER) ### Response: SELECT\", 'height'),\n",
    "(\"### Instruction: How many inhabitants live in each city? ### Context: CREATE TABLE cities (name TEXT, population INTEGER, country TEXT) ### Response: SELECT\", 'population'),\n",
    "(\"### Instruction: How heavy are the packages? ### Context: CREATE TABLE shipments (tracking_id TEXT, weight DECIMAL, destination TEXT) ### Response: SELECT\", 'weight'),\n",
    "(\"### Instruction: When was each book released? ### Context: CREATE TABLE books (title TEXT, publication_date DATE, author TEXT) ### Response: SELECT\", 'publication_date'),\n",
    "(\"### Instruction: How far is each destination? ### Context: CREATE TABLE locations (place TEXT, distance INTEGER, transport_mode TEXT) ### Response: SELECT\", 'distance'),\n",
    "(\"### Instruction: What's the temperature in each room? ### Context: CREATE TABLE sensors (room_id TEXT, temp_celsius DECIMAL, humidity INTEGER) ### Response: SELECT\", 'temp_celsius'),\n",
    "(\"### Instruction: How deep are the wells? ### Context: CREATE TABLE wells (location TEXT, depth INTEGER, status TEXT) ### Response: SELECT\", 'depth'),\n",
    "(\"### Instruction: How long are the movies? ### Context: CREATE TABLE films (title TEXT, duration INTEGER, genre TEXT) ### Response: SELECT\", 'duration'),\n",
    "(\"### Instruction: How fast can each vehicle go? ### Context: CREATE TABLE vehicles (model TEXT, speed INTEGER, manufacturer TEXT) ### Response: SELECT\", 'speed'),\n",
    "]\n",
    "\n",
    "results = []\n",
    "prob_layers_all = {}\n",
    "for i, (prompt, gt) in enumerate(prompts):\n",
    "    print(f\"Prompt {i+1}\")\n",
    "    with model.generate(prompt, max_new_tokens=1, temperature=0.0001) as tracer:\n",
    "        out = model.generator.output.save()\n",
    "\n",
    "    output_text = model.tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    answer = output_text.split(\"SELECT\")[-1].strip()\n",
    "\n",
    "    correct = answer in gt\n",
    "    results.append(correct)\n",
    "\n",
    "    if correct:\n",
    "        layers = model.model.layers\n",
    "        # What I want to do next is see how the probabilities change as we move through the layers\n",
    "        prob_layers = []\n",
    "        final_ln = model.model.norm\n",
    "        lm_head = model.lm_head\n",
    "        with model.trace() as tracer:\n",
    "            with tracer.invoke(prompt) as invoker:\n",
    "                for layer_idx, layer in enumerate(layers):\n",
    "                    # Process layer output through the model's head and layer normalization\n",
    "                    layer_output = lm_head(final_ln(layer.output[0]))\n",
    "                    # Apply softmax to obtain probabilities and save the result\n",
    "                    probs = torch.nn.functional.softmax(layer_output, dim=-1)\n",
    "\n",
    "                    answer = \" \" + gt\n",
    "                    answer_idx = model.tokenizer(answer)[\"input_ids\"][1]\n",
    "                    # Get the probability of the correct answer\n",
    "                    correct_prob = probs[0, -1, answer_idx].save()\n",
    "                    prob_layers.append(correct_prob)\n",
    "                    \n",
    "        prob_layers_all[answer] = prob_layers\n",
    "        \n",
    "\n",
    "print(sum(results)/len(results)) # About 50-60% accuracy in this unscientific test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salary ### Instruction: How much do employees earn? ### Context: CREATE TABLE employees (name TEXT, salary INTEGER, department TEXT) ### Response: SELECT\n",
      "height ### Instruction: How tall are all the buildings? ### Context: CREATE TABLE buildings (address TEXT, height INTEGER, year_built INTEGER) ### Response: SELECT\n",
      "weight ### Instruction: How heavy are the packages? ### Context: CREATE TABLE shipments (tracking_id TEXT, weight DECIMAL, destination TEXT) ### Response: SELECT\n",
      "distance ### Instruction: How far is each destination? ### Context: CREATE TABLE locations (place TEXT, distance INTEGER, transport_mode TEXT) ### Response: SELECT\n",
      "depth ### Instruction: How deep are the wells? ### Context: CREATE TABLE wells (location TEXT, depth INTEGER, status TEXT) ### Response: SELECT\n",
      "speed ### Instruction: How fast can each vehicle go? ### Context: CREATE TABLE vehicles (model TEXT, speed INTEGER, manufacturer TEXT) ### Response: SELECT\n"
     ]
    }
   ],
   "source": [
    "correct_prompts = {}\n",
    "for i, (prompt, gt) in enumerate(prompts):\n",
    "    if results[i]:\n",
    "        correct_prompts[gt] = prompt\n",
    "\n",
    "for gt, prompt in correct_prompts.items():\n",
    "    print(gt, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See how probability of correct propmts evolve\n",
    "They become higher in the last 6 layers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.01', '0.13', '0.17', '0.85']  salary\n",
      "['0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.05', '0.03', '0.05', '0.72', '1.00']  height\n",
      "['0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.43', '0.94']  weight\n",
      "['0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.01', '0.54', '0.65', '0.54', '0.89', '0.92']  distance\n",
      "['0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.01', '0.23', '0.92', '0.99']  depth\n",
      "['0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.02', '0.93', '1.00']  speed\n"
     ]
    }
   ],
   "source": [
    "# set plotly to notebook\n",
    "pio.renderers.default = \"vscode\"\n",
    "\n",
    "# plot all on the same graph\n",
    "for answer, probs in prob_layers_all.items():\n",
    "    probs = [p.item() for p in probs]\n",
    "    # Print probs but to 2 decimal places)\n",
    "    print([f\"{p:.2f}\" for p in probs], answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run attention blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 0/30\n",
      "Probs for 0 — 0.0002192\n",
      "Token 1/30\n",
      "Probs for 1 — 0.8062636\n",
      "Token 2/30\n",
      "Probs for 2 — 0.8612688\n",
      "Token 3/30\n",
      "Probs for 3 — 0.8209375\n",
      "Token 4/30\n",
      "Probs for 4 — 0.8525844\n",
      "Token 5/30\n",
      "Probs for 5 — 0.9148036\n",
      "Token 6/30\n",
      "Probs for 6 — 0.8349769\n",
      "Token 7/30\n",
      "Probs for 7 — 0.8367761\n",
      "Token 8/30\n",
      "Probs for 8 — 0.9422179\n",
      "Token 9/30\n",
      "Probs for 9 — 0.8918879\n",
      "Token 10/30\n",
      "Probs for 10 — 0.8411002\n",
      "Token 11/30\n",
      "Probs for 11 — 0.8456310\n",
      "Token 12/30\n",
      "Probs for 12 — 0.8457880\n",
      "Token 13/30\n",
      "Probs for 13 — 0.8466424\n",
      "Token 14/30\n",
      "Probs for 14 — 0.8489893\n",
      "Token 15/30\n",
      "Probs for 15 — 0.8427060\n",
      "Token 16/30\n",
      "Probs for 16 — 0.8607543\n",
      "Token 17/30\n",
      "Probs for 17 — 0.8523888\n",
      "Token 18/30\n",
      "Probs for 18 — 0.8465830\n",
      "Token 19/30\n",
      "Probs for 19 — 0.8566743\n",
      "Token 20/30\n",
      "Probs for 20 — 0.1155154\n",
      "Token 21/30\n",
      "Probs for 21 — 0.8270299\n",
      "Token 22/30\n",
      "Probs for 22 — 0.8535358\n",
      "Token 23/30\n",
      "Probs for 23 — 0.8406035\n",
      "Token 24/30\n",
      "Probs for 24 — 0.8458364\n",
      "Token 25/30\n",
      "Probs for 25 — 0.8364301\n",
      "Token 26/30\n",
      "Probs for 26 — 0.8470449\n",
      "Token 27/30\n",
      "Probs for 27 — 0.8576281\n",
      "Token 28/30\n",
      "Probs for 28 — 0.8384448\n",
      "Token 29/30\n",
      "Probs for 29 — 0.5828055\n",
      "Token 0/33\n",
      "Probs for 0 — 0.0019598\n",
      "Token 1/33\n",
      "Probs for 1 — 0.9979370\n",
      "Token 2/33\n",
      "Probs for 2 — 0.9986579\n",
      "Token 3/33\n",
      "Probs for 3 — 0.9986099\n",
      "Token 4/33\n",
      "Probs for 4 — 0.9986425\n",
      "Token 5/33\n",
      "Probs for 5 — 0.9801474\n",
      "Token 6/33\n",
      "Probs for 6 — 0.9985996\n",
      "Token 7/33\n",
      "Probs for 7 — 0.9986640\n",
      "Token 8/33\n",
      "Probs for 8 — 0.9987056\n",
      "Token 9/33\n",
      "Probs for 9 — 0.9989448\n",
      "Token 10/33\n",
      "Probs for 10 — 0.9986858\n",
      "Token 11/33\n",
      "Probs for 11 — 0.9986285\n",
      "Token 12/33\n",
      "Probs for 12 — 0.9986379\n",
      "Token 13/33\n",
      "Probs for 13 — 0.9986940\n",
      "Token 14/33\n",
      "Probs for 14 — 0.9987282\n",
      "Token 15/33\n",
      "Probs for 15 — 0.9987527\n",
      "Token 16/33\n",
      "Probs for 16 — 0.9986339\n",
      "Token 17/33\n",
      "Probs for 17 — 0.9988376\n",
      "Token 18/33\n",
      "Probs for 18 — 0.9986596\n",
      "Token 19/33\n",
      "Probs for 19 — 0.9987053\n",
      "Token 20/33\n",
      "Probs for 20 — 0.9988117\n",
      "Token 21/33\n",
      "Probs for 21 — 0.9082785\n",
      "Token 22/33\n",
      "Probs for 22 — 0.9986691\n",
      "Token 23/33\n",
      "Probs for 23 — 0.9987801\n",
      "Token 24/33\n",
      "Probs for 24 — 0.9988444\n",
      "Token 25/33\n",
      "Probs for 25 — 0.9986851\n",
      "Token 26/33\n",
      "Probs for 26 — 0.9987174\n",
      "Token 27/33\n",
      "Probs for 27 — 0.9987085\n",
      "Token 28/33\n",
      "Probs for 28 — 0.9986634\n",
      "Token 29/33\n",
      "Probs for 29 — 0.9986792\n",
      "Token 30/33\n",
      "Probs for 30 — 0.9987509\n",
      "Token 31/33\n",
      "Probs for 31 — 0.9987338\n",
      "Token 32/33\n",
      "Probs for 32 — 0.9721388\n",
      "Token 0/32\n",
      "Probs for 0 — 0.0100493\n",
      "Token 1/32\n",
      "Probs for 1 — 0.9296126\n",
      "Token 2/32\n",
      "Probs for 2 — 0.9422198\n",
      "Token 3/32\n",
      "Probs for 3 — 0.9373564\n",
      "Token 4/32\n",
      "Probs for 4 — 0.9446303\n",
      "Token 5/32\n",
      "Probs for 5 — 0.8134837\n",
      "Token 6/32\n",
      "Probs for 6 — 0.9382555\n",
      "Token 7/32\n",
      "Probs for 7 — 0.9299216\n",
      "Token 8/32\n",
      "Probs for 8 — 0.9571629\n",
      "Token 9/32\n",
      "Probs for 9 — 0.9524079\n",
      "Token 10/32\n",
      "Probs for 10 — 0.9396602\n",
      "Token 11/32\n",
      "Probs for 11 — 0.9397038\n",
      "Token 12/32\n",
      "Probs for 12 — 0.9409708\n",
      "Token 13/32\n",
      "Probs for 13 — 0.9431158\n",
      "Token 14/32\n",
      "Probs for 14 — 0.9426091\n",
      "Token 15/32\n",
      "Probs for 15 — 0.9414166\n",
      "Token 16/32\n",
      "Probs for 16 — 0.9434150\n",
      "Token 17/32\n",
      "Probs for 17 — 0.9450714\n",
      "Token 18/32\n",
      "Probs for 18 — 0.9415068\n",
      "Token 19/32\n",
      "Probs for 19 — 0.9425788\n",
      "Token 20/32\n",
      "Probs for 20 — 0.9435023\n",
      "Token 21/32\n",
      "Probs for 21 — 0.4384536\n",
      "Token 22/32\n",
      "Probs for 22 — 0.9403503\n",
      "Token 23/32\n",
      "Probs for 23 — 0.9408439\n",
      "Token 24/32\n",
      "Probs for 24 — 0.9420659\n",
      "Token 25/32\n",
      "Probs for 25 — 0.9435037\n",
      "Token 26/32\n",
      "Probs for 26 — 0.9420292\n",
      "Token 27/32\n",
      "Probs for 27 — 0.9426529\n",
      "Token 28/32\n",
      "Probs for 28 — 0.9397234\n",
      "Token 29/32\n",
      "Probs for 29 — 0.9444145\n",
      "Token 30/32\n",
      "Probs for 30 — 0.9419163\n",
      "Token 31/32\n",
      "Probs for 31 — 0.8435034\n",
      "Token 0/31\n",
      "Probs for 0 — 0.0007819\n",
      "Token 1/31\n",
      "Probs for 1 — 0.9033928\n",
      "Token 2/31\n",
      "Probs for 2 — 0.9242061\n",
      "Token 3/31\n",
      "Probs for 3 — 0.9186952\n",
      "Token 4/31\n",
      "Probs for 4 — 0.9237597\n",
      "Token 5/31\n",
      "Probs for 5 — 0.1889818\n",
      "Token 6/31\n",
      "Probs for 6 — 0.9184342\n",
      "Token 7/31\n",
      "Probs for 7 — 0.9070237\n",
      "Token 8/31\n",
      "Probs for 8 — 0.9242780\n",
      "Token 9/31\n",
      "Probs for 9 — 0.9340578\n",
      "Token 10/31\n",
      "Probs for 10 — 0.9177580\n",
      "Token 11/31\n",
      "Probs for 11 — 0.9174224\n",
      "Token 12/31\n",
      "Probs for 12 — 0.9194410\n",
      "Token 13/31\n",
      "Probs for 13 — 0.9192689\n",
      "Token 14/31\n",
      "Probs for 14 — 0.9216020\n",
      "Token 15/31\n",
      "Probs for 15 — 0.9263479\n",
      "Token 16/31\n",
      "Probs for 16 — 0.9248280\n",
      "Token 17/31\n",
      "Probs for 17 — 0.9240245\n",
      "Token 18/31\n",
      "Probs for 18 — 0.9191136\n",
      "Token 19/31\n",
      "Probs for 19 — 0.9185491\n",
      "Token 20/31\n",
      "Probs for 20 — 0.7079913\n",
      "Token 21/31\n",
      "Probs for 21 — 0.9187323\n",
      "Token 22/31\n",
      "Probs for 22 — 0.9183234\n",
      "Token 23/31\n",
      "Probs for 23 — 0.9192179\n",
      "Token 24/31\n",
      "Probs for 24 — 0.9193496\n",
      "Token 25/31\n",
      "Probs for 25 — 0.9191469\n",
      "Token 26/31\n",
      "Probs for 26 — 0.9186016\n",
      "Token 27/31\n",
      "Probs for 27 — 0.9140037\n",
      "Token 28/31\n",
      "Probs for 28 — 0.9201919\n",
      "Token 29/31\n",
      "Probs for 29 — 0.9100474\n",
      "Token 30/31\n",
      "Probs for 30 — 0.3907807\n",
      "Token 0/30\n",
      "Probs for 0 — 0.0081703\n",
      "Token 1/30\n",
      "Probs for 1 — 0.9931905\n",
      "Token 2/30\n",
      "Probs for 2 — 0.9947254\n",
      "Token 3/30\n",
      "Probs for 3 — 0.9939383\n",
      "Token 4/30\n",
      "Probs for 4 — 0.9944934\n",
      "Token 5/30\n",
      "Probs for 5 — 0.9987893\n",
      "Token 6/30\n",
      "Probs for 6 — 0.9945215\n",
      "Token 7/30\n",
      "Probs for 7 — 0.9934623\n",
      "Token 8/30\n",
      "Probs for 8 — 0.9951587\n",
      "Token 9/30\n",
      "Probs for 9 — 0.9957973\n",
      "Token 10/30\n",
      "Probs for 10 — 0.9946653\n",
      "Token 11/30\n",
      "Probs for 11 — 0.9946358\n",
      "Token 12/30\n",
      "Probs for 12 — 0.9946865\n",
      "Token 13/30\n",
      "Probs for 13 — 0.9948760\n",
      "Token 14/30\n",
      "Probs for 14 — 0.9947797\n",
      "Token 15/30\n",
      "Probs for 15 — 0.9947057\n",
      "Token 16/30\n",
      "Probs for 16 — 0.9953585\n",
      "Token 17/30\n",
      "Probs for 17 — 0.9951069\n",
      "Token 18/30\n",
      "Probs for 18 — 0.9946865\n",
      "Token 19/30\n",
      "Probs for 19 — 0.9951289\n",
      "Token 20/30\n",
      "Probs for 20 — 0.7678191\n",
      "Token 21/30\n",
      "Probs for 21 — 0.9945652\n",
      "Token 22/30\n",
      "Probs for 22 — 0.9950798\n",
      "Token 23/30\n",
      "Probs for 23 — 0.9949202\n",
      "Token 24/30\n",
      "Probs for 24 — 0.9946813\n",
      "Token 25/30\n",
      "Probs for 25 — 0.9948241\n",
      "Token 26/30\n",
      "Probs for 26 — 0.9948418\n",
      "Token 27/30\n",
      "Probs for 27 — 0.9952461\n",
      "Token 28/30\n",
      "Probs for 28 — 0.9934783\n",
      "Token 29/30\n",
      "Probs for 29 — 0.9892944\n",
      "Token 0/31\n",
      "Probs for 0 — 0.0020357\n",
      "Token 1/31\n",
      "Probs for 1 — 0.9969603\n",
      "Token 2/31\n",
      "Probs for 2 — 0.9978478\n",
      "Token 3/31\n",
      "Probs for 3 — 0.9974282\n",
      "Token 4/31\n",
      "Probs for 4 — 0.9980469\n",
      "Token 5/31\n",
      "Probs for 5 — 0.9940994\n",
      "Token 6/31\n",
      "Probs for 6 — 0.9976577\n",
      "Token 7/31\n",
      "Probs for 7 — 0.9973210\n",
      "Token 8/31\n",
      "Probs for 8 — 0.9976894\n",
      "Token 9/31\n",
      "Probs for 9 — 0.9984140\n",
      "Token 10/31\n",
      "Probs for 10 — 0.9980993\n",
      "Token 11/31\n",
      "Probs for 11 — 0.9976183\n",
      "Token 12/31\n",
      "Probs for 12 — 0.9976063\n",
      "Token 13/31\n",
      "Probs for 13 — 0.9976786\n",
      "Token 14/31\n",
      "Probs for 14 — 0.9977132\n",
      "Token 15/31\n",
      "Probs for 15 — 0.9977160\n",
      "Token 16/31\n",
      "Probs for 16 — 0.9976647\n",
      "Token 17/31\n",
      "Probs for 17 — 0.9977388\n",
      "Token 18/31\n",
      "Probs for 18 — 0.9979258\n",
      "Token 19/31\n",
      "Probs for 19 — 0.9977022\n",
      "Token 20/31\n",
      "Probs for 20 — 0.9976689\n",
      "Token 21/31\n",
      "Probs for 21 — 0.8142630\n",
      "Token 22/31\n",
      "Probs for 22 — 0.9974578\n",
      "Token 23/31\n",
      "Probs for 23 — 0.9976307\n",
      "Token 24/31\n",
      "Probs for 24 — 0.9977477\n",
      "Token 25/31\n",
      "Probs for 25 — 0.9977177\n",
      "Token 26/31\n",
      "Probs for 26 — 0.9976172\n",
      "Token 27/31\n",
      "Probs for 27 — 0.9975290\n",
      "Token 28/31\n",
      "Probs for 28 — 0.9976792\n",
      "Token 29/31\n",
      "Probs for 29 — 0.9975430\n",
      "Token 30/31\n",
      "Probs for 30 — 0.9701028\n"
     ]
    }
   ],
   "source": [
    "attn_blocking_full_results = {}\n",
    "\n",
    "for gt, prompt in correct_prompts.items():\n",
    "    print(f\"Doing {gt}\")\n",
    "    prompt_tokens = model.tokenizer.batch_decode(model.tokenizer(prompt)['input_ids'])\n",
    "    num_tokens = len(prompt_tokens)\n",
    "    num_layers = model.config.num_hidden_layers\n",
    "    exp_results = {'logits': [], 'probs': [], 'tokens': prompt_tokens}\n",
    "\n",
    "    # Get the actual indices of the last six layers\n",
    "    layers_to_block = [num_layers - i - 1 for i in range(6)]\n",
    "\n",
    "    # Block attention\n",
    "    for index in range(0, num_tokens):\n",
    "        indices_list = [(index, num_tokens-1)]\n",
    "        with model.trace(prompt):\n",
    "            # Create attention mask\n",
    "            attention_mask = torch.ones(1, 1, num_tokens, num_tokens, dtype=torch.bool, device=model.device).tril(diagonal=0)\n",
    "            for i, j in indices_list:\n",
    "                attention_mask[:, :, j, i] = False # i think it's j, i\n",
    "\n",
    "            # Run Blocking\n",
    "            for layer_num in layers_to_block:\n",
    "                attn = model.model.layers[layer_num].self_attn.inputs\n",
    "                kwargs = attn[1]\n",
    "                kwargs[\"attention_mask\"] = attention_mask\n",
    "                attn = (attn[0], kwargs)\n",
    "                model.model.layers[layer_num].self_attn.inputs = attn\n",
    "\n",
    "            logits = model.output.logits.save()\n",
    "\n",
    "        # Save logits and probs to results\n",
    "        exp_results['logits'].append(logits.to(\"cpu\").detach())\n",
    "\n",
    "        answer = \" \" + gt\n",
    "        answer_idx = model.tokenizer(answer)[\"input_ids\"][1]\n",
    "        prob = torch.nn.functional.softmax(logits[0, -1], dim=-1)[answer_idx].item()\n",
    "        \n",
    "        exp_results['probs'].append(prob)\n",
    "    \n",
    "    attn_blocking_full_results[gt] = exp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "                               Where the model is predicting 'salary'\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.81 ###   \n",
      "0.86  Instruction   \n",
      "0.82 :   \n",
      "0.85  How   \n",
      "0.91  much   \n",
      "0.83  do   \n",
      "0.84  employees   \n",
      "0.94  earn   \n",
      "0.89 ?   \n",
      "0.84  ###   \n",
      "0.85  Context   \n",
      "0.85 :   \n",
      "0.85  CREATE   \n",
      "0.85  TABLE   \n",
      "0.84  employees   \n",
      "0.86  (   \n",
      "0.85 name   \n",
      "0.85  TEXT   \n",
      "0.86 ,   \n",
      "0.12  salary   <--- High probability drop\n",
      "0.83  INTEGER   \n",
      "0.85 ,   \n",
      "0.84  department   \n",
      "0.85  TEXT   \n",
      "0.84 )   \n",
      "0.85  ###   \n",
      "0.86  Response   \n",
      "0.84 :   \n",
      "0.58  SELECT   \n",
      "----------------------------------------------------------------------------------------------------\n",
      "                               Where the model is predicting 'height'\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1.00 ###   \n",
      "1.00  Instruction   \n",
      "1.00 :   \n",
      "1.00  How   \n",
      "0.98  tall   \n",
      "1.00  are   \n",
      "1.00  all   \n",
      "1.00  the   \n",
      "1.00  buildings   \n",
      "1.00 ?   \n",
      "1.00  ###   \n",
      "1.00  Context   \n",
      "1.00 :   \n",
      "1.00  CREATE   \n",
      "1.00  TABLE   \n",
      "1.00  buildings   \n",
      "1.00  (   \n",
      "1.00 address   \n",
      "1.00  TEXT   \n",
      "1.00 ,   \n",
      "0.91  height   \n",
      "1.00  INTEGER   \n",
      "1.00 ,   \n",
      "1.00  year   \n",
      "1.00 _b   \n",
      "1.00 uilt   \n",
      "1.00  INTEGER   \n",
      "1.00 )   \n",
      "1.00  ###   \n",
      "1.00  Response   \n",
      "1.00 :   \n",
      "0.97  SELECT   \n",
      "----------------------------------------------------------------------------------------------------\n",
      "                               Where the model is predicting 'weight'\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.93 ###   \n",
      "0.94  Instruction   \n",
      "0.94 :   \n",
      "0.94  How   \n",
      "0.81  heavy   \n",
      "0.94  are   \n",
      "0.93  the   \n",
      "0.96  packages   \n",
      "0.95 ?   \n",
      "0.94  ###   \n",
      "0.94  Context   \n",
      "0.94 :   \n",
      "0.94  CREATE   \n",
      "0.94  TABLE   \n",
      "0.94  shipments   \n",
      "0.94  (   \n",
      "0.95 tracking   \n",
      "0.94 _id   \n",
      "0.94  TEXT   \n",
      "0.94 ,   \n",
      "0.44  weight   <--- High probability drop\n",
      "0.94  DEC   \n",
      "0.94 IMAL   \n",
      "0.94 ,   \n",
      "0.94  destination   \n",
      "0.94  TEXT   \n",
      "0.94 )   \n",
      "0.94  ###   \n",
      "0.94  Response   \n",
      "0.94 :   \n",
      "0.84  SELECT   \n",
      "----------------------------------------------------------------------------------------------------\n",
      "                              Where the model is predicting 'distance'\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.90 ###   \n",
      "0.92  Instruction   \n",
      "0.92 :   \n",
      "0.92  How   \n",
      "0.19  far   <--- High probability drop\n",
      "0.92  is   \n",
      "0.91  each   \n",
      "0.92  destination   \n",
      "0.93 ?   \n",
      "0.92  ###   \n",
      "0.92  Context   \n",
      "0.92 :   \n",
      "0.92  CREATE   \n",
      "0.92  TABLE   \n",
      "0.93  locations   \n",
      "0.92  (   \n",
      "0.92 place   \n",
      "0.92  TEXT   \n",
      "0.92 ,   \n",
      "0.71  distance   \n",
      "0.92  INTEGER   \n",
      "0.92 ,   \n",
      "0.92  transport   \n",
      "0.92 _mode   \n",
      "0.92  TEXT   \n",
      "0.92 )   \n",
      "0.91  ###   \n",
      "0.92  Response   \n",
      "0.91 :   \n",
      "0.39  SELECT   <--- High probability drop\n",
      "----------------------------------------------------------------------------------------------------\n",
      "                                Where the model is predicting 'depth'\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.99 ###   \n",
      "0.99  Instruction   \n",
      "0.99 :   \n",
      "0.99  How   \n",
      "1.00  deep   \n",
      "0.99  are   \n",
      "0.99  the   \n",
      "1.00  wells   \n",
      "1.00 ?   \n",
      "0.99  ###   \n",
      "0.99  Context   \n",
      "0.99 :   \n",
      "0.99  CREATE   \n",
      "0.99  TABLE   \n",
      "0.99  wells   \n",
      "1.00  (   \n",
      "1.00 location   \n",
      "0.99  TEXT   \n",
      "1.00 ,   \n",
      "0.77  depth   \n",
      "0.99  INTEGER   \n",
      "1.00 ,   \n",
      "0.99  status   \n",
      "0.99  TEXT   \n",
      "0.99 )   \n",
      "0.99  ###   \n",
      "1.00  Response   \n",
      "0.99 :   \n",
      "0.99  SELECT   \n",
      "----------------------------------------------------------------------------------------------------\n",
      "                                Where the model is predicting 'speed'\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1.00 ###   \n",
      "1.00  Instruction   \n",
      "1.00 :   \n",
      "1.00  How   \n",
      "0.99  fast   \n",
      "1.00  can   \n",
      "1.00  each   \n",
      "1.00  vehicle   \n",
      "1.00  go   \n",
      "1.00 ?   \n",
      "1.00  ###   \n",
      "1.00  Context   \n",
      "1.00 :   \n",
      "1.00  CREATE   \n",
      "1.00  TABLE   \n",
      "1.00  vehicles   \n",
      "1.00  (   \n",
      "1.00 model   \n",
      "1.00  TEXT   \n",
      "1.00 ,   \n",
      "0.81  speed   \n",
      "1.00  INTEGER   \n",
      "1.00 ,   \n",
      "1.00  manufacturer   \n",
      "1.00  TEXT   \n",
      "1.00 )   \n",
      "1.00  ###   \n",
      "1.00  Response   \n",
      "1.00 :   \n",
      "0.97  SELECT   \n"
     ]
    }
   ],
   "source": [
    "for gt, results in attn_blocking_full_results.items():\n",
    "    title = f\"Where the model is predicting '{gt}'\"\n",
    "    print(\"-\"*100)\n",
    "    print(f\"{' '*(50- len(title)//2)}{title}\")\n",
    "    print(\"-\"*100)\n",
    "    for token, prob in zip(results['tokens'], results['probs']):\n",
    "        if token == \"<|begin_of_text|>\": continue\n",
    "        highlight_string = \"<--- High probability drop\" if prob < 0.5 else \"\"\n",
    "        print(f\"{prob:.2f} {token}   {highlight_string}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Still WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do direct logit attribution\n",
    "final_ln = model.model.norm\n",
    "lm_head = model.lm_head\n",
    "\n",
    "layers = model.model.layers\n",
    "probs_layers = []\n",
    "\n",
    "with model.trace() as tracer:\n",
    "    with tracer.invoke(prompt) as invoker:\n",
    "        for layer_idx, layer in enumerate(layers):\n",
    "            # Process layer output through the model's head and layer normalization\n",
    "            layer_output = lm_head(final_ln(layer.output[0]))\n",
    "            # Apply softmax to obtain probabilities and save the result\n",
    "            probs = torch.nn.functional.softmax(layer_output, dim=-1).save()\n",
    "            probs_layers.append(probs)\n",
    "\n",
    "probs = torch.cat([probs.value for probs in probs_layers])\n",
    "\n",
    "# Find the maximum probability and corresponding tokens for each position\n",
    "max_probs, tokens = probs.max(dim=-1)\n",
    "\n",
    "# Decode token IDs to words for each layer\n",
    "words = [[model.tokenizer.decode(t.cpu()).encode(\"unicode_escape\").decode() for t in layer_tokens]\n",
    "    for layer_tokens in tokens]\n",
    "\n",
    "# Access the 'input_ids' attribute of the invoker object to get the input words\n",
    "input_words = [model.tokenizer.decode(t) for t in invoker.inputs[0][\"input_ids\"][0]]\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "z1tQZgLUnojc",
    "S-TNF3PVlGV0"
   ],
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
