{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQAsYjSQVXHC"
      },
      "source": [
        "# Activation Patching on GPT2, M1, M2 and M3 models using nnsight\n",
        "- Developed on Google Colab using an A100 with 40GB GPU and 80GB system RAM.\n",
        "- Runs with GPT2/TinyStories/Qwen/Llama with base/CS1/CS2/CS3.  \n",
        "- Requires a GITHUB_TOKEN secret to access Martian quanta_text_to_sql code repository.\n",
        "- Requires a HF_TOKEN secret to access Martian HuggingFace repository.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1tQZgLUnojc"
      },
      "source": [
        "# Import libraries\n",
        "Imports standard libraries. Do not read."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qt2jt2bHcY2v"
      },
      "outputs": [],
      "source": [
        "# https://nnsight.net/\n",
        "!pip install -U nnsight -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vif7qLNrlC0P"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import einops\n",
        "import torch\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = \"colab\"\n",
        "\n",
        "import nnsight\n",
        "from nnsight import LanguageModel, util"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -q"
      ],
      "metadata": {
        "id": "e2pmPxvcTO9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMWEb8TJoske"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from getpass import getpass\n",
        "    from google.colab import userdata\n",
        "    import gc\n",
        "    import weakref\n",
        "    github_token = userdata.get(\"GITHUB_TOKEN\")\n",
        "except:\n",
        "    import os\n",
        "    github_token = os.getenv(\"GITHUB_TOKEN\")\n",
        "\n",
        "# # Install the private repository using the token\n",
        "!pip install --upgrade git+https://{github_token}@github.com/withmartian/quanta_text_to_sql.git -q\n",
        "\n",
        "import TinySQL as qts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6k59y0OZhp7"
      },
      "source": [
        "# Select model and command set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rl1akjeAUfoV"
      },
      "outputs": [],
      "source": [
        "model_num = 3   # 0=GPT2, 1=TinyStories, 2=Qwen or 3=Llama\n",
        "cs_num = 1      # 0=BaseModel, 1=CS1, 2=CS2 or 3=CS3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y72URtyynvnv"
      },
      "source": [
        "# Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFaFTy6LnbxI"
      },
      "outputs": [],
      "source": [
        "if model_num == 1:\n",
        "    the_tokenizer, the_model = qts.load_sql_interp_model(model_num, cs_num, auth_token=userdata.get(\"HF_TOKEN\"))\n",
        "\n",
        "    model = LanguageModel(the_model, the_tokenizer)\n",
        "    model.tokenizer = the_tokenizer\n",
        "else:\n",
        "    if torch.backends.mps.is_available():\n",
        "        model = LanguageModel(qts.sql_interp_model_location(model_num, cs_num), device_map=\"mps\")\n",
        "    else:\n",
        "        model = LanguageModel(qts.sql_interp_model_location(model_num, cs_num), device_map=\"auto\")\n",
        "\n",
        "clear_output()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2Kc14_tQVTA"
      },
      "source": [
        "# Run prompts to see which one the model gets right"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYFYWe-kQVTA"
      },
      "outputs": [],
      "source": [
        "prompts = [\n",
        "(\"### Instruction: How much do employees earn? ### Context: CREATE TABLE employees (name TEXT, salary INTEGER, department TEXT) ### Response: SELECT\", 'salary'),\n",
        "(\"### Instruction: How much does each product cost? ### Context: CREATE TABLE products (name TEXT, price INTEGER, category TEXT) ### Response: SELECT\", 'price'),\n",
        "(\"### Instruction: How tall are all the buildings? ### Context: CREATE TABLE buildings (address TEXT, height INTEGER, year_built INTEGER) ### Response: SELECT\", 'height'),\n",
        "(\"### Instruction: How many inhabitants live in each city? ### Context: CREATE TABLE cities (name TEXT, population INTEGER, country TEXT) ### Response: SELECT\", 'population'),\n",
        "(\"### Instruction: How heavy are the packages? ### Context: CREATE TABLE shipments (tracking_id TEXT, weight DECIMAL, destination TEXT) ### Response: SELECT\", 'weight'),\n",
        "(\"### Instruction: When was each book released? ### Context: CREATE TABLE books (title TEXT, publication_date DATE, author TEXT) ### Response: SELECT\", 'publication_date'),\n",
        "(\"### Instruction: How far is each destination? ### Context: CREATE TABLE locations (place TEXT, distance INTEGER, transport_mode TEXT) ### Response: SELECT\", 'distance'),\n",
        "(\"### Instruction: What's the temperature in each room? ### Context: CREATE TABLE sensors (room_id TEXT, temp_celsius DECIMAL, humidity INTEGER) ### Response: SELECT\", 'temp_celsius'),\n",
        "(\"### Instruction: How deep are the wells? ### Context: CREATE TABLE wells (location TEXT, depth INTEGER, status TEXT) ### Response: SELECT\", 'depth'),\n",
        "(\"### Instruction: How long are the movies? ### Context: CREATE TABLE films (title TEXT, duration INTEGER, genre TEXT) ### Response: SELECT\", 'duration'),\n",
        "(\"### Instruction: How fast can each vehicle go? ### Context: CREATE TABLE vehicles (model TEXT, speed INTEGER, manufacturer TEXT) ### Response: SELECT\", 'speed'),\n",
        "]\n",
        "\n",
        "results = []\n",
        "prob_layers_all = {}\n",
        "for i, (prompt, gt) in enumerate(prompts):\n",
        "    print(f\"Prompt {i+1}\")\n",
        "    with model.generate(prompt, max_new_tokens=1, temperature=0.0001) as tracer:\n",
        "        out = model.generator.output.save()\n",
        "\n",
        "    output_text = model.tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "    answer = output_text.split(\"SELECT\")[-1].strip()\n",
        "\n",
        "    correct = answer in gt\n",
        "    results.append(correct)\n",
        "\n",
        "    if correct:\n",
        "        layers = model.model.layers\n",
        "        # What I want to do next is see how the probabilities change as we move through the layers\n",
        "        prob_layers = []\n",
        "        final_ln = model.model.norm\n",
        "        lm_head = model.lm_head\n",
        "        with model.trace() as tracer:\n",
        "            with tracer.invoke(prompt) as invoker:\n",
        "                for layer_idx, layer in enumerate(layers):\n",
        "                    # Process layer output through the model's head and layer normalization\n",
        "                    layer_output = lm_head(final_ln(layer.output[0]))\n",
        "                    # Apply softmax to obtain probabilities and save the result\n",
        "                    probs = torch.nn.functional.softmax(layer_output, dim=-1)\n",
        "\n",
        "                    answer = \" \" + gt\n",
        "                    answer_idx = model.tokenizer(answer)[\"input_ids\"][1]\n",
        "                    # Get the probability of the correct answer\n",
        "                    correct_prob = probs[0, -1, answer_idx].save()\n",
        "                    prob_layers.append(correct_prob)\n",
        "\n",
        "        prob_layers_all[answer] = prob_layers\n",
        "\n",
        "\n",
        "print(sum(results)/len(results)) # About 50-60% accuracy in this unscientific test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZqlccpVQVTA"
      },
      "outputs": [],
      "source": [
        "correct_prompts = {}\n",
        "for i, (prompt, gt) in enumerate(prompts):\n",
        "    if results[i]:\n",
        "        correct_prompts[gt] = prompt\n",
        "\n",
        "for gt, prompt in correct_prompts.items():\n",
        "    print(gt, prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2O_dkUPQVTB"
      },
      "source": [
        "# See how probability of correct prompts evolve\n",
        "They become higher in the last 6 layers!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoiIDxPjQVTB"
      },
      "outputs": [],
      "source": [
        "# set plotly to notebook\n",
        "pio.renderers.default = \"vscode\"\n",
        "\n",
        "# plot all on the same graph\n",
        "for answer, probs in prob_layers_all.items():\n",
        "    probs = [p.item() for p in probs]\n",
        "    # Print probs but to 2 decimal places)\n",
        "    print([f\"{p:.2f}\" for p in probs], answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHQ48lIKQVTB"
      },
      "source": [
        "# Run attention blocking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MasoiKHQVTB"
      },
      "outputs": [],
      "source": [
        "attn_blocking_full_results = {}\n",
        "\n",
        "for gt, prompt in correct_prompts.items():\n",
        "    print(f\"Doing {gt}\")\n",
        "    prompt_tokens = model.tokenizer.batch_decode(model.tokenizer(prompt)['input_ids'])\n",
        "    num_tokens = len(prompt_tokens)\n",
        "    num_layers = model.config.num_hidden_layers\n",
        "    exp_results = {'logits': [], 'probs': [], 'tokens': prompt_tokens}\n",
        "\n",
        "    # Get the actual indices of the last six layers\n",
        "    layers_to_block = [num_layers - i - 1 for i in range(6)]\n",
        "\n",
        "    # Block attention\n",
        "    for index in range(0, num_tokens):\n",
        "        indices_list = [(index, num_tokens-1)]\n",
        "        with model.trace(prompt):\n",
        "            # Create attention mask\n",
        "            attention_mask = torch.ones(1, 1, num_tokens, num_tokens, dtype=torch.bool, device=model.device).tril(diagonal=0)\n",
        "            for i, j in indices_list:\n",
        "                attention_mask[:, :, j, i] = False # i think it's j, i\n",
        "\n",
        "            # Run Blocking\n",
        "            for layer_num in layers_to_block:\n",
        "                attn = model.model.layers[layer_num].self_attn.inputs\n",
        "                kwargs = attn[1]\n",
        "                kwargs[\"attention_mask\"] = attention_mask\n",
        "                attn = (attn[0], kwargs)\n",
        "                model.model.layers[layer_num].self_attn.inputs = attn\n",
        "\n",
        "            logits = model.output.logits.save()\n",
        "\n",
        "        # Save logits and probs to results\n",
        "        exp_results['logits'].append(logits.to(\"cpu\").detach())\n",
        "\n",
        "        answer = \" \" + gt\n",
        "        answer_idx = model.tokenizer(answer)[\"input_ids\"][1]\n",
        "        prob = torch.nn.functional.softmax(logits[0, -1], dim=-1)[answer_idx].item()\n",
        "\n",
        "        exp_results['probs'].append(prob)\n",
        "\n",
        "    attn_blocking_full_results[gt] = exp_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6wpLMeOQVTC"
      },
      "outputs": [],
      "source": [
        "for gt, results in attn_blocking_full_results.items():\n",
        "    title = f\"Where the model is predicting '{gt}'\"\n",
        "    print(\"-\"*100)\n",
        "    print(f\"{' '*(50- len(title)//2)}{title}\")\n",
        "    print(\"-\"*100)\n",
        "    for token, prob in zip(results['tokens'], results['probs']):\n",
        "        if token == \"<|begin_of_text|>\": continue\n",
        "        highlight_string = \"<--- High probability drop\" if prob < 0.5 else \"\"\n",
        "        print(f\"{prob:.2f} {token}   {highlight_string}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsnOUBAnQVTC"
      },
      "source": [
        "# WIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jyYd00aQVTC"
      },
      "outputs": [],
      "source": [
        "# Do direct logit attribution\n",
        "final_ln = model.model.norm\n",
        "lm_head = model.lm_head\n",
        "\n",
        "layers = model.model.layers\n",
        "probs_layers = []\n",
        "\n",
        "with model.trace() as tracer:\n",
        "    with tracer.invoke(prompt) as invoker:\n",
        "        for layer_idx, layer in enumerate(layers):\n",
        "            # Process layer output through the model's head and layer normalization\n",
        "            layer_output = lm_head(final_ln(layer.output[0]))\n",
        "            # Apply softmax to obtain probabilities and save the result\n",
        "            probs = torch.nn.functional.softmax(layer_output, dim=-1).save()\n",
        "            probs_layers.append(probs)\n",
        "\n",
        "probs = torch.cat([probs.value for probs in probs_layers])\n",
        "\n",
        "# Find the maximum probability and corresponding tokens for each position\n",
        "max_probs, tokens = probs.max(dim=-1)\n",
        "\n",
        "# Decode token IDs to words for each layer\n",
        "words = [[model.tokenizer.decode(t.cpu()).encode(\"unicode_escape\").decode() for t in layer_tokens]\n",
        "    for layer_tokens in tokens]\n",
        "\n",
        "clean_tokens = model.tokenizer(prompt)[\"input_ids\"]\n",
        "input_words = [model.tokenizer.decode(t) for t in clean_tokens]\n",
        "print( input_words )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "z1tQZgLUnojc",
        "y72URtyynvnv"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}