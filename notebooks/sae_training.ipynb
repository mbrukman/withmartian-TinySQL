{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b2999a-4505-47df-840c-d7d533818c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def install_dependencies():\n",
    "    ! rm -rf sae\n",
    "    ! rm -rf TinySQL\n",
    "    ! git clone https://github.com/amirabdullah19852020/sae.git\n",
    "    ! cd sae && pip install .\n",
    "    ! git clone https://github.com/withmartian/TinySQL.git\n",
    "    ! cd TinySQL && pip install .\n",
    "\n",
    "install_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda9f755-3e79-4453-b769-1b4bb8d9888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from huggingface_hub import upload_folder\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sae import SaeConfig, SaeTrainer, TrainConfig\n",
    "from textwrap import dedent\n",
    "from transformers import AutoModelForCausalLM, AutoModel, AutoTokenizer\n",
    "from TinySQL import sql_interp_model_location\n",
    "from TinySQL.load_data.load_model import free_memory\n",
    "\n",
    "wandb_project_name = \"sql_interp_saes\"\n",
    "sae_repo_name = \"withmartian/sql_interp_saes\"\n",
    "\n",
    "wandb.init(project=wandb_project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6743303-ea11-4ccc-9b07-1c15338b540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cs_num = [1, 2, 3]\n",
    "all_model_num = [1, 2, 3]\n",
    "\n",
    "# This is essentially the same as \"num_epochs\" or number of times the SAE sees each data point.\n",
    "dataset_multiplication_factor = 7\n",
    "expansion_factor = 15\n",
    "max_seq_len = 450\n",
    "batch_size = 16\n",
    "all_k = [256]\n",
    "\n",
    "@dataclass\n",
    "class ModelDatasetFullName:\n",
    "    model_id: str\n",
    "    dataset_id: str\n",
    "    full_dataset_name: str\n",
    "    full_model_name: str\n",
    "    syn: bool\n",
    "    k: int\n",
    "\n",
    "    def load_model(self):\n",
    "        model = AutoModelForCausalLM.from_pretrained(full_model_name)\n",
    "        return model\n",
    "\n",
    "    def get_model_abbrev(self):\n",
    "        model_name_split = self.full_model_name.split(\"/\")[1]\n",
    "        model_abbrev = f\"saes_{model_name_split}_syn={self.syn}\"\n",
    "        return model_abbrev\n",
    "\n",
    "# 1, 2 and 3 map to TinyStores, Qwen and Llama respectively.\n",
    "# Set the below list to any of [1,2,3] based on what you need to train.\n",
    "selected_model_ids = [2]\n",
    "\n",
    "selected_syn = [True]\n",
    "\n",
    "# From cs1 through to cs3.\n",
    "# Need to figure out a dataset for \"base\" SAE.\n",
    "dataset_ids = [1, 2, 3]\n",
    "\n",
    "alpaca_prompt = \"### Instruction: {} ### Context: {} ### Response: {}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8f8554-8b61-4bc6-a970-eebc6fdbf896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_text(input_features):\n",
    "    instruction = input_features['english_prompt']\n",
    "    context = input_features['create_statement']\n",
    "    response = input_features['sql_statement']\n",
    "    final_prompt = alpaca_prompt.format(instruction, context, response)\n",
    "    output = {\"text\": final_prompt}\n",
    "    return output\n",
    "\n",
    "\n",
    "# Define the tokenization function\n",
    "def tokenize_function(max_seq_len, tokenizer, example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        padding=\"max_length\",  # Pad to the max sequence length\n",
    "        truncation=True,       # Truncate sequences longer than max_length\n",
    "        max_length=max_seq_len,    # Set maximum sequence length\n",
    "        return_tensors='pt'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b972865-557c-49bb-acb5-3ab78a827bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_dataset_full_names = []\n",
    "\n",
    "for model_id in selected_model_ids:\n",
    "    for dataset_id in dataset_ids:\n",
    "        for syn in selected_syn:\n",
    "            for k in all_k:\n",
    "                full_model_name = sql_interp_model_location(model_id, dataset_id, synonym=syn)\n",
    "                if syn:\n",
    "                    full_dataset_name = f\"withmartian/cs{dataset_id}_dataset_synonyms\"\n",
    "                else:\n",
    "                    full_dataset_name = f\"withmartian/cs{dataset_id}_dataset\"\n",
    "    \n",
    "                model_dataset_full_name = ModelDatasetFullName(\n",
    "                    model_id=model_id, dataset_id=dataset_id,\n",
    "                    full_dataset_name=full_dataset_name, full_model_name=full_model_name, syn=syn, k=k\n",
    "                )\n",
    "    \n",
    "                all_model_dataset_full_names.append(model_dataset_full_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6372402c-3bcd-4fb5-a0b5-9465f4fe37a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_dataset_full_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3362642a-4a5a-40f1-b427-6e7465bfde61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hookpoints(model_id: int):\n",
    "    model_id = int(model_id)\n",
    "    if model_id == 1:\n",
    "        hookpoints= [\"transformer.h.*.attn\", \"transformer.h.*.mlp\"]\n",
    "        return hookpoints\n",
    "    # Qwen and LLama-3 have same module naming conventions.\n",
    "    elif model_id == 2:\n",
    "        hookpoints = [f\"model.layers.{i}.mlp\" for i in range(5, 20)] + [f\"model.layers.{i}.self_attn\" for i in range(5, 20)]\n",
    "    elif model_id == 3:\n",
    "        hookpoints = [f\"model.layers.{i}.mlp\" for i in range(7, 18)] + [f\"model.layers.{i}.self_attn\" for i in range(7, 18)]\n",
    "        return hookpoints\n",
    "    else:\n",
    "        raise Exception(f\"Hookpoints not added yet for model id {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c3ca12-a4cc-4ba2-9bcf-6da55715a453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sae_config(model_dataset_full_name: ModelDatasetFullName, model_abbrev=None):\n",
    "\n",
    "    k = model_dataset_full_name.k\n",
    "    sae_config = SaeConfig(expansion_factor=expansion_factor, k=k)\n",
    "    hookpoints = get_hookpoints(model_dataset_full_name.model_id)\n",
    "    print(hookpoints)\n",
    "\n",
    "    dataset_name = model_dataset_full_name.full_dataset_name\n",
    "    model_name = model_dataset_full_name.full_model_name\n",
    "\n",
    "    if model_abbrev is None:\n",
    "        model_abbrev = model_dataset_full_name.get_model_abbrev()\n",
    "\n",
    "    run_name = f\"{model_abbrev}/k={k}\"\n",
    "\n",
    "    custom_config = {\n",
    "        \"model_name\": model_name, \"dataset_name\": dataset_name,\n",
    "        \"model_abbrev\":model_abbrev\n",
    "    }\n",
    "\n",
    "    train_config =  TrainConfig(\n",
    "        sae=sae_config, hookpoints=hookpoints, run_name=run_name, batch_size=batch_size, save_every=100000\n",
    "    )\n",
    "    return train_config, custom_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f75e568-26a8-43d0-b40c-2d12445668f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_dataset_full_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62697fc9-f6b1-48b4-b494-68f8430355d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging.\n",
    "get_sae_config(all_model_dataset_full_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97ef7d2-6abd-4d5c-98c2-206e9f79b901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_for_wandb_run_and_layer(run_name, layer_name):\n",
    "    api = wandb.Api()\n",
    "    runs = api.runs(path=\"sae\")\n",
    "    matching_runs = [run for run in runs if run.name == run_name]\n",
    "\n",
    "    # Get the latest run (by creation time)\n",
    "    latest_run = max(matching_runs, key=lambda run: run.created_at) if matching_runs else None\n",
    "\n",
    "    if latest_run:\n",
    "        # Fetch the latest history of the run\n",
    "        history = latest_run.history()\n",
    "\n",
    "        # Filter out the latest values of metrics starting with 'fvu'\n",
    "        latest_metrics = {\n",
    "            key: history[key].iloc[-1]\n",
    "            for key in history.columns if key.endswith(layer_name)\n",
    "        }\n",
    "    \n",
    "        latest_metrics = {key.split(\"/\")[0]: float(value) for key, value in latest_metrics.items()}\n",
    "\n",
    "        if not latest_metrics:\n",
    "            print(f'Warning! No layer metrics found matching {layer_name}. Return empty metrics.')            \n",
    "\n",
    "        return latest_metrics\n",
    "    else:\n",
    "        print(f'Warning! No run found matching {run_name}. Return empty metrics.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02d6a02-c079-4e52-9318-f39b20c1531e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_saes(train_config, custom_config=None):\n",
    "    run_name = train_config.run_name\n",
    "    layers = train_config.hookpoints\n",
    "    folder_path = train_config.run_name\n",
    "\n",
    "    for layer in layers:\n",
    "        layer_metrics_path = f\"{folder_path}/{layer}/metrics.json\"\n",
    "        metrics = get_metrics_for_wandb_run_and_layer(run_name, layer)\n",
    "\n",
    "        with open(layer_metrics_path, \"w\") as f_out:\n",
    "            metrics = json.dump(metrics, f_out)\n",
    "\n",
    "    if custom_config:\n",
    "        with open(f\"{folder_path}/model_config.json\", \"w\") as f_out:\n",
    "            json.dump(custom_config, f_out)\n",
    "\n",
    "    model_abbrev = folder_path.replace(\"saes_\", \"\")\n",
    "\n",
    "    upload_folder(\n",
    "        folder_path=f\"{folder_path}\",\n",
    "        path_in_repo=f\"{folder_path}\",\n",
    "        repo_id=sae_repo_name,\n",
    "        commit_message=f\"Uploading saes for {layers} and {model_abbrev}\",  # Optional commit message\n",
    "        repo_type=\"model\",\n",
    "        ignore_patterns=[\"*.tmp\", \"*.log\"],  # Optionally exclude specific files\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbf6523-a5e0-4b38-9111-f9dd41612231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_upload_sae(model_and_dataset_full_name: ModelDatasetFullName):\n",
    "\n",
    "    mdfn = model_and_dataset_full_name\n",
    "    full_model_name = mdfn.full_model_name\n",
    "\n",
    "    print('Freeing memory')\n",
    "    free_memory()\n",
    "\n",
    "    print(f'Loading {full_model_name}')\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        full_model_name, device_map={\"\": \"cuda\"},\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    print(model)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(full_model_name)\n",
    "\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    print(f\"pad_token is {tokenizer.decode(model.config.pad_token_id)}\")\n",
    "\n",
    "    model_abbrev = model_and_dataset_full_name.get_model_abbrev()\n",
    "\n",
    "    print_message = f\"\"\"Working with model {full_model_name} with dataset name\n",
    "    {mdfn.full_dataset_name} for {model_abbrev}.\n",
    "    \"\"\"\n",
    "\n",
    "    print(dedent(print_message))\n",
    "\n",
    "    partial_tokenize_function = partial(tokenize_function, max_seq_len, tokenizer)\n",
    "\n",
    "    dataset = load_dataset(full_dataset_name)['train']\n",
    "    mapped_dataset = dataset.map(map_text, remove_columns=dataset.features)\n",
    "    copies = [mapped_dataset] * dataset_multiplication_factor\n",
    "    duplicated_dataset = concatenate_datasets(copies)\n",
    "\n",
    "    tokenized_dataset = duplicated_dataset.map(partial_tokenize_function, batched=True, num_proc=8)\n",
    "    tokenized_dataset = tokenized_dataset.shuffle(seed=42)\n",
    "    print(tokenized_dataset)\n",
    "    tokenized_dataset.set_format(type=\"torch\")\n",
    "\n",
    "    train_config, custom_config = get_sae_config(model_and_dataset_full_name, model_abbrev)\n",
    "    print(train_config)\n",
    "    run_name = train_config.run_name\n",
    "    print(f'Prepping trainer for {model_abbrev} with train_config {train_config}')\n",
    "\n",
    "    trainer = SaeTrainer(train_config, tokenized_dataset, model=model)\n",
    "    trainer.fit()\n",
    "\n",
    "    wandb.finish()\n",
    "    print(\"Uploading sae\")\n",
    "    upload_saes(train_config=train_config, custom_config=custom_config)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2a4429-8190-4a98-bde8-4bd88e21bf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freeing memory\n",
      "Loading withmartian/sql_interp_bm2_cs1_experiment_4.2\n",
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n",
      "pad_token is <|endoftext|>\n",
      "Working with model withmartian/sql_interp_bm2_cs1_experiment_4.2 with dataset name\n",
      "    withmartian/cs1_dataset_synonyms for saes_sql_interp_bm2_cs1_experiment_4.2_syn=True.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf64264f1d54d3e953e1e159f6be97b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Map (num_proc=8):   0%|          | 0/535500 [00:00<?, ? examples/s],))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 535500\n",
      "})\n",
      "None\n",
      "TrainConfig(sae=SaeConfig(expansion_factor=15, normalize_decoder=True, num_latents=0, k=256, multi_topk=False, skip_connection=False), batch_size=16, grad_acc_steps=1, micro_acc_steps=1, lr=None, lr_warmup_steps=1000, auxk_alpha=0.0, dead_feature_threshold=10000000, hookpoints=None, init_seeds=[0], layers=[], layer_stride=1, transcode=False, distribute_modules=False, save_every=100000, log_to_wandb=True, run_name='saes_sql_interp_bm2_cs1_experiment_4.2_syn=True/k=256', wandb_log_frequency=1)\n",
      "Prepping trainer for saes_sql_interp_bm2_cs1_experiment_4.2_syn=True with train_config TrainConfig(sae=SaeConfig(expansion_factor=15, normalize_decoder=True, num_latents=0, k=256, multi_topk=False, skip_connection=False), batch_size=16, grad_acc_steps=1, micro_acc_steps=1, lr=None, lr_warmup_steps=1000, auxk_alpha=0.0, dead_feature_threshold=10000000, hookpoints=None, init_seeds=[0], layers=[], layer_stride=1, transcode=False, distribute_modules=False, save_every=100000, log_to_wandb=True, run_name='saes_sql_interp_bm2_cs1_experiment_4.2_syn=True/k=256', wandb_log_frequency=1)\n",
      "Training on modules: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23']\n",
      "Initializing SAEs with random seed(s) [0]\n",
      "Learning rate: 2.21e-04\n",
      "Using 8-bit Adam from bitsandbytes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:zai4vvgw) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fluent-oath-110</strong> at: <a href='https://wandb.ai/nlp_and_interpretability/sql_interp_saes/runs/zai4vvgw' target=\"_blank\">https://wandb.ai/nlp_and_interpretability/sql_interp_saes/runs/zai4vvgw</a><br/> View project at: <a href='https://wandb.ai/nlp_and_interpretability/sql_interp_saes' target=\"_blank\">https://wandb.ai/nlp_and_interpretability/sql_interp_saes</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250124_124723-zai4vvgw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:zai4vvgw). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/amir_google/codes/TinySQL/notebooks/wandb/run-20250124_124845-2mshselo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nlp_and_interpretability/sae/runs/2mshselo' target=\"_blank\">saes_sql_interp_bm2_cs1_experiment_4.2_syn=True/k=256</a></strong> to <a href='https://wandb.ai/nlp_and_interpretability/sae' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nlp_and_interpretability/sae' target=\"_blank\">https://wandb.ai/nlp_and_interpretability/sae</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nlp_and_interpretability/sae/runs/2mshselo' target=\"_blank\">https://wandb.ai/nlp_and_interpretability/sae/runs/2mshselo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of SAE parameters: 578_371_584\n",
      "Number of model parameters: 494_032_768\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12868e867cba42438d01f3987849ea76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Training:   0%|          | 0/33468 [00:00<?, ?it/s],))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for mdfn in all_model_dataset_full_names:\n",
    "    tokenized=train_and_upload_sae(mdfn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
