{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda9f755-3e79-4453-b769-1b4bb8d9888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from huggingface_hub import upload_folder\n",
    "from sae import SaeConfig, SaeTrainer, TrainConfig\n",
    "from sae.data import chunk_and_tokenize\n",
    "from textwrap import dedent\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from TinySQL import sql_interp_model_location\n",
    "from TinySQL.ablate.load_model import free_memory\n",
    "\n",
    "\n",
    "all_cs_num = [1, 2, 3]\n",
    "all_model_num = [0, 1, 2, 3]\n",
    "\n",
    "# This is essentially the same as \"num_epochs\" or number of times the SAE sees each data point.\n",
    "dataset_multiplication_factor = 3\n",
    "\n",
    "wandb_project_name = \"sql_interp_saes\"\n",
    "sae_repo_name = \"withmartian/sql_interp_saes\"\n",
    "\n",
    "wandb.init(project=wandb_project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853a9ec5-aebd-4443-81ef-27461e1d7d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelDatasetFullName:\n",
    "    model_id: str\n",
    "    dataset_id: str\n",
    "    full_dataset_name: str\n",
    "    full_model_name: str\n",
    "\n",
    "    def load_model(self):\n",
    "        model = AutoModelForCausalLM.from_pretrained(full_model_name)\n",
    "        return model\n",
    "\n",
    "# 1, 2 and 3 map to TinyStores, Qwen and Llama respectively.\n",
    "# Set the below list to any of [1,2,3] based on what you need to train.\n",
    "selected_model_ids = [3, 2, 1]\n",
    "\n",
    "# From cs1 through to cs3.\n",
    "# Need to figure out a dataset for \"base\" SAE.\n",
    "dataset_ids = [1, 2, 3]\n",
    "\n",
    "alpaca_prompt = \"### Instruction:\\n{}\\n### Context:\\n{}\\n### Response:\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b972865-557c-49bb-acb5-3ab78a827bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_dataset_full_names = []\n",
    "\n",
    "for model_id in selected_model_ids:\n",
    "    for dataset_id in dataset_ids:\n",
    "        full_model_name = sql_interp_model_location(model_id, dataset_id)\n",
    "        full_dataset_name = f\"withmartian/cs{dataset_id}_dataset\"\n",
    "\n",
    "        model_dataset_full_name = ModelDatasetFullName(\n",
    "            model_id=model_id, dataset_id=dataset_id,\n",
    "            full_dataset_name=full_dataset_name, full_model_name=full_model_name)\n",
    "\n",
    "        all_model_dataset_full_names.append(model_dataset_full_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3362642a-4a5a-40f1-b427-6e7465bfde61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hookpoints(model_id: int):\n",
    "    model_id = int(model_id)\n",
    "    if model_id == 1:\n",
    "        hookpoints= [\"transformer.h.*.attn\", \"transformer.h.*.mlp\"]\n",
    "        return hookpoints\n",
    "    # Qwen and LLama-3 have same module naming conventions.\n",
    "    elif model_id in [2, 3]:\n",
    "        hookpoints = [\"model.layers.*.mlp\", \"model.layers.*.self_attn\"]\n",
    "        return hookpoints\n",
    "    else:\n",
    "        raise Exception(f\"Hookpoints not added yet for model id {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c3ca12-a4cc-4ba2-9bcf-6da55715a453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sae_config(model_dataset_full_name: ModelDatasetFullName, model_abbrev=None):\n",
    "    expansion_factor = 4\n",
    "    k = 32\n",
    "    sae_config = SaeConfig(expansion_factor=expansion_factor)\n",
    "    hookpoints = get_hookpoints(model_dataset_full_name.model_id)\n",
    "    print(hookpoints)\n",
    "\n",
    "    dataset_name = model_dataset_full_name.full_dataset_name\n",
    "    model_name = model_dataset_full_name.full_model_name\n",
    "\n",
    "    if model_abbrev is None:\n",
    "        model_name_splits = model_name.split(\"_\")\n",
    "        model_abbrev = f\"{model_name_splits[2]}_{model_name_splits[3]}\"\n",
    "\n",
    "    run_name = f\"saes_{model_abbrev}\"\n",
    "\n",
    "    custom_config = {\n",
    "        \"model_name\": model_name, \"dataset_name\": dataset_name,\n",
    "        \"model_abbrev\":model_abbrev\n",
    "    }\n",
    "\n",
    "    train_config =  TrainConfig(\n",
    "        sae=sae_config, hookpoints=hookpoints, run_name=run_name, lr=5e-4,\n",
    "    )\n",
    "    return train_config, custom_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f75e568-26a8-43d0-b40c-2d12445668f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_dataset_full_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62697fc9-f6b1-48b4-b494-68f8430355d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging.\n",
    "get_sae_config(all_model_dataset_full_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788f95e9-b396-487a-a62c-089c06475315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_text(input_features):\n",
    "    instruction = input_features['english_prompt']\n",
    "    context = input_features['create_statement']\n",
    "    response = input_features['sql_statement']\n",
    "    final_prompt = alpaca_prompt.format(instruction, context, response)\n",
    "    output = {\"text\": final_prompt}\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c72160-78c6-44bc-8204-f009ba7457a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(model_dataset_full_name: ModelDatasetFullName):\n",
    "    full_dataset_name = model_dataset_full_name.full_dataset_name\n",
    "    full_model_name = model_dataset_full_name.full_model_name\n",
    "\n",
    "    dataset = load_dataset(\n",
    "        full_dataset_name,\n",
    "        split=\"train\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    concatenated_dataset =  concatenate_datasets(dataset_multiplication_factor * [dataset]).shuffle()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(full_model_name)\n",
    "\n",
    "    mapped_dataset = concatenated_dataset.map(map_text)\n",
    "    return mapped_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97ef7d2-6abd-4d5c-98c2-206e9f79b901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_for_wandb_run_and_layer(run_name, layer_name):\n",
    "    api = wandb.Api()\n",
    "    runs = api.runs(path=\"sae\")\n",
    "    matching_runs = [run for run in runs if run.name == run_name]\n",
    "\n",
    "    # Get the latest run (by creation time)\n",
    "    latest_run = max(matching_runs, key=lambda run: run.created_at) if matching_runs else None\n",
    "\n",
    "    if latest_run:\n",
    "        # Fetch the latest history of the run\n",
    "        history = latest_run.history()\n",
    "    \n",
    "        # Filter out the latest values of metrics starting with 'fvu'\n",
    "        latest_metrics = {\n",
    "            key: history[key].iloc[-1]\n",
    "            for key in history.columns if key.endswith(layer_name)\n",
    "        }\n",
    "    \n",
    "        latest_metrics = {key.split(\"/\")[0]: float(value) for key, value in latest_metrics.items()}\n",
    "\n",
    "        if not latest_metrics:\n",
    "            print(f'Warning! No layer metrics found matching {layer_name}. Return empty metrics.')\n",
    "            \n",
    "    \n",
    "        return latest_metrics\n",
    "    else:\n",
    "        print(f'Warning! No run found matching {run_name}. Return empty metrics.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02d6a02-c079-4e52-9318-f39b20c1531e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_saes(train_config, custom_config=None):\n",
    "    run_name = train_config.run_name\n",
    "    layers = train_config.hookpoints\n",
    "    folder_path = train_config.run_name\n",
    "\n",
    "    for layer in layers:\n",
    "        layer_metrics_path = f\"{folder_path}/{layer}/metrics.json\"\n",
    "        metrics = get_metrics_for_wandb_run_and_layer(run_name, layer)\n",
    "\n",
    "        with open(layer_metrics_path, \"w\") as f_out:\n",
    "            metrics = json.dump(metrics, f_out)\n",
    "\n",
    "    if custom_config:\n",
    "        with open(f\"{folder_path}/model_config.json\", \"w\") as f_out:\n",
    "            json.dump(custom_config, f_out)\n",
    "\n",
    "    model_abbrev = folder_path.replace(\"saes_\", \"\")\n",
    "\n",
    "    upload_folder(\n",
    "        folder_path=f\"{folder_path}\",\n",
    "        path_in_repo=f\"{folder_path}\",\n",
    "        repo_id=sae_repo_name,\n",
    "        commit_message=f\"Uploading saes for {layers} and {model_abbrev}\",  # Optional commit message\n",
    "        repo_type=\"model\",\n",
    "        ignore_patterns=[\"*.tmp\", \"*.log\"],  # Optionally exclude specific files\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbf6523-a5e0-4b38-9111-f9dd41612231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_upload_sae(model_and_dataset_full_name: ModelDatasetFullName):\n",
    "\n",
    "    mdfn = model_and_dataset_full_name\n",
    "    full_model_name = mdfn.full_model_name\n",
    "\n",
    "    print('Freeing memory')\n",
    "    free_memory()\n",
    "\n",
    "    print(f'Loading {full_model_name}')\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        full_model_name, device_map={\"\": \"cuda\"},\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    model_abbrev = f\"bm{mdfn.model_id}_cs{mdfn.dataset_id}\"\n",
    "\n",
    "    print_message = f\"\"\"Working with model {full_model_name} with dataset name\n",
    "    {mdfn.full_dataset_name} for {model_abbrev}.\n",
    "    \"\"\"\n",
    "\n",
    "    print(dedent(print_message))\n",
    "\n",
    "    mapped_dataset = get_dataset(model_and_dataset_full_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(full_model_name)\n",
    "\n",
    "    tokenized = chunk_and_tokenize(mapped_dataset, tokenizer)\n",
    "    train_config, custom_config = get_sae_config(model_and_dataset_full_name, model_abbrev)\n",
    "    run_name = train_config.run_name\n",
    "\n",
    "    print(f'Prepping trainer for {model_abbrev} with train_config {train_config}')\n",
    "\n",
    "    trainer = SaeTrainer(train_config, tokenized, model)\n",
    "    trainer.fit()\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    upload_saes(train_config=train_config, custom_config=custom_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2a4429-8190-4a98-bde8-4bd88e21bf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mdfn in all_model_dataset_full_names:\n",
    "    train_and_upload_sae(mdfn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
