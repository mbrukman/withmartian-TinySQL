{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b2999a-4505-47df-840c-d7d533818c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def install_dependencies():\n",
    "    ! rm -rf sae\n",
    "    ! rm -rf TinySQL\n",
    "    ! git clone https://github.com/amirabdullah19852020/sae.git\n",
    "    ! cd sae && pip install .\n",
    "    ! git clone https://github.com/withmartian/TinySQL.git\n",
    "    ! cd TinySQL && pip install .\n",
    "\n",
    "install_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda9f755-3e79-4453-b769-1b4bb8d9888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from huggingface_hub import upload_folder\n",
    "from sae import SaeConfig, SaeTrainer, TrainConfig\n",
    "from textwrap import dedent\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from TinySQL import sql_interp_model_location\n",
    "from TinySQL.load_data.load_model import free_memory\n",
    "\n",
    "wandb_project_name = \"sql_interp_saes\"\n",
    "sae_repo_name = \"withmartian/sql_interp_saes\"\n",
    "\n",
    "wandb.init(project=wandb_project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6743303-ea11-4ccc-9b07-1c15338b540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cs_num = [1, 2, 3]\n",
    "all_model_num = [1, 2, 3]\n",
    "\n",
    "# This is essentially the same as \"num_epochs\" or number of times the SAE sees each data point.\n",
    "dataset_multiplication_factor = 12\n",
    "expansion_factor = 4\n",
    "max_seq_len = 256\n",
    "batch_size = 32\n",
    "all_k = [256]\n",
    "\n",
    "@dataclass\n",
    "class ModelDatasetFullName:\n",
    "    model_id: str\n",
    "    dataset_id: str\n",
    "    full_dataset_name: str\n",
    "    full_model_name: str\n",
    "    syn: bool\n",
    "    k: int\n",
    "\n",
    "    def load_model(self):\n",
    "        model = AutoModelForCausalLM.from_pretrained(full_model_name)\n",
    "        return model\n",
    "\n",
    "    def get_model_abbrev(self):\n",
    "        model_name_split = self.full_model_name.split(\"/\")[1]\n",
    "        model_abbrev = f\"saes_{model_name_split}_syn={self.syn}\"\n",
    "        return model_abbrev\n",
    "\n",
    "# 1, 2 and 3 map to TinyStores, Qwen and Llama respectively.\n",
    "# Set the below list to any of [1,2,3] based on what you need to train.\n",
    "selected_model_ids = [1]\n",
    "\n",
    "selected_syn = [False]\n",
    "\n",
    "# From cs1 through to cs3.\n",
    "# Need to figure out a dataset for \"base\" SAE.\n",
    "dataset_ids = [1, 2, 3]\n",
    "\n",
    "alpaca_prompt = \"### Instruction: {} ### Context: {} ### Response: {}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8f8554-8b61-4bc6-a970-eebc6fdbf896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_text(input_features):\n",
    "    instruction = input_features['english_prompt']\n",
    "    context = input_features['create_statement']\n",
    "    response = input_features['sql_statement']\n",
    "    final_prompt = alpaca_prompt.format(instruction, context, response)\n",
    "    output = {\"text\": final_prompt}\n",
    "    return output\n",
    "\n",
    "# Define the tokenization function\n",
    "def tokenize_function(max_seq_len, tokenizer, example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        padding=\"max_length\",  # Pad to the max sequence length\n",
    "        truncation=True,       # Truncate sequences longer than max_length\n",
    "        max_length=max_seq_len,    # Set maximum sequence length\n",
    "        return_tensors='pt'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b972865-557c-49bb-acb5-3ab78a827bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_dataset_full_names = []\n",
    "\n",
    "for model_id in selected_model_ids:\n",
    "    for dataset_id in dataset_ids:\n",
    "        for syn in selected_syn:\n",
    "            for k in all_k:\n",
    "                full_model_name = sql_interp_model_location(model_id, dataset_id, synonym=syn)\n",
    "                if syn:\n",
    "                    full_dataset_name = f\"withmartian/cs{dataset_id}_dataset_synonyms\"\n",
    "                else:\n",
    "                    full_dataset_name = f\"withmartian/cs{dataset_id}_dataset\"\n",
    "    \n",
    "                model_dataset_full_name = ModelDatasetFullName(\n",
    "                    model_id=model_id, dataset_id=dataset_id,\n",
    "                    full_dataset_name=full_dataset_name, full_model_name=full_model_name, syn=syn, k=k\n",
    "                )\n",
    "    \n",
    "                all_model_dataset_full_names.append(model_dataset_full_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6372402c-3bcd-4fb5-a0b5-9465f4fe37a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_dataset_full_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3362642a-4a5a-40f1-b427-6e7465bfde61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hookpoints(model_id: int):\n",
    "    model_id = int(model_id)\n",
    "    if model_id == 1:\n",
    "        hookpoints= [\"transformer.h.*.attn\", \"transformer.h.*.mlp\"]\n",
    "        return hookpoints\n",
    "    # Qwen and LLama-3 have same module naming conventions.\n",
    "    elif model_id in [2, 3]:\n",
    "        hookpoints = [\"model.layers.*.mlp\", \"model.layers.*.self_attn\"]\n",
    "        return hookpoints\n",
    "    else:\n",
    "        raise Exception(f\"Hookpoints not added yet for model id {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c3ca12-a4cc-4ba2-9bcf-6da55715a453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sae_config(model_dataset_full_name: ModelDatasetFullName, model_abbrev=None):\n",
    "\n",
    "    k = model_dataset_full_name.k\n",
    "    sae_config = SaeConfig(expansion_factor=expansion_factor, k=k)\n",
    "    hookpoints = get_hookpoints(model_dataset_full_name.model_id)\n",
    "    print(hookpoints)\n",
    "\n",
    "    dataset_name = model_dataset_full_name.full_dataset_name\n",
    "    model_name = model_dataset_full_name.full_model_name\n",
    "\n",
    "    if model_abbrev is None:\n",
    "        model_abbrev = model_dataset_full_name.get_model_abbrev()\n",
    "\n",
    "    run_name = f\"{model_abbrev}/k={k}\"\n",
    "\n",
    "    custom_config = {\n",
    "        \"model_name\": model_name, \"dataset_name\": dataset_name,\n",
    "        \"model_abbrev\":model_abbrev\n",
    "    }\n",
    "\n",
    "    train_config =  TrainConfig(\n",
    "        sae=sae_config, hookpoints=hookpoints, run_name=run_name, batch_size=batch_size, save_every=100000\n",
    "    )\n",
    "    return train_config, custom_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f75e568-26a8-43d0-b40c-2d12445668f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_dataset_full_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62697fc9-f6b1-48b4-b494-68f8430355d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging.\n",
    "get_sae_config(all_model_dataset_full_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97ef7d2-6abd-4d5c-98c2-206e9f79b901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_for_wandb_run_and_layer(run_name, layer_name):\n",
    "    api = wandb.Api()\n",
    "    runs = api.runs(path=\"sae\")\n",
    "    matching_runs = [run for run in runs if run.name == run_name]\n",
    "\n",
    "    # Get the latest run (by creation time)\n",
    "    latest_run = max(matching_runs, key=lambda run: run.created_at) if matching_runs else None\n",
    "\n",
    "    if latest_run:\n",
    "        # Fetch the latest history of the run\n",
    "        history = latest_run.history()\n",
    "\n",
    "        # Filter out the latest values of metrics starting with 'fvu'\n",
    "        latest_metrics = {\n",
    "            key: history[key].iloc[-1]\n",
    "            for key in history.columns if key.endswith(layer_name)\n",
    "        }\n",
    "    \n",
    "        latest_metrics = {key.split(\"/\")[0]: float(value) for key, value in latest_metrics.items()}\n",
    "\n",
    "        if not latest_metrics:\n",
    "            print(f'Warning! No layer metrics found matching {layer_name}. Return empty metrics.')            \n",
    "\n",
    "        return latest_metrics\n",
    "    else:\n",
    "        print(f'Warning! No run found matching {run_name}. Return empty metrics.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02d6a02-c079-4e52-9318-f39b20c1531e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_saes(train_config, custom_config=None):\n",
    "    run_name = train_config.run_name\n",
    "    layers = train_config.hookpoints\n",
    "    folder_path = train_config.run_name\n",
    "\n",
    "    for layer in layers:\n",
    "        layer_metrics_path = f\"{folder_path}/{layer}/metrics.json\"\n",
    "        metrics = get_metrics_for_wandb_run_and_layer(run_name, layer)\n",
    "\n",
    "        with open(layer_metrics_path, \"w\") as f_out:\n",
    "            metrics = json.dump(metrics, f_out)\n",
    "\n",
    "    if custom_config:\n",
    "        with open(f\"{folder_path}/model_config.json\", \"w\") as f_out:\n",
    "            json.dump(custom_config, f_out)\n",
    "\n",
    "    model_abbrev = folder_path.replace(\"saes_\", \"\")\n",
    "\n",
    "    upload_folder(\n",
    "        folder_path=f\"{folder_path}\",\n",
    "        path_in_repo=f\"{folder_path}\",\n",
    "        repo_id=sae_repo_name,\n",
    "        commit_message=f\"Uploading saes for {layers} and {model_abbrev}\",  # Optional commit message\n",
    "        repo_type=\"model\",\n",
    "        ignore_patterns=[\"*.tmp\", \"*.log\"],  # Optionally exclude specific files\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbf6523-a5e0-4b38-9111-f9dd41612231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_upload_sae(model_and_dataset_full_name: ModelDatasetFullName):\n",
    "\n",
    "    mdfn = model_and_dataset_full_name\n",
    "    full_model_name = mdfn.full_model_name\n",
    "\n",
    "    print('Freeing memory')\n",
    "    free_memory()\n",
    "\n",
    "    print(f'Loading {full_model_name}')\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        full_model_name, device_map={\"\": \"cuda\"},\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(full_model_name)\n",
    "\n",
    "    model_abbrev = model_and_dataset_full_name.get_model_abbrev()\n",
    "\n",
    "    print_message = f\"\"\"Working with model {full_model_name} with dataset name\n",
    "    {mdfn.full_dataset_name} for {model_abbrev}.\n",
    "    \"\"\"\n",
    "\n",
    "    print(dedent(print_message))\n",
    "\n",
    "    partial_tokenize_function = partial(tokenize_function, max_seq_len, tokenizer)\n",
    "    # Apply tokenization\n",
    "\n",
    "\n",
    "    dataset = load_dataset(full_dataset_name)['train']\n",
    "    mapped_dataset = dataset.map(map_text, remove_columns=dataset.features)\n",
    "    copies = [mapped_dataset] * dataset_multiplication_factor\n",
    "    duplicated_dataset = concatenate_datasets(copies)\n",
    "\n",
    "    tokenized_dataset = duplicated_dataset.map(partial_tokenize_function, batched=True, num_proc=8)\n",
    "    tokenized_dataset = tokenized_dataset.shuffle(seed=42)\n",
    "    print(tokenized_dataset)\n",
    "    tokenized_dataset.set_format(type=\"torch\")\n",
    "    #return tokenized_dataset\n",
    "\n",
    "    train_config, custom_config = get_sae_config(model_and_dataset_full_name, model_abbrev)\n",
    "    print(train_config)\n",
    "    run_name = train_config.run_name\n",
    "    print(f'Prepping trainer for {model_abbrev} with train_config {train_config}')\n",
    "\n",
    "    trainer = SaeTrainer(train_config, tokenized_dataset, model)\n",
    "    trainer.fit()\n",
    "\n",
    "    wandb.finish()\n",
    "    print(\"Uploading sae\")\n",
    "    upload_saes(train_config=train_config, custom_config=custom_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2a4429-8190-4a98-bde8-4bd88e21bf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mdfn in all_model_dataset_full_names:\n",
    "    tokenized=train_and_upload_sae(mdfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d967b53-f12b-4e28-9bdd-f7fc4c26bc3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
