{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ee16cbd-5bee-4e17-8ddd-bca2a6ec4082",
   "metadata": {},
   "source": [
    "### Create statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a363a321-f35d-4e5f-a7d1-27695d781df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3208ff79-79d8-4da4-b066-f30b76848e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af9b474-c0f7-47d0-b048-a9ea0ea96af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_entity = \"nlp_and_interpretability\"  # Change for your own wandb entity\n",
    "wandb_project = \"tinysql\"\n",
    "artifact_name = \"TinyStoriesStatistics\"\n",
    "wandb.init(project=wandb_project, entity=wandb_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d10c128-9b26-46a5-95e9-83540c6f916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def get_tokenizer_vocab(model_name):\n",
    "    \"\"\"\n",
    "    Retrieves the vocabulary of a tokenizer given the model name.\n",
    "    Args:\n",
    "    - model_name (str): The name of the model to load the tokenizer for.\n",
    "    Returns:\n",
    "    - dict: A dictionary where keys are tokens and values are token IDs.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    return vocab, tokenizer\n",
    "\n",
    "# Example usage\n",
    "model_name = 'roneneldan/TinyStories-33M'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer_vocab, tokenizer = get_tokenizer_vocab(model_name)\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "if generate_data:\n",
    "    dataset = load_dataset('roneneldan/TinyStoriesInstruct')\n",
    "    \n",
    "    from tqdm import tqdm\n",
    "    texts = []\n",
    "    dtrain = dataset['train']\n",
    "    for item in tqdm(dtrain):\n",
    "        texts.append(item['text'])\n",
    "    \n",
    "    len(texts)\n",
    "    #texts = [item for item in dataset['train']]\n",
    "    \n",
    "    from collections import Counter\n",
    "    import re\n",
    "    \n",
    "    \n",
    "    words = []\n",
    "    for sentence in tqdm(texts):\n",
    "        # Tokenize and clean each sentence\n",
    "        words.extend(re.findall(r'\\b\\w+\\b', sentence.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f3b3b4-ba0a-45e3-9a8a-4571a03d022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_data:\n",
    "    # Count the occurrences of each word\n",
    "    print(f'Running counter')\n",
    "    word_counts = Counter(words)\n",
    "    print(f'Finished counter')\n",
    "    \n",
    "    # Filter to get words that occur at least 5 times\n",
    "    unigram_statistics = {word: count for word, count in word_counts.items() if count >= 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47960c70-635b-454b-a598-d899b7d2987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "if generate_data:\n",
    "    with open(\"tokenizer_vocab.json\", \"w\") as f:\n",
    "        json.dump(tokenizer_vocab, f)\n",
    "        \n",
    "    with open(\"unigram_statistics.json\", \"w\") as f:\n",
    "        json.dump(unigram_statistics, f)\n",
    "    \n",
    "    # Create W&B artifact and add files\n",
    "    artifact = wandb.Artifact(artifact_name, type=\"dataset\")\n",
    "    artifact.add_file(\"tokenizer_vocab.json\")\n",
    "    artifact.add_file(\"unigram_statistics.json\")\n",
    "    \n",
    "    # Log the artifact to the W&B run\n",
    "    wandb.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9d77fa-f4dd-4916-bc57-c8b9e5c48177",
   "metadata": {},
   "source": [
    "### Load and analyze statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c12e078-c9d7-4339-b657-e0b812b7e4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import wandb\n",
    "\n",
    "run = wandb.init()\n",
    "artifact = run.use_artifact('nlp_and_interpretability/tinysql/TinyStoriesStatistics:v0', type='dataset')\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "with open(f'{artifact_dir}/tokenizer_vocab.json', 'r') as f_in:\n",
    "    tokenizer_vocab = json.load(f_in)\n",
    "\n",
    "with open(f'{artifact_dir}/unigram_statistics.json', 'r') as f_in:\n",
    "    unigram_statistics = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f703d44-b12b-4312-a072-0f516de9de36",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2514f9-6eb8-4deb-993b-f37318d19b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = sum(unigram_statistics.values())\n",
    "\n",
    "normalized_unigram_statistics = {key: value/total_words for key, value in unigram_statistics.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0499dc0-62cd-45ce-bfac-f60b520484cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Example word frequency counter\n",
    "# Sort the word frequency by frequency\n",
    "sorted_word_freq = dict(sorted(unigram_statistics.items(), key=lambda item: item[1], reverse=True))\n",
    "num_keys = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910b8940-4918-4b6c-877b-6e05577c5b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot of 50 most common words\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.plot(\n",
    "    list(sorted_word_freq.keys())[:num_keys], list(sorted_word_freq.values())[:num_keys],\n",
    "    marker='o'\n",
    ")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Word Frequency Distribution')\n",
    "\n",
    "# Show the plot\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels for better readability\n",
    "plt.tight_layout()  # Adjust layout to fit the labels\n",
    "plt.savefig(\"tiny_stories_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49d3460-4a97-4e04-a9d6-3fae061bc416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bd2bb6-b07e-4bc2-a27a-5ba5e0278bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "martian_template = \"withmartian/{}_dataset\"\n",
    "\n",
    "keys = [\"cs1\", \"cs2\", \"cs3\"]\n",
    "\n",
    "datasets = {key: load_dataset(martian_template.format(key)) for key in keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5da618c-a27d-4f4e-845f-abec2d7ce3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_fields = [\n",
    "    \"table_name\", \"english_prompt\", \"sql_statement\", \"table_fields\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed0a39d-14eb-4d83-aaab-f08cb20eb80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from functools import lru_cache\n",
    "\n",
    "def remove_punctuation_with_space(text):\n",
    "    # Create a translation table mapping each punctuation to a space\n",
    "    translation_table = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "    return text.translate(translation_table)\n",
    "\n",
    "@lru_cache(maxsize=128)\n",
    "def get_all_tokens(dataset_key, field_name):\n",
    "    \"\"\"\n",
    "    Extracts all unique tokens from specified fields in a dataset by splitting on whitespace and lowercasing.\n",
    "    \n",
    "    Args:\n",
    "    - dataset_key\n",
    "    - field_names (list): A list of field names to extract tokens from (e.g., [\"english_prompt\", \"sql_statement\"]).\n",
    "    \n",
    "    Returns:\n",
    "    - all_tokens (set): A set of all unique tokens found in the specified fields across the dataset.\n",
    "    \"\"\"\n",
    "    # Initialize a set to store all unique tokens\n",
    "    all_tokens = set()\n",
    "\n",
    "    dataset = datasets[dataset_key][\"train\"]\n",
    "    \n",
    "    # Loop through the dataset and process the specified fields\n",
    "    for entry in tqdm(dataset):\n",
    "        text = entry.get(field_name, \"\")\n",
    "        text = remove_punctuation_with_space(text)\n",
    "\n",
    "        # Process only if the field exists and is not empty\n",
    "        if text:\n",
    "            # Split on whitespace and lowercase the tokens\n",
    "            tokens = text.lower().split()\n",
    "            all_tokens.update(tokens)\n",
    "\n",
    "    return all_tokens\n",
    "\n",
    "def calculate_token_occurrence_rate(dataset_key, field_names, unigram_statistics):\n",
    "    \"\"\"\n",
    "    This function takes in a dataset, field names, and unigram statistics to plot the token occurrence rates.\n",
    "\n",
    "    Args:\n",
    "    - dataset (list): A list of dictionaries where each dictionary represents an entry in the dataset.\n",
    "    - field_name (str): The field name to extract tokens from (e.g., \"english_prompt\", \"sql_statement\", \"table_fields\").\n",
    "    - unigram_statistics (dict): A dictionary where keys are tokens and values are their occurrence rates.\n",
    "    \n",
    "    Returns:\n",
    "    - A plot showing the token occurrence rates for the specified field in the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Processing {key}\")\n",
    "\n",
    "\n",
    "    all_stats = {}\n",
    "    for field in field_names:\n",
    "        tokens_and_rates = []\n",
    "        print(f'Processing {field} for {dataset_key}')\n",
    "        all_tokens = get_all_tokens(dataset_key, field)\n",
    "        tokens_and_rates = [(token, unigram_statistics.get(token, 0)) for token in all_tokens]\n",
    "        tokens_and_rates = sorted(tokens_and_rates, key = lambda x: -x[1])\n",
    "\n",
    "        null_tokens = sorted([token_and_rate[0] for token_and_rate in tokens_and_rates if token_and_rate[1] < 5])\n",
    "\n",
    "        all_stats[field] = {\n",
    "            \"null_tokens\": null_tokens.copy(),\n",
    "            \"tokens_and_rates\": tokens_and_rates.copy(),\n",
    "            \"num_tokens\": len(tokens_and_rates),\n",
    "            \"num_null\": len(null_tokens)\n",
    "        }\n",
    "    return all_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a72896e-07b7-48a2-ab0c-fb9ca09f8955",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_stats = {}\n",
    "for key in datasets:\n",
    "    dataset_stats[key] = calculate_token_occurrence_rate(key, relevant_fields, unigram_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ca0936-f275-4d1e-b583-8a920a0cfc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "trimmed_dataset_stats = deepcopy(dataset_stats)\n",
    "\n",
    "for key, curr_stats in trimmed_dataset_stats.items():\n",
    "    for field, stats in curr_stats.items():\n",
    "        del stats['null_tokens']\n",
    "        del stats['tokens_and_rates']\n",
    "\n",
    "trimmed_dataset_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d07d2c-fa46-43d6-85b0-f77180c0afa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_stats['cs3']['table_fields']['null_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bd8472-9a1a-4633-bdea-62652635e61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_sql_artifact = \"TinySQLStatistics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48320ff2-e76a-4ae0-a0a1-f9134872e535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create W&B artifact and add files\n",
    "import json\n",
    "\n",
    "artifact = wandb.Artifact(tiny_sql_artifact, type=\"dataset\")\n",
    "\n",
    "filename = \"dataset_stats.json\"\n",
    "with open(filename, \"w\") as f_out:\n",
    "    json.dump(dataset_stats, f_out)\n",
    "    artifact.add_file(filename)\n",
    "\n",
    "# Log the artifact to the W&B run\n",
    "wandb.log_artifact(artifact)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
